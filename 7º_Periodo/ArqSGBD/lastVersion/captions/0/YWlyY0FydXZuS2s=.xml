<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0" dur="2.51">[Music]</text><text start="4.48" dur="4">this is a three it&amp;#39;s sloppily written</text><text start="6.879" dur="4.401">and rendered at an extremely low</text><text start="8.48" dur="4.92">resolution of 28x 28 pixels but your</text><text start="11.28" dur="4.2">brain has no trouble recognizing it as a</text><text start="13.4" dur="4.24">three and I want you to take a moment to</text><text start="15.48" dur="4.959">appreciate how crazy it is that brains</text><text start="17.64" dur="5.08">can do this so effortlessly I mean this</text><text start="20.439" dur="4.641">this and this are also recognizable as</text><text start="22.72" dur="4.68">threes even though the specific values</text><text start="25.08" dur="5">of each pixel is very different from one</text><text start="27.4" dur="4.48">image to the next the particular light</text><text start="30.08" dur="4.319">sensitive cells in your eye that are</text><text start="31.88" dur="4.12">firing when you see this three are very</text><text start="34.399" dur="4.241">different from the ones firing when you</text><text start="36" dur="5.079">see this three but something in that</text><text start="38.64" dur="4.759">crazy smart visual cortex of yours</text><text start="41.079" dur="4.64">resolves these as representing the same</text><text start="43.399" dur="5.721">idea while at the same time recognizing</text><text start="45.719" dur="5.761">other images as their own distinct ideas</text><text start="49.12" dur="5.239">but if I told you hey sit down and write</text><text start="51.48" dur="6.079">for me a program that takes in a grid of</text><text start="54.359" dur="6">28x 28 pixels like this and outputs a</text><text start="57.559" dur="5.401">single number between 0 and 10 telling</text><text start="60.359" dur="4.681">you what it thinks the digit is well the</text><text start="62.96" dur="4.64">task goes from comically trivial to</text><text start="65.04" dur="4.2">dauntingly difficult unless you&amp;#39;ve been</text><text start="67.6" dur="3.24">living under a rock I think I hardly</text><text start="69.24" dur="3">need to motivate the relevance and</text><text start="70.84" dur="3.36">importance of machine learning and</text><text start="72.24" dur="3.919">neural networks to the present and to</text><text start="74.2" dur="4.08">the future but what I want to do here is</text><text start="76.159" dur="4.521">show you what a neural network actually</text><text start="78.28" dur="4.4">is assuming no background and to help</text><text start="80.68" dur="4.68">visualize what it&amp;#39;s doing not as a</text><text start="82.68" dur="4.24">buzzword but as a piece of math my hope</text><text start="85.36" dur="3.96">is just that you come away feeling like</text><text start="86.92" dur="3.92">the structure itself is motivated and to</text><text start="89.32" dur="3.4">feel like you know what it means when</text><text start="90.84" dur="4.68">you read or you hear about a neural</text><text start="92.72" dur="4.359">network quote unquote learning this</text><text start="95.52" dur="3.239">video is just going to be devoted to the</text><text start="97.079" dur="2.761">structure component of that and the</text><text start="98.759" dur="3.241">following one is going to tackle</text><text start="99.84" dur="4.12">learning what we&amp;#39;re going to do is put</text><text start="102" dur="5.56">together a neural network that can learn</text><text start="103.96" dur="3.6">to recognize handwritten</text><text start="108.6" dur="4.24">digits this is a somewhat classic</text><text start="110.88" dur="3.48">example for introducing the topic and</text><text start="112.84" dur="3">I&amp;#39;m happy to stick with the status quo</text><text start="114.36" dur="3.039">here because at the end of the two</text><text start="115.84" dur="3.48">videos I want to point you to a couple</text><text start="117.399" dur="3.64">good resources where you can learn more</text><text start="119.32" dur="4.799">and where you can download the code that</text><text start="121.039" dur="6.04">does this and play with it on your own</text><text start="124.119" dur="4.881">computer there are many many variants of</text><text start="127.079" dur="3.88">neural networks and in recent years</text><text start="129" dur="4.319">there&amp;#39;s been sort of a boom in research</text><text start="130.959" dur="4.521">towards these variants but in these two</text><text start="133.319" dur="3.801">introductory videos you and I are just</text><text start="135.48" dur="4.32">going to look at the simplest plain</text><text start="137.12" dur="5">vanilla form with no added Frills this</text><text start="139.8" dur="3.719">is kind of a necessary prerequisite for</text><text start="142.12" dur="3.8">understanding any of the more powerful</text><text start="143.519" dur="4.241">modern variants and trust me it still</text><text start="145.92" dur="4">has plenty of complexity for us to wrap</text><text start="147.76" dur="4.24">our minds around but even in this</text><text start="149.92" dur="4.08">simplest form it can learn to recognize</text><text start="152" dur="4.44">handwritten digits which is a pretty</text><text start="154" dur="5.04">cool thing for a computer to be able to</text><text start="156.44" dur="4.48">do and at the same time you&amp;#39;ll see how</text><text start="159.04" dur="3.52">it does fall short of a couple hopes</text><text start="160.92" dur="4.52">that we might have for</text><text start="162.56" dur="5.24">it as the name suggests neural networks</text><text start="165.44" dur="4.56">are inspired by the brain but let&amp;#39;s</text><text start="167.8" dur="4.68">break that down what are the neurons and</text><text start="170" dur="5.12">in what sense are they linked together</text><text start="172.48" dur="4.839">right now when I say neuron all I want</text><text start="175.12" dur="4.72">you to think about is a thing that holds</text><text start="177.319" dur="5.48">a number specifically a number between Z</text><text start="179.84" dur="5.959">0 and 1 it&amp;#39;s really not more than</text><text start="182.799" dur="5.16">that for example the network starts with</text><text start="185.799" dur="5.961">a bunch of neurons corresponding to each</text><text start="187.959" dur="6.961">of the 28 * 28 pixels of the input image</text><text start="191.76" dur="5.479">which is 784 neurons in total each one</text><text start="194.92" dur="5.16">of these holds a number that represents</text><text start="197.239" dur="5.64">the grayscale value of the corresponding</text><text start="200.08" dur="5.879">pixel ranging from zero for black pixels</text><text start="202.879" dur="5.121">up to one for white pixels this number</text><text start="205.959" dur="4">inside the neuron is called its</text><text start="208" dur="3.92">activation and the image you might have</text><text start="209.959" dur="5.64">in mind here is that each neuron is lit</text><text start="211.92" dur="7.44">up when its activation is a high</text><text start="215.599" dur="7.56">number so all of these 784 neurons make</text><text start="219.36" dur="3.799">up the first layer of our</text><text start="225.319" dur="4.681">Network now jumping over to the last</text><text start="227.599" dur="4.601">layer this has 10 neurons each</text><text start="230" dur="4.76">representing one of the digits the</text><text start="232.2" dur="4.64">activation in these neurons again some</text><text start="234.76" dur="4.399">number that&amp;#39;s between 0 and one</text><text start="236.84" dur="4.64">represents how much the system thinks</text><text start="239.159" dur="5">that a given image corresponds with a</text><text start="241.48" dur="5.119">given digit there&amp;#39;s also a couple layers</text><text start="244.159" dur="4.521">in between called The Hidden layers</text><text start="246.599" dur="4.28">which for the time being should just be</text><text start="248.68" dur="4.119">a giant question mark for how on Earth</text><text start="250.879" dur="4">this process of recognizing digits is</text><text start="252.799" dur="4.44">going to be handled in this network I</text><text start="254.879" dur="4.801">chose two hidden layers each one with 16</text><text start="257.239" dur="4.56">neurons and admittedly that&amp;#39;s kind of an</text><text start="259.68" dur="3.44">arbitrary choice to be honest I chose</text><text start="261.799" dur="3.481">two layers based on how I want to</text><text start="263.12" dur="4.2">motivate the structure in just a moment</text><text start="265.28" dur="4.04">and 16 well that was just a nice number</text><text start="267.32" dur="3.68">to fit on the screen in practice there</text><text start="269.32" dur="4.12">is a lot lot of room for experiment with</text><text start="271" dur="4.52">a specific structure here the way the</text><text start="273.44" dur="4.479">network operates activations in one</text><text start="275.52" dur="4.56">layer determine the activations of the</text><text start="277.919" dur="4.441">next layer and of course the heart of</text><text start="280.08" dur="5.119">the network as an information processing</text><text start="282.36" dur="4.559">mechanism comes down to exactly how</text><text start="285.199" dur="4.201">those activations from one layer bring</text><text start="286.919" dur="4.881">about activations in the next layer it&amp;#39;s</text><text start="289.4" dur="4.6">meant to be Loosely analogous to how in</text><text start="291.8" dur="4.56">biological networks of neurons some</text><text start="294" dur="4.56">groups of neurons firing cause certain</text><text start="296.36" dur="4.24">others to fire now the network I&amp;#39;m</text><text start="298.56" dur="3.88">showing here has already been trained to</text><text start="300.6" dur="3.96">recognize digits and let me show you</text><text start="302.44" dur="5.8">what I mean by that it means if you feed</text><text start="304.56" dur="5.8">in an image lighting up all 784 neurons</text><text start="308.24" dur="4.519">of the input layer according to the</text><text start="310.36" dur="4.96">brightness of each pixel in the image</text><text start="312.759" dur="4.801">that pattern of activations causes some</text><text start="315.32" dur="3.68">very specific pattern in the next layer</text><text start="317.56" dur="3.24">which causes some pattern in the one</text><text start="319" dur="3.919">after it which finally gives some</text><text start="320.8" dur="4.88">pattern in the output layer and the</text><text start="322.919" dur="4.921">brightest neuron of that output layer is</text><text start="325.68" dur="5.959">the Network&amp;#39;s choice so to speak for</text><text start="327.84" dur="3.799">what digit this image represents</text><text start="332.479" dur="3.801">and before jumping into the math for how</text><text start="334.28" dur="4.44">one layer influences the next or how</text><text start="336.28" dur="4.68">training Works let&amp;#39;s just talk about why</text><text start="338.72" dur="4.4">it&amp;#39;s even reasonable to expect a layered</text><text start="340.96" dur="4.4">structure like this to behave</text><text start="343.12" dur="3.84">intelligently what are we expecting here</text><text start="345.36" dur="4.6">what is the best hope for what those</text><text start="346.96" dur="5.44">middle layers might be doing well when</text><text start="349.96" dur="4.88">you or I recognize digits we piece</text><text start="352.4" dur="5.12">together various components a nine has a</text><text start="354.84" dur="4.84">loop up top and a line on the right an</text><text start="357.52" dur="5.519">eight also has a loop up top but it&amp;#39;s</text><text start="359.68" dur="5">paired with another loop down low a four</text><text start="363.039" dur="4.681">basically breaks down into three</text><text start="364.68" dur="5.239">specific lines and things like that now</text><text start="367.72" dur="4.84">in a perfect world we might hope that</text><text start="369.919" dur="4.361">each neuron in the second to last layer</text><text start="372.56" dur="3.8">corresponds with one of these</text><text start="374.28" dur="5.039">subcomponents that anytime you feed in</text><text start="376.36" dur="5.44">an image with say a loop up top like a 9</text><text start="379.319" dur="4.121">or an eight there&amp;#39;s some specific neuron</text><text start="381.8" dur="4.16">whose activation is going to be close to</text><text start="383.44" dur="4.64">one and I don&amp;#39;t mean this specific Loop</text><text start="385.96" dur="4.48">of pixels the hope would be that any</text><text start="388.08" dur="5.28">generally loopy pattern to WS the top</text><text start="390.44" dur="5.12">sets off this neuron that way going from</text><text start="393.36" dur="4.36">the third layer to the last one just</text><text start="395.56" dur="4.4">requires learning which combination of</text><text start="397.72" dur="4.479">subcomponents corresponds to which</text><text start="399.96" dur="3.88">digits of course that just kicks the</text><text start="402.199" dur="3.361">problem down the road because how would</text><text start="403.84" dur="3.28">you recognize these subcomponents or</text><text start="405.56" dur="3.12">even learn what the right subcomponents</text><text start="407.12" dur="3.32">should be and I still haven&amp;#39;t even</text><text start="408.68" dur="3.919">talked about how one layer influences</text><text start="410.44" dur="4.759">the next but run with me on this one for</text><text start="412.599" dur="4.961">a moment recognizing a loop can also</text><text start="415.199" dur="3.961">break down into sub problems one</text><text start="417.56" dur="4.319">reasonable way to do this would be to</text><text start="419.16" dur="6.159">First recognize the various little edges</text><text start="421.879" dur="4.6">that make it up similarly a long line</text><text start="425.319" dur="3.761">like the kind you might see in the</text><text start="426.479" dur="4.72">digits one or four or seven well that&amp;#39;s</text><text start="429.08" dur="3.959">really just a long Edge or maybe you</text><text start="431.199" dur="5.961">think of it as a certain pattern of</text><text start="433.039" dur="6.041">several smaller edges so maybe our hope</text><text start="437.16" dur="3.96">is that each neuron in the second layer</text><text start="439.08" dur="5.119">of the network corresponds with the</text><text start="441.12" dur="5.639">various relevant little edges maybe when</text><text start="444.199" dur="4.921">an image like this one comes in it</text><text start="446.759" dur="4.521">lights up all of the neurons associated</text><text start="449.12" dur="4.4">with a around 8 to 10 specific little</text><text start="451.28" dur="4.24">edges which in turn lights up the</text><text start="453.52" dur="4.519">neurons associated with the upper Loop</text><text start="455.52" dur="5.079">and a long vertical line and those light</text><text start="458.039" dur="3.88">up the neuron associated with a nine</text><text start="460.599" dur="3.28">whether or not this is what our final</text><text start="461.919" dur="3.84">Network actually does is another</text><text start="463.879" dur="4.121">question one that I&amp;#39;ll come back to once</text><text start="465.759" dur="4.521">we see how to train the network but this</text><text start="468" dur="4.12">is a hope that we might have a sort of</text><text start="470.28" dur="4.479">goal with the layered structure like</text><text start="472.12" dur="4.639">this moreover you can imagine how being</text><text start="474.759" dur="4.041">able to detect edges and patterns like</text><text start="476.759" dur="5.16">this would be really useful for other</text><text start="478.8" dur="4.6">image recog tasks and even Beyond image</text><text start="481.919" dur="3.28">recognition there are all sorts of</text><text start="483.4" dur="3.4">intelligent things you might want to do</text><text start="485.199" dur="4.44">that break down into layers of</text><text start="486.8" dur="4.72">abstraction parsing speech for example</text><text start="489.639" dur="4.321">involves taking raw audio and picking</text><text start="491.52" dur="4.639">out distinct sounds which combine to</text><text start="493.96" dur="3.959">make certain syllables which combine to</text><text start="496.159" dur="5">form words which combine to make up</text><text start="497.919" dur="4.921">phrases and more abstract thoughts Etc</text><text start="501.159" dur="3.681">but getting back to how any of this</text><text start="502.84" dur="4.079">actually works picture yourself right</text><text start="504.84" dur="4.16">now designing how exactly the</text><text start="506.919" dur="4.84">activations in one layer might determine</text><text start="509" dur="4.76">the activ in the next the goal is to</text><text start="511.759" dur="4.801">have some mechanism that could</text><text start="513.76" dur="4.839">conceivably combine pixels into edges or</text><text start="516.56" dur="4.359">edges into patterns or patterns into</text><text start="518.599" dur="4.641">digits and to zoom in on one very</text><text start="520.919" dur="4.801">specific example let&amp;#39;s say the hope is</text><text start="523.24" dur="4.68">for one particular neuron in the second</text><text start="525.72" dur="5.64">layer to pick up on whether or not the</text><text start="527.92" dur="6.12">image has an edge in this region here</text><text start="531.36" dur="5.28">the question at hand is what parameters</text><text start="534.04" dur="4.72">should the network have what dials and</text><text start="536.64" dur="3.84">knobs should you be able to tweak so</text><text start="538.76" dur="4.36">that it&amp;#39;s Express iive enough to</text><text start="540.48" dur="4.84">potentially capture this pattern or any</text><text start="543.12" dur="3.959">other pixel pattern or the pattern that</text><text start="545.32" dur="4.759">several edges can make a loop and other</text><text start="547.079" dur="5.481">such things Well what we&amp;#39;ll do is assign</text><text start="550.079" dur="4.721">a weight to each one of the connections</text><text start="552.56" dur="5.08">between our neuron and the neurons from</text><text start="554.8" dur="5.4">the first layer these weights are just</text><text start="557.64" dur="4.96">numbers then take all of those</text><text start="560.2" dur="4.68">activations from the first layer and</text><text start="562.6" dur="3.96">compute their weighted sum according to</text><text start="564.88" dur="3.8">these</text><text start="566.56" dur="3.48">weights I find it helpful to think of</text><text start="568.68" dur="3.96">these weights as being being organized</text><text start="570.04" dur="4.28">into a little grid of their own and I&amp;#39;m</text><text start="572.64" dur="3.439">going to use green pixels to indicate</text><text start="574.32" dur="3.519">positive weights and red pixels to</text><text start="576.079" dur="4.161">indicate negative weights where the</text><text start="577.839" dur="5.201">brightness of that pixel is some loose</text><text start="580.24" dur="4.52">depiction of the weights value now if we</text><text start="583.04" dur="4">made the weights associated with almost</text><text start="584.76" dur="4.079">all of the pixel zero except for some</text><text start="587.04" dur="4.44">positive weights in this region that we</text><text start="588.839" dur="4.641">care about then taking the weighted sum</text><text start="591.48" dur="3.76">of all the pixel values really just</text><text start="593.48" dur="4.479">amounts to adding up the values of the</text><text start="595.24" dur="4.8">pixel just in the region that we care</text><text start="597.959" dur="4.641">about and if if you really wanted to</text><text start="600.04" dur="4.16">pick up on whether there&amp;#39;s an edge here</text><text start="602.6" dur="3.88">what you might do is have some negative</text><text start="604.2" dur="5.12">weights associated with the surrounding</text><text start="606.48" dur="4.52">pixels then the sum is largest when</text><text start="609.32" dur="4.28">those middle pixels are bright but the</text><text start="611" dur="4.92">surrounding pixels are</text><text start="613.6" dur="4.239">darker when you compute a weighted sum</text><text start="615.92" dur="4.28">like this you might come out with any</text><text start="617.839" dur="4.321">number but for this network what we want</text><text start="620.2" dur="4.639">is for activations to be some value</text><text start="622.16" dur="5.239">between 0o and one so a common thing to</text><text start="624.839" dur="4.24">do is to pump this weighted sum into</text><text start="627.399" dur="3.321">some function that squishes the real</text><text start="629.079" dur="4.161">number number line into the range</text><text start="630.72" dur="4.32">between 0 and 1 and a common function</text><text start="633.24" dur="4.8">that does this is called the sigmoid</text><text start="635.04" dur="5.12">function also known as a logistic curve</text><text start="638.04" dur="4.52">basically very negative inputs end up</text><text start="640.16" dur="4.6">close to zero very positive inputs end</text><text start="642.56" dur="5.64">up close to one and it just steadily</text><text start="644.76" dur="3.44">increases around the input</text><text start="648.36" dur="5.56">zero so the activation of the neuron</text><text start="651.16" dur="5.679">here is basically a measure of how</text><text start="653.92" dur="5.479">positive the relevant weighted sum</text><text start="656.839" dur="4.161">is but maybe it&amp;#39;s not that you want the</text><text start="659.399" dur="3.841">neuron to light up when the weighted sum</text><text start="661" dur="4.2">is bigger than zero maybe you only want</text><text start="663.24" dur="5.68">it to be active when the sum is bigger</text><text start="665.2" dur="5">than say 10 that is you want some bias</text><text start="668.92" dur="3.8">for it to be</text><text start="670.2" dur="5.36">inactive what we&amp;#39;ll do then is just add</text><text start="672.72" dur="5.119">in some other number like -10 to this</text><text start="675.56" dur="5.04">weighted sum before plugging it through</text><text start="677.839" dur="4.601">the sigmoid squish ification function</text><text start="680.6" dur="5.039">that additional number is called the</text><text start="682.44" dur="4.88">bias so the weights tell you what pixel</text><text start="685.639" dur="4.601">pattern this neuron in the second layer</text><text start="687.32" dur="5.24">is picking up on and the bias tells you</text><text start="690.24" dur="3.76">how high the weighted sum needs to be</text><text start="692.56" dur="4.76">before the neuron starts getting</text><text start="694" dur="6.639">meaningfully active and that is just one</text><text start="697.32" dur="5.92">neuron every other neuron in this layer</text><text start="700.639" dur="5.081">is going to be connected to all 784</text><text start="703.24" dur="5.48">pixel neurons from the first layer and</text><text start="705.72" dur="6.28">each one of those 784 connections has</text><text start="708.72" dur="5.64">its own weight associated with it also</text><text start="712" dur="3.959">each one has some bias some other number</text><text start="714.36" dur="4.039">that you add on to the weighted sum</text><text start="715.959" dur="4.201">before squishing it with the sigmoid and</text><text start="718.399" dur="4.281">that&amp;#39;s a lot to think about about with</text><text start="720.16" dur="7.6">this hidden layer of 16 neurons that&amp;#39;s a</text><text start="722.68" dur="7.36">total of 784 * 16 weights along with 16</text><text start="727.76" dur="3.879">biases and all of that is just the</text><text start="730.04" dur="3.76">connections from the first layer to the</text><text start="731.639" dur="4.161">second the connections between the other</text><text start="733.8" dur="5.039">layers also have a bunch of weights and</text><text start="735.8" dur="5.68">biases associated with them all said and</text><text start="738.839" dur="6.401">done this network has almost exactly</text><text start="741.48" dur="5.719">13,000 total weights and biases 13,000</text><text start="745.24" dur="4">knobs and dials that can be tweaked and</text><text start="747.199" dur="4.721">turned to make this network behave and</text><text start="749.24" dur="5.2">different ways so when we talk about</text><text start="751.92" dur="4.68">learning what that&amp;#39;s referring to is</text><text start="754.44" dur="4">getting the computer to find a valid</text><text start="756.6" dur="3.84">setting for all of these many many</text><text start="758.44" dur="3.04">numbers so that it&amp;#39;ll actually solve the</text><text start="760.44" dur="3.72">problem at</text><text start="761.48" dur="5.039">hand one thought experiment that is at</text><text start="764.16" dur="4.2">once fun and kind of horrifying is to</text><text start="766.519" dur="3.801">imagine sitting down and setting all of</text><text start="768.36" dur="3.68">these weights and biases by hand</text><text start="770.32" dur="3.959">purposefully tweaking the numbers so</text><text start="772.04" dur="5.239">that the second layer picks up on edges</text><text start="774.279" dur="4.92">the third layer picks up on patterns Etc</text><text start="777.279" dur="3.161">I personally find this satisfying rather</text><text start="779.199" dur="3.601">rather than just treating the network as</text><text start="780.44" dur="4.12">a total Black Box because when the</text><text start="782.8" dur="3.8">network doesn&amp;#39;t perform the way you</text><text start="784.56" dur="3.56">anticipate if you&amp;#39;ve built up a little</text><text start="786.6" dur="3.76">bit of a relationship with what those</text><text start="788.12" dur="4.04">weights and biases actually mean you</text><text start="790.36" dur="3.52">have a starting place for experimenting</text><text start="792.16" dur="4.28">with how to change the structure to</text><text start="793.88" dur="4.519">improve or when the network does work</text><text start="796.44" dur="3.839">but not for the reasons you might expect</text><text start="798.399" dur="3.721">digging into what the weights and biases</text><text start="800.279" dur="3.761">are doing is a good way to challenge</text><text start="802.12" dur="3.6">your assumptions and really expose the</text><text start="804.04" dur="4.32">full space of possible</text><text start="805.72" dur="4.119">solutions by the way the actual function</text><text start="808.36" dur="3.36">here is a little cumbersome to write</text><text start="809.839" dur="4.281">down don&amp;#39;t you</text><text start="811.72" dur="4.239">think so let me show you a more</text><text start="814.12" dur="3.88">notationally compact way that these</text><text start="815.959" dur="3.361">connections are represented this is how</text><text start="818" dur="3.959">you&amp;#39;d see it if you choose to read up</text><text start="819.32" dur="5.36">more about neural networks organize all</text><text start="821.959" dur="4.88">of the activations from one layer into a</text><text start="824.68" dur="4.839">column as a</text><text start="826.839" dur="5.321">vector then organize all of the weights</text><text start="829.519" dur="4.641">as a matrix where each row of that</text><text start="832.16" dur="4.239">Matrix corresponds to the connections</text><text start="834.16" dur="4.96">between one layer and a particular</text><text start="836.399" dur="4.841">neuron in the next layer what that means</text><text start="839.12" dur="3.92">is that taking the weighted sum of the</text><text start="841.24" dur="3.839">activations in the first layer according</text><text start="843.04" dur="4.52">to these weights corresponds to one of</text><text start="845.079" dur="6.12">the terms in the Matrix Vector product</text><text start="847.56" dur="3.639">of everything we have on the left</text><text start="852.92" dur="3.96">here by the way so much of machine</text><text start="855.399" dur="3.761">learning just comes down to having a</text><text start="856.88" dur="3.959">good grasp of linear algebra so for any</text><text start="859.16" dur="3.16">of you who want a nice visual</text><text start="860.839" dur="3.68">understanding for matrices and what</text><text start="862.32" dur="4.16">Matrix Vector multiplication means take</text><text start="864.519" dur="5.12">a look at the series I did on linear</text><text start="866.48" dur="4.919">algebra especially chapter 3 back to our</text><text start="869.639" dur="3.56">expression instead of talking about</text><text start="871.399" dur="4.521">adding the bias to each one of these</text><text start="873.199" dur="4.841">values independently we represent it by</text><text start="875.92" dur="4.52">organizing all those biases into a</text><text start="878.04" dur="5.479">vector and adding the entire Vector to</text><text start="880.44" dur="5.56">the previous Matrix Vector product then</text><text start="883.519" dur="5">as a final step I&amp;#39;ll wrap a sigmoid</text><text start="886" dur="3.92">around the outside here and what that&amp;#39;s</text><text start="888.519" dur="3.041">supposed to represent is that you&amp;#39;re</text><text start="889.92" dur="3.96">going to apply the sigmoid function to</text><text start="891.56" dur="3.32">each specific component of the resulting</text><text start="893.88" dur="3.6">Vector</text><text start="894.88" dur="4.56">inside so once you write down this</text><text start="897.48" dur="4.56">weight Matrix and these vectors as their</text><text start="899.44" dur="4.72">own symbols you can communicate the full</text><text start="902.04" dur="4.359">transition of activations from one layer</text><text start="904.16" dur="4.56">to the next in an extremely tight and</text><text start="906.399" dur="4.88">neat little expression and this makes</text><text start="908.72" dur="4.799">the relevant code both a lot simpler and</text><text start="911.279" dur="5.441">a lot faster since many libraries</text><text start="913.519" dur="5.281">optimize the heck out of matrix</text><text start="916.72" dur="3.88">multiplication remember how earlier I</text><text start="918.8" dur="4.44">said these neurons are simply things</text><text start="920.6" dur="4.56">that hold numbers well of course the</text><text start="923.24" dur="3.959">specific numbers that they hold depends</text><text start="925.16" dur="4.56">on the image you feed</text><text start="927.199" dur="4.961">in so it&amp;#39;s actually more accurate to</text><text start="929.72" dur="4.76">think of each neuron as a function one</text><text start="932.16" dur="4.479">that takes in the outputs of all the</text><text start="934.48" dur="5.039">neurons in the previous layer and spits</text><text start="936.639" dur="4.921">out a number between 0 and one really</text><text start="939.519" dur="4.601">the entire network is just a function</text><text start="941.56" dur="5.279">one that takes in 784 numbers as an</text><text start="944.12" dur="5.079">input and spits out 10 numbers as an</text><text start="946.839" dur="4.601">output it&amp;#39;s an absurdly complicated</text><text start="949.199" dur="3.921">function one that involves 13,000</text><text start="951.44" dur="3.399">parameters in the forms of these weights</text><text start="953.12" dur="3.6">and biases that pick up on certain</text><text start="954.839" dur="3.68">patterns and which involves iterating</text><text start="956.72" dur="4.679">many Matrix Vector products and the</text><text start="958.519" dur="5.961">sigmo squish ification function but it&amp;#39;s</text><text start="961.399" dur="4.841">just a function nonetheless and in a way</text><text start="964.48" dur="3.64">it&amp;#39;s kind of reassuring that it looks</text><text start="966.24" dur="3.839">complicated I mean if it were any</text><text start="968.12" dur="3.079">simpler what hope would we have that it</text><text start="970.079" dur="3.88">could take on the challenge of</text><text start="971.199" dur="4.921">recognizing digits and how does it take</text><text start="973.959" dur="4.281">on that challenge how does this network</text><text start="976.12" dur="4.56">learn the appropriate weights and biases</text><text start="978.24" dur="4.159">just by looking at data well that&amp;#39;s what</text><text start="980.68" dur="3.44">I&amp;#39;ll show in the next video and I&amp;#39;ll</text><text start="982.399" dur="3.12">also dig a little more into what this</text><text start="984.12" dur="2.36">particular Network we&amp;#39;re seeing is</text><text start="985.519" dur="3.041">really</text><text start="986.48" dur="3.44">doing now is the point I suppose I</text><text start="988.56" dur="3.199">should say say subscribe to stay</text><text start="989.92" dur="4.2">notified about when that video or any</text><text start="991.759" dur="3.88">new videos come out but realistically</text><text start="994.12" dur="4.079">most of you don&amp;#39;t actually receive</text><text start="995.639" dur="4.481">notifications from YouTube do you maybe</text><text start="998.199" dur="3.681">more honestly I should say subscribe so</text><text start="1000.12" dur="3.8">that the neural networks that underly</text><text start="1001.88" dur="3.84">YouTube&amp;#39;s recommendation algorithm are</text><text start="1003.92" dur="3">primed to believe that you want to see</text><text start="1005.72" dur="3.72">content from this channel get</text><text start="1006.92" dur="5.12">recommended to you anyway stay posted</text><text start="1009.44" dur="4.68">for more thank you very much to everyone</text><text start="1012.04" dur="3.479">supporting these videos on patreon I&amp;#39;ve</text><text start="1014.12" dur="3.36">been a little slow to progress in the</text><text start="1015.519" dur="3.8">probability Series this summer but I&amp;#39;m</text><text start="1017.48" dur="5.159">jumping back into it after this project</text><text start="1019.319" dur="5.681">so patrons you can look out for updates</text><text start="1022.639" dur="4.601">there to close things off here I have</text><text start="1025" dur="4">with me Lea Lee who did her PhD work on</text><text start="1027.24" dur="3.04">the theoretical side of deep learning</text><text start="1029" dur="3.48">and who currently works at a venture</text><text start="1030.28" dur="3.759">capital firm called amplify Partners who</text><text start="1032.48" dur="4.359">kindly provided some of the funding for</text><text start="1034.039" dur="4.721">this video so Leisa one thing I think we</text><text start="1036.839" dur="3.681">should quickly bring up is this sigmoid</text><text start="1038.76" dur="3.559">function as I understand it early</text><text start="1040.52" dur="3.679">networks use this to squish the relevant</text><text start="1042.319" dur="4.041">weighted sum into that interval between</text><text start="1044.199" dur="4.081">0 and one you know kind of motivated by</text><text start="1046.36" dur="4.24">this biological analogy of neurons</text><text start="1048.28" dur="4.399">either being in active or active exactly</text><text start="1050.6" dur="3.959">but relatively few modern networks</text><text start="1052.679" dur="4.521">actually use sigmoid anymore that&amp;#39;s kind</text><text start="1054.559" dur="5.441">of old school right yeah or rather reu</text><text start="1057.2" dur="5.839">seems to be much easier to train and reu</text><text start="1060" dur="4.72">reu stands for rectified linear unit yes</text><text start="1063.039" dur="4.921">it&amp;#39;s this kind of function where you</text><text start="1064.72" dur="5.72">just taking a Max of zero and a where a</text><text start="1067.96" dur="4.52">is given by what you were explaining in</text><text start="1070.44" dur="4.68">the video and what this was sort of</text><text start="1072.48" dur="6.8">motivated from I think was a partially</text><text start="1075.12" dur="6.559">by a biological analogy with how neurons</text><text start="1079.28" dur="4.6">would either be activated or not and so</text><text start="1081.679" dur="4.88">if it passes a certain threshold it</text><text start="1083.88" dur="4.44">would be the identity function but if it</text><text start="1086.559" dur="3.401">did not then it would just not be</text><text start="1088.32" dur="4.16">activated so it be zero so it&amp;#39;s kind of</text><text start="1089.96" dur="4.12">a simplification using sigmoids didn&amp;#39;t</text><text start="1092.48" dur="4.16">help training or it was very difficult</text><text start="1094.08" dur="5.4">to train it&amp;#39;s at some point and people</text><text start="1096.64" dur="6.399">just tried ra you and it happened to</text><text start="1099.48" dur="6.079">work very well for these incredibly um</text><text start="1103.039" dur="4.931">deep neural networks all right thank you</text><text start="1105.559" dur="15.68">Alicia</text><text start="1107.97" dur="13.269">[Music]</text></transcript>