<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0.08" dur="5.48">the initials GPT stand for generative</text><text start="3.04" dur="4.2">pre-trained Transformer so that first</text><text start="5.56" dur="4.84">word is straightforward enough these are</text><text start="7.24" dur="4.84">Bots that generate new text pre-trained</text><text start="10.4" dur="3.399">refers to how the model went through a</text><text start="12.08" dur="4.199">process of learning from a massive</text><text start="13.799" dur="4.081">amount of data and the prefix insinuates</text><text start="16.279" dur="3.361">that there&amp;#39;s more room to fine-tune it</text><text start="17.88" dur="4.12">on specific tasks with additional</text><text start="19.64" dur="4.879">training but the last word that&amp;#39;s the</text><text start="22" dur="4.4">real key piece a Transformer is a</text><text start="24.519" dur="4.16">specific kind of neural network a</text><text start="26.4" dur="4.279">machine learning model and it&amp;#39;s the core</text><text start="28.679" dur="4.4">invention underlying the current boom in</text><text start="30.679" dur="4.481">AI what I want to do with this video and</text><text start="33.079" dur="4.041">the following chapters is go through a</text><text start="35.16" dur="4.32">visually driven explanation for what</text><text start="37.12" dur="3.68">actually happens inside a Transformer</text><text start="39.48" dur="3.919">we&amp;#39;re going to follow the data that</text><text start="40.8" dur="4.52">flows through it and go step by step</text><text start="43.399" dur="4.281">there are many different kinds of models</text><text start="45.32" dur="4.96">that you can build using Transformers</text><text start="47.68" dur="4.48">some models take in audio and produce a</text><text start="50.28" dur="3.56">transcript this sentence comes from a</text><text start="52.16" dur="3.68">model going the other way around</text><text start="53.84" dur="4.12">producing synthetic speech just from</text><text start="55.84" dur="4.399">text all those tools that took the World</text><text start="57.96" dur="4.119">by storm in 2022 like doll in</text><text start="60.239" dur="4.081">mid-journey that take in a text</text><text start="62.079" dur="4.281">description and produce an image are</text><text start="64.32" dur="3.76">based on Transformers and even if I</text><text start="66.36" dur="3.84">can&amp;#39;t quite get it to understand what a</text><text start="68.08" dur="3.8">pie creature is supposed to be I&amp;#39;m still</text><text start="70.2" dur="4.32">blown away that this kind of thing is</text><text start="71.88" dur="5.52">even remotely possible and the original</text><text start="74.52" dur="4.84">Transformer introduced in 2017 by Google</text><text start="77.4" dur="3.96">was invented for the specific use case</text><text start="79.36" dur="4.28">of translating text from one language</text><text start="81.36" dur="4.2">into another but the variant that you</text><text start="83.64" dur="4.4">and I will focus on which is the type</text><text start="85.56" dur="4.599">that underlies tools like chat GPT will</text><text start="88.04" dur="4.119">be a model that&amp;#39;s trained to take in in</text><text start="90.159" dur="4.161">a piece of text maybe even with some</text><text start="92.159" dur="4.761">surrounding images or sound accompanying</text><text start="94.32" dur="4.36">it and produce a prediction for what</text><text start="96.92" dur="3.08">comes next in the passage that</text><text start="98.68" dur="3.28">prediction takes the form of a</text><text start="100" dur="3.88">probability distribution over many</text><text start="101.96" dur="4.08">different chunks of text that might</text><text start="103.88" dur="3.72">follow at first glance you might think</text><text start="106.04" dur="3.24">that predicting the next word feels like</text><text start="107.6" dur="4.24">a very different goal from generating</text><text start="109.28" dur="4.28">new text but once you have a prediction</text><text start="111.84" dur="3.599">model like this a simple thing you could</text><text start="113.56" dur="4.199">try to make it generate a longer piece</text><text start="115.439" dur="4.64">of text is to give it an initial snippet</text><text start="117.759" dur="3.72">to work with have it take a random samp</text><text start="120.079" dur="4.04">Le from the distribution it just</text><text start="121.479" dur="4.201">generated append that sample to the text</text><text start="124.119" dur="3.401">and then run the whole process again to</text><text start="125.68" dur="4.4">make a new prediction based on all the</text><text start="127.52" dur="3.719">new text including what it just added I</text><text start="130.08" dur="2.6">don&amp;#39;t know about you but it really</text><text start="131.239" dur="3.561">doesn&amp;#39;t feel like this should actually</text><text start="132.68" dur="5.08">work in this animation for example I&amp;#39;m</text><text start="134.8" dur="4.6">running gpt2 on my laptop and having it</text><text start="137.76" dur="3.72">repeatedly predict and Sample the next</text><text start="139.4" dur="4.72">chunk of text to generate a story based</text><text start="141.48" dur="4.28">on the seed text and the story just</text><text start="144.12" dur="4.52">doesn&amp;#39;t actually really make that much</text><text start="145.76" dur="5.759">sense but if I swap it out for API calls</text><text start="148.64" dur="5.4">to gpt3 in instead which is the same</text><text start="151.519" dur="4.481">basic model just much bigger suddenly</text><text start="154.04" dur="4.04">almost magically we do get a sensible</text><text start="156" dur="3.76">Story one that even seems to infer that</text><text start="158.08" dur="4.159">a pie creature would live in a Land of</text><text start="159.76" dur="4.64">math and computation this process here</text><text start="162.239" dur="3.681">of repeated prediction and sampling is</text><text start="164.4" dur="3.96">essentially what&amp;#39;s happening when you</text><text start="165.92" dur="4.36">interact with chat GPT or any of these</text><text start="168.36" dur="4.239">other large language models and you see</text><text start="170.28" dur="4.039">them producing one word at a time in</text><text start="172.599" dur="3.56">fact one feature that I would very much</text><text start="174.319" dur="3.92">enjoy is the ability to see the</text><text start="176.159" dur="5.641">underlying distribution for each new</text><text start="178.239" dur="3.561">word that it chooses</text><text start="184.04" dur="3.44">let&amp;#39;s kick things off with a very high</text><text start="185.68" dur="3.96">level preview of how data flows through</text><text start="187.48" dur="4">a Transformer we will spend much more</text><text start="189.64" dur="3.84">time motivating and interpreting and</text><text start="191.48" dur="3.72">expanding on the details of each step</text><text start="193.48" dur="3.92">but in Broad Strokes when one of these</text><text start="195.2" dur="4.52">chat Bots generates a given word here&amp;#39;s</text><text start="197.4" dur="3.88">what&amp;#39;s going on under the hood first the</text><text start="199.72" dur="3.68">input is broken up into a bunch of</text><text start="201.28" dur="4.239">little pieces these pieces are called</text><text start="203.4" dur="4.24">tokens and in the case of text these</text><text start="205.519" dur="3.681">tend to be words or little pieces of</text><text start="207.64" dur="3.08">words or other common character</text><text start="209.2" dur="3.84">combination</text><text start="210.72" dur="4.28">if images or sound are involved then</text><text start="213.04" dur="4.399">tokens could be little patches of that</text><text start="215" dur="4.04">image or little chunks of that sound</text><text start="217.439" dur="4">each one of these tokens is then</text><text start="219.04" dur="4.08">associated with a vector meaning some</text><text start="221.439" dur="4.44">list of numbers which is meant to</text><text start="223.12" dur="4.16">somehow encode the meaning of that piece</text><text start="225.879" dur="2.801">if you think of these vectors as giving</text><text start="227.28" dur="3.64">coordinates in some very high</text><text start="228.68" dur="4.24">dimensional space words with similar</text><text start="230.92" dur="4.319">meanings tend to land on vectors that</text><text start="232.92" dur="4.08">are close to each other in that space</text><text start="235.239" dur="3.401">this sequence of vectors then passes</text><text start="237" dur="3.439">through an operation that&amp;#39;s known as an</text><text start="238.64" dur="3.56">attention block and this allows the</text><text start="240.439" dur="3.401">vectors to talk to each other and pass</text><text start="242.2" dur="3.759">information back and forth to update</text><text start="243.84" dur="4.319">their values for example the meaning of</text><text start="245.959" dur="4.081">the word model in the phrase a machine</text><text start="248.159" dur="3.961">learning model is different from its</text><text start="250.04" dur="3.36">meaning in the phrase a fashion model</text><text start="252.12" dur="3.399">the attention block is what&amp;#39;s</text><text start="253.4" dur="4.079">responsible for figuring out which words</text><text start="255.519" dur="4.44">in the context are relevant to updating</text><text start="257.479" dur="4">the meanings of which other words and</text><text start="259.959" dur="3.561">how exactly those meanings should be</text><text start="261.479" dur="4.361">updated and again whenever I use the</text><text start="263.52" dur="4.64">word meaning this is somehow entirely</text><text start="265.84" dur="4.48">encoded in the entries of those</text><text start="268.16" dur="3.479">vectors after that the these vectors</text><text start="270.32" dur="3.12">pass through a different kind of</text><text start="271.639" dur="2.681">operation and depending on the source</text><text start="273.44" dur="3.12">that you&amp;#39;re reading this will be</text><text start="274.32" dur="4.64">referred to as a multi-layer perceptron</text><text start="276.56" dur="3.96">or maybe a feed forward layer and here</text><text start="278.96" dur="3">the vectors don&amp;#39;t talk to each other</text><text start="280.52" dur="3.6">they all go through the same operation</text><text start="281.96" dur="4.04">in parallel and while this block is a</text><text start="284.12" dur="3.359">little bit harder to interpret later on</text><text start="286" dur="3.28">we&amp;#39;ll talk about how the step is a</text><text start="287.479" dur="4.16">little bit like asking a long list of</text><text start="289.28" dur="4">questions about each vector and then</text><text start="291.639" dur="4.601">updating them based on the answers to</text><text start="293.28" dur="5.24">those questions all of the operations in</text><text start="296.24" dur="4.76">both of these blocks look like a giant</text><text start="298.52" dur="4.84">pile of Matrix multiplic ations and our</text><text start="301" dur="4.88">primary job is going to be to understand</text><text start="303.36" dur="4.76">how to read the underlying</text><text start="305.88" dur="4">matrices I&amp;#39;m glossing over some details</text><text start="308.12" dur="3.88">about some normalization steps that</text><text start="309.88" dur="4.319">happen in between but this is after all</text><text start="312" dur="4.12">a high level preview after that the</text><text start="314.199" dur="3.801">process essentially repeats you go back</text><text start="316.12" dur="4.84">and forth between attention blocks and</text><text start="318" dur="5.12">multi-layer perceptron blocks until at</text><text start="320.96" dur="4.2">the very end the hope is that all of the</text><text start="323.12" dur="4.16">essential meaning of the passage has</text><text start="325.16" dur="4.56">somehow been baked into the very last</text><text start="327.28" dur="4.44">Vector in the sequence we then perform a</text><text start="329.72" dur="4.199">certain operation on that last Vector</text><text start="331.72" dur="4.36">that produces a probability distribution</text><text start="333.919" dur="4.081">over all possible tokens all possible</text><text start="336.08" dur="4.48">little chunks of text that might come</text><text start="338" dur="4.4">next and like I said once you have a</text><text start="340.56" dur="3.88">tool that predicts what comes next given</text><text start="342.4" dur="3.88">a snippet of text you can feed it a</text><text start="344.44" dur="3.72">little bit of seed text and have it</text><text start="346.28" dur="3.56">repeatedly play this game of predicting</text><text start="348.16" dur="3.8">what comes next sampling from the</text><text start="349.84" dur="4.4">distribution appending it and then</text><text start="351.96" dur="4.359">repeating over and over some of you in</text><text start="354.24" dur="4.32">the no may remember how long before chat</text><text start="356.319" dur="4.761">GPT came into the scene this is what</text><text start="358.56" dur="4.199">early demos of GPT 3 looked like you</text><text start="361.08" dur="4.6">would have it autocomplete stories and</text><text start="362.759" dur="5.521">essays based on an initial snippet to</text><text start="365.68" dur="4.239">make a tool like this into a chatbot the</text><text start="368.28" dur="3.4">easiest starting point is to have a</text><text start="369.919" dur="3.801">little bit of text that establishes the</text><text start="371.68" dur="4.079">setting of a user interacting with a</text><text start="373.72" dur="4.319">helpful AI assistant what you would call</text><text start="375.759" dur="4.641">the system prompt and then you would use</text><text start="378.039" dur="4.241">the user&amp;#39;s initial question or prompt as</text><text start="380.4" dur="3.56">the first bit of dialogue and then you</text><text start="382.28" dur="4.479">have it start predicting what such a</text><text start="383.96" dur="4.959">helpful AI assistant would say in</text><text start="386.759" dur="3.801">response there is more to say about an</text><text start="388.919" dur="3.4">added step of training that&amp;#39;s required</text><text start="390.56" dur="4.4">to make this work well but at a high</text><text start="392.319" dur="5.081">level this is the general</text><text start="394.96" dur="4.32">idea in this chapter you and I are going</text><text start="397.4" dur="4.04">to expand on the details of what happens</text><text start="399.28" dur="4.039">at the very beginning of the network at</text><text start="401.44" dur="3.4">the very end of the network and I also</text><text start="403.319" dur="2.841">want to spend a lot of time reviewing</text><text start="404.84" dur="3.12">some important bits of background</text><text start="406.16" dur="3.8">knowledge things that would have been</text><text start="407.96" dur="4.16">second nature to any machine learning</text><text start="409.96" dur="4.2">engineer by the time Transformers came</text><text start="412.12" dur="3.079">around if you&amp;#39;re comfortable with that</text><text start="414.16" dur="2.56">background knowledge and a little</text><text start="415.199" dur="3.081">impatient you could probably feel free</text><text start="416.72" dur="3.8">to skip to the next chapter which is</text><text start="418.28" dur="4.199">going to focus on the atten blocks</text><text start="420.52" dur="3.92">generally considered the heart of the</text><text start="422.479" dur="3.761">Transformer after that I want to talk</text><text start="424.44" dur="4.199">more about these multi-layer perceptron</text><text start="426.24" dur="4.079">blocks how training works and a number</text><text start="428.639" dur="4.041">of other details that will have been</text><text start="430.319" dur="4.481">skipped up to that point for broader</text><text start="432.68" dur="4.6">context these videos are additions to a</text><text start="434.8" dur="3.839">minseries about deep learning and it&amp;#39;s</text><text start="437.28" dur="3.44">okay if you haven&amp;#39;t watched the previous</text><text start="438.639" dur="3.84">ones I think you can do it out of order</text><text start="440.72" dur="3.56">but before diving into Transformers</text><text start="442.479" dur="3.44">specifically I do think it&amp;#39;s worth</text><text start="444.28" dur="3.599">making sure that we&amp;#39;re on the same page</text><text start="445.919" dur="4.28">about the basic premise and structure of</text><text start="447.879" dur="4.44">deep learning at the risk of stating the</text><text start="450.199" dur="4.521">obvious this is one approach to machine</text><text start="452.319" dur="4.801">learning which describes any model where</text><text start="454.72" dur="5.159">you&amp;#39;re using data to somehow determine</text><text start="457.12" dur="4.32">how a model behaves what I mean by that</text><text start="459.879" dur="3.401">is let&amp;#39;s say you want a function that</text><text start="461.44" dur="3.92">takes in an image and it produces a</text><text start="463.28" dur="4.08">label describing it or our example of</text><text start="465.36" dur="4.44">predicting the next word given a passage</text><text start="467.36" dur="4.48">of text or any other task that seems to</text><text start="469.8" dur="4.079">require some element of intuition and</text><text start="471.84" dur="3.759">pattern recognition we almost take this</text><text start="473.879" dur="3.28">for granted these days but the idea with</text><text start="475.599" dur="3.921">machine learning is that rather than</text><text start="477.159" dur="4.44">trying to explicitly Define a procedure</text><text start="479.52" dur="3.2">for how to do that task in code which is</text><text start="481.599" dur="3.961">what people would have done in the</text><text start="482.72" dur="4.759">earliest days of AI instead you set up a</text><text start="485.56" dur="3.72">very flexible structure with tunable</text><text start="487.479" dur="4.84">parameters like a bunch of knobs and</text><text start="489.28" dur="4.879">dials and then somehow you use many</text><text start="492.319" dur="4.681">examples of what the output should look</text><text start="494.159" dur="4.921">like for a given input to tweak and tune</text><text start="497" dur="4.4">the values of those parameters to mimic</text><text start="499.08" dur="4.399">this behavior for example maybe the</text><text start="501.4" dur="4.28">simplest form of machine learning is</text><text start="503.479" dur="4.44">linear regression where your inputs and</text><text start="505.68" dur="3.88">your outputs are each single numbers</text><text start="507.919" dur="4.12">something like the square footage of a</text><text start="509.56" dur="4.8">house and its price and what you want is</text><text start="512.039" dur="4.401">to find a line of best fit through this</text><text start="514.36" dur="4.679">data you know to predict future house</text><text start="516.44" dur="5.039">prices that line is described by two</text><text start="519.039" dur="4.56">continuous parameters say the slope and</text><text start="521.479" dur="4.48">the Y intercept and the goal of linear</text><text start="523.599" dur="5.201">regression is to determine those</text><text start="525.959" dur="5.121">parameters to closely match the data</text><text start="528.8" dur="5.36">needless to say deep learning models get</text><text start="531.08" dur="7.04">much more complicated gpt3 for example</text><text start="534.16" dur="5.799">has not two but 175 billion parameters</text><text start="538.12" dur="3.6">but here&amp;#39;s the thing it&amp;#39;s it&amp;#39;s not a</text><text start="539.959" dur="4">given that you can create some giant</text><text start="541.72" dur="4.2">model with a huge number of parameters</text><text start="543.959" dur="4.44">without it either grossly overfitting</text><text start="545.92" dur="5.12">the training data or being completely</text><text start="548.399" dur="4.841">intractable to train deep learning</text><text start="551.04" dur="4.239">describes a class of models that in the</text><text start="553.24" dur="4.24">last couple decades have proven to scale</text><text start="555.279" dur="3.68">remarkably well what unifies them is</text><text start="557.48" dur="3.479">that they all use the same training</text><text start="558.959" dur="4.12">algorithm it&amp;#39;s called back propagation</text><text start="560.959" dur="3.721">we talked about it in previous chapters</text><text start="563.079" dur="3.401">and the context that I want you to have</text><text start="564.68" dur="4">as we go in is that in order for this</text><text start="566.48" dur="3.84">training algorithm to work well at scale</text><text start="568.68" dur="3.88">these models have to follow a certain</text><text start="570.32" dur="4.56">specific format and if you know this</text><text start="572.56" dur="4.04">format going in it helps to explain many</text><text start="574.88" dur="3.88">of the choices for how a Transformer</text><text start="576.6" dur="3.84">processes language which otherwise run</text><text start="578.76" dur="4.04">the risk of feeling kind of</text><text start="580.44" dur="4">arbitrary first whatever kind of model</text><text start="582.8" dur="4.44">you&amp;#39;re making the input has to be</text><text start="584.44" dur="4.8">formatted as an array of real numbers</text><text start="587.24" dur="4.12">this could simply mean a list of numbers</text><text start="589.24" dur="3.56">it could be a two-dimensional array or</text><text start="591.36" dur="3.12">very often you deal with higher</text><text start="592.8" dur="4.36">dimensional arrays where the general</text><text start="594.48" dur="4.52">term used is tensor you often think of</text><text start="597.16" dur="5.119">that input data as being progressively</text><text start="599" dur="4.72">TR transformed into many distinct layers</text><text start="602.279" dur="3.161">where again each layer is always</text><text start="603.72" dur="3.76">structured as some kind of array of real</text><text start="605.44" dur="3.92">numbers until you get to a final layer</text><text start="607.48" dur="3.76">which you consider the output for</text><text start="609.36" dur="3.919">example the final layer in our text</text><text start="611.24" dur="3.44">processing model is a list of numbers</text><text start="613.279" dur="3.321">representing the probability</text><text start="614.68" dur="4.44">distribution for all possible next</text><text start="616.6" dur="4.479">tokens in deep learning these model</text><text start="619.12" dur="4">parameters are almost always referred to</text><text start="621.079" dur="4.041">as weights and this is because a key</text><text start="623.12" dur="3.8">feature of these models is that the only</text><text start="625.12" dur="4.399">way these parameters interact with the</text><text start="626.92" dur="4.68">data being processed is through weighted</text><text start="629.519" dur="3.801">sums you also sprinkle some nonlinear</text><text start="631.6" dur="4.52">functions throughout but they won&amp;#39;t</text><text start="633.32" dur="4.72">depend on parameters typically though</text><text start="636.12" dur="3.8">instead of seeing the weighted sums all</text><text start="638.04" dur="4.12">naked and written out explicitly like</text><text start="639.92" dur="4.4">this you&amp;#39;ll instead find them packaged</text><text start="642.16" dur="3.479">together as various components in a</text><text start="644.32" dur="3.6">matrix Vector</text><text start="645.639" dur="3.88">product it amounts to saying the same</text><text start="647.92" dur="3.56">thing if you think back to how Matrix</text><text start="649.519" dur="3.961">Vector multiplication Works each</text><text start="651.48" dur="4.159">component in the output it looks like a</text><text start="653.48" dur="3.96">weighted sum it&amp;#39;s just often</text><text start="655.639" dur="3.961">conceptually cleaner for you and me to</text><text start="657.44" dur="4.76">think about matrices that are filled</text><text start="659.6" dur="4.88">with tunable parameters that transform</text><text start="662.2" dur="3.24">vectors that are drawn from the data</text><text start="664.48" dur="4.24">being</text><text start="665.44" dur="6.399">processed for example those 175 billion</text><text start="668.72" dur="6.04">weights in gpt3 are organized into just</text><text start="671.839" dur="4.601">under 28,000 distinct matrices those</text><text start="674.76" dur="3.4">matrices in turn fall into eight</text><text start="676.44" dur="3.399">different categories and what you and I</text><text start="678.16" dur="3.6">are going to do is Step through each one</text><text start="679.839" dur="4.321">of those categories to understand what</text><text start="681.76" dur="3.759">that type does as we go through I think</text><text start="684.16" dur="4.119">it&amp;#39;s kind of fun to reference the</text><text start="685.519" dur="6.361">specific numbers from gpt3 to count up</text><text start="688.279" dur="5.441">exactly where those 75 billion come from</text><text start="691.88" dur="3.88">even if nowadays there are bigger and</text><text start="693.72" dur="3.76">better models this one has a certain</text><text start="695.76" dur="3.48">charm as the first large language model</text><text start="697.48" dur="4.24">to really capture the world&amp;#39;s attention</text><text start="699.24" dur="4.08">outside of ml communities also</text><text start="701.72" dur="3.08">practically speaking companies tend to</text><text start="703.32" dur="2.959">keep much tighter lips around the</text><text start="704.8" dur="3.52">specific numbers for more modern</text><text start="706.279" dur="4.041">networks I just want to set the scene</text><text start="708.32" dur="3.759">going in that as you peek under the hood</text><text start="710.32" dur="4.36">to see what happens inside a tool like</text><text start="712.079" dur="4.76">chat GPT almost all of the actual</text><text start="714.68" dur="3.92">computation looks like Matrix Vector</text><text start="716.839" dur="3.761">multiplication there&amp;#39;s a little bit of a</text><text start="718.6" dur="4.08">risk getting lost in the sea of billions</text><text start="720.6" dur="4.28">of numbers but you should draw a very</text><text start="722.68" dur="4.159">sharp distinction in your mind between</text><text start="724.88" dur="4.399">the weights of the model which I&amp;#39;ll</text><text start="726.839" dur="4.481">always color in blue or red and the data</text><text start="729.279" dur="4.041">being processed which I&amp;#39;ll always color</text><text start="731.32" dur="4.12">in Gray the weights are the actual</text><text start="733.32" dur="3.959">brains they are the things learned</text><text start="735.44" dur="4.36">during training and they determine how</text><text start="737.279" dur="4.961">it behaves the data being processed</text><text start="739.8" dur="5">simply encodes whatever specific input</text><text start="742.24" dur="4.36">is fed into the model for a given run</text><text start="744.8" dur="4.399">like an example snippet of</text><text start="746.6" dur="4.4">text with all of that as Foundation</text><text start="749.199" dur="3.76">let&amp;#39;s dig into the first step of this</text><text start="751" dur="3.76">text processing example which is to</text><text start="752.959" dur="3.961">break up the input into little chunks</text><text start="754.76" dur="3.56">and turn those chunks into vectors I</text><text start="756.92" dur="3.76">mentioned how those chunks are called</text><text start="758.32" dur="4.36">tokens which might be pieces of words or</text><text start="760.68" dur="3.56">punctuation but every now and then in</text><text start="762.68" dur="3.599">this chapter and especially in the next</text><text start="764.24" dur="4.64">one I&amp;#39;d like to just pretend that it&amp;#39;s</text><text start="766.279" dur="4.321">broken more cleanly into words because</text><text start="768.88" dur="3.24">we humans think in words this will just</text><text start="770.6" dur="5.039">make it much easier to reference little</text><text start="772.12" dur="6.079">examples and clarify each step the model</text><text start="775.639" dur="5.56">has a predefined vocabulary some list of</text><text start="778.199" dur="4.281">all possible words say 50,000 of them</text><text start="781.199" dur="3.88">and the first Matrix that we&amp;#39;ll</text><text start="782.48" dur="4.52">encounter known as the embedding Matrix</text><text start="785.079" dur="2.801">has a single column for each one of</text><text start="787" dur="3.6">these</text><text start="787.88" dur="5.12">words these columns are what determines</text><text start="790.6" dur="3.359">what Vector each word turns into in that</text><text start="793" dur="4.6">first</text><text start="793.959" dur="6.521">step we label it we and like all the</text><text start="797.6" dur="4.88">matrices we see its values begin random</text><text start="800.48" dur="4.84">but they&amp;#39;re going to be learned Based on</text><text start="802.48" dur="4.56">data turning words into vectors was</text><text start="805.32" dur="3.84">common practice in machine learning long</text><text start="807.04" dur="3.56">before Transformers but it&amp;#39;s a little</text><text start="809.16" dur="3.479">little weird if you&amp;#39;ve never seen it</text><text start="810.6" dur="3.72">before and it sets the foundation for</text><text start="812.639" dur="3.721">everything that follows so let&amp;#39;s take a</text><text start="814.32" dur="4.4">moment to get familiar with it we often</text><text start="816.36" dur="3.96">call this embedding a word which invites</text><text start="818.72" dur="3.64">you to think of these vectors very</text><text start="820.32" dur="4.44">geometrically as points in some</text><text start="822.36" dur="4.36">high-dimensional space visualizing a</text><text start="824.76" dur="4.319">list of three numbers as coordinates for</text><text start="826.72" dur="4.28">points in 3D space would be no problem</text><text start="829.079" dur="5.12">but word embeddings tend to be much much</text><text start="831" dur="6.24">higher dimensional in gpt3 they have</text><text start="834.199" dur="4.64">12,288 dimensions and as you&amp;#39;ll see it</text><text start="837.24" dur="4.36">matters to work in a space that has a</text><text start="838.839" dur="3.68">lot of distinct directions in the same</text><text start="841.6" dur="3.4">way that you could take a</text><text start="842.519" dur="4.201">two-dimensional slice through a 3D space</text><text start="845" dur="4.12">and project all the points onto that</text><text start="846.72" dur="4.16">slice for the sake of animating word</text><text start="849.12" dur="3.88">embeddings that a simple model is giving</text><text start="850.88" dur="4.199">me I&amp;#39;m going to do an analogous Thing by</text><text start="853" dur="3.88">choosing a three-dimensional slice</text><text start="855.079" dur="3.361">through this very high dimensional space</text><text start="856.88" dur="4.48">and projecting the word vectors down</text><text start="858.44" dur="5.319">onto that and displaying the results the</text><text start="861.36" dur="4.76">big idea here is that as a model tweaks</text><text start="863.759" dur="4.2">and tunes its weights to determine how</text><text start="866.12" dur="4.12">exactly words get embedded as vectors</text><text start="867.959" dur="4.201">during training it tends to settle on a</text><text start="870.24" dur="3.839">set of embeddings where directions in</text><text start="872.16" dur="4.119">the space have a kind of semantic</text><text start="874.079" dur="4.361">meaning for the simple word to Vector</text><text start="876.279" dur="3.881">model I&amp;#39;m running here if I run a search</text><text start="878.44" dur="4.16">for all the words whose embeddings are</text><text start="880.16" dur="4.32">closest to that of tower you&amp;#39;ll notice</text><text start="882.6" dur="4.32">how they all seem to give very similar</text><text start="884.48" dur="4.32">Tower is Vibes and if you want to pull</text><text start="886.92" dur="3.279">up some Python and play along at home</text><text start="888.8" dur="3.36">this is the specific model that I&amp;#39;m</text><text start="890.199" dur="3.56">using to make the animations it&amp;#39;s not a</text><text start="892.16" dur="3.599">Transformer but it&amp;#39;s enough to</text><text start="893.759" dur="4.681">illustrate the idea that directions in</text><text start="895.759" dur="4.801">the space can carry semantic meaning a</text><text start="898.44" dur="3.959">very classic classic example of this is</text><text start="900.56" dur="4.04">how if you take the difference between</text><text start="902.399" dur="3.841">the vectors for woman and man something</text><text start="904.6" dur="3.56">you would visualize as a little Vector</text><text start="906.24" dur="3.959">in the space connecting the tip of one</text><text start="908.16" dur="4.28">to the tip of the other it&amp;#39;s very</text><text start="910.199" dur="3.801">similar to the difference between king</text><text start="912.44" dur="3.839">and</text><text start="914" dur="4.56">queen so let&amp;#39;s say you didn&amp;#39;t know the</text><text start="916.279" dur="5.201">word for a female Monarch you could find</text><text start="918.56" dur="5.079">it by taking King adding this woman</text><text start="921.48" dur="4.44">minus man Direction and searching for</text><text start="923.639" dur="5.161">the embeddings closest to that</text><text start="925.92" dur="4.44">point at least kind of despite this</text><text start="928.8" dur="2.719">being being a classic example for the</text><text start="930.36" dur="3">model I&amp;#39;m playing with the true</text><text start="931.519" dur="3.68">embedding of Queen is actually a little</text><text start="933.36" dur="3.719">farther off than this would suggest</text><text start="935.199" dur="4.281">presumably because the way that Queen is</text><text start="937.079" dur="4.88">used in training data is not merely a</text><text start="939.48" dur="4.2">feminine version of King when I played</text><text start="941.959" dur="4.36">around family relations seemed to</text><text start="943.68" dur="4.36">illustrate the idea much better the</text><text start="946.319" dur="3.401">point is it looks like during training</text><text start="948.04" dur="3.44">the model found it advantageous to</text><text start="949.72" dur="6.039">choose embeddings such that One</text><text start="951.48" dur="6.32">Direction in this space encodes gender</text><text start="955.759" dur="4.2">information another example is that if</text><text start="957.8" dur="4.32">you take the embedding of Italy and you</text><text start="959.959" dur="4.041">subtract the embedding of Germany and</text><text start="962.12" dur="4.48">then you add that to the embedding of</text><text start="964" dur="5.04">Hitler you get something very close to</text><text start="966.6" dur="4.08">the embedding of musolini it&amp;#39;s as if the</text><text start="969.04" dur="4.64">model learned to associate some</text><text start="970.68" dur="6.2">directions with italianness and others</text><text start="973.68" dur="5.32">with World War II AIS leaders maybe my</text><text start="976.88" dur="3.92">favorite example in this vein is how in</text><text start="979" dur="4.36">some models if you take the difference</text><text start="980.8" dur="5.64">between Germany and Japan and you add it</text><text start="983.36" dur="5.2">to Sushi you end up very close to broadw</text><text start="986.44" dur="3.759">worst also in playing this game of</text><text start="988.56" dur="4">finding nearest neighbors I was very</text><text start="990.199" dur="4.841">pleased to see how close cat was to both</text><text start="992.56" dur="4.16">beast and monster one bit of</text><text start="995.04" dur="3.239">mathematical intuition that&amp;#39;s helpful to</text><text start="996.72" dur="3.76">have in mind especially for the next</text><text start="998.279" dur="3.961">chapter is how the dot product of two</text><text start="1000.48" dur="4.2">vectors can be thought of as a way to</text><text start="1002.24" dur="4.56">measure how well they align</text><text start="1004.68" dur="3.88">computationally dot products involve</text><text start="1006.8" dur="4.08">multiplying all the corresponding</text><text start="1008.56" dur="3.839">components and then adding the results</text><text start="1010.88" dur="3.04">which is good since so much of our</text><text start="1012.399" dur="4.68">computation has to look like weighted</text><text start="1013.92" dur="5.44">sums geometrically the dot product is</text><text start="1017.079" dur="4.56">positive when vectors point in similar</text><text start="1019.36" dur="4.52">directions it&amp;#39;s zero if they&amp;#39;re</text><text start="1021.639" dur="5.04">perpendicular and it&amp;#39;s negative whenever</text><text start="1023.88" dur="4.52">they point in opposite directions for</text><text start="1026.679" dur="4.12">example let&amp;#39;s say you were playing with</text><text start="1028.4" dur="5.639">this model and you hypothesize that the</text><text start="1030.799" dur="5.441">embedding of cats minus cat might</text><text start="1034.039" dur="4.441">represent a sort of plurality Direction</text><text start="1036.24" dur="4">in the space to test this I&amp;#39;m going to</text><text start="1038.48" dur="3.4">take this vector and compute its dot</text><text start="1040.24" dur="3.88">product against the embeddings of</text><text start="1041.88" dur="3.919">certain singular nouns and compare it to</text><text start="1044.12" dur="3.88">the dot products with the corresponding</text><text start="1045.799" dur="3.921">plural nouns if you play around with</text><text start="1048" dur="3.48">this you&amp;#39;ll notice that the plural ones</text><text start="1049.72" dur="3.88">do indeed seem to consistently give</text><text start="1051.48" dur="3.92">higher values than the singular ones</text><text start="1053.6" dur="4.52">indicating that they align more with</text><text start="1055.4" dur="4.159">this direction it&amp;#39;s also fun how if you</text><text start="1058.12" dur="4.919">take this do product with the embeddings</text><text start="1059.559" dur="5.761">of the words 1 2 3 and so on they give</text><text start="1063.039" dur="4.52">increasing values so it&amp;#39;s as if we can</text><text start="1065.32" dur="3.96">quantitatively measure how plural the</text><text start="1067.559" dur="4.321">model finds a given</text><text start="1069.28" dur="4.8">word again the specifics for how words</text><text start="1071.88" dur="4.279">get embedded is learned using data this</text><text start="1074.08" dur="4.16">embedding Matrix whose columns tell us</text><text start="1076.159" dur="4.041">what happens to each word is the first</text><text start="1078.24" dur="4.6">pile of weights in our model and using</text><text start="1080.2" dur="4.52">the gpt3 numbers the vocabulary size</text><text start="1082.84" dur="4.12">specifically is</text><text start="1084.72" dur="4.72">50257 and again technically this</text><text start="1086.96" dur="5.8">consists not of words per se but of</text><text start="1089.44" dur="5.76">tokens and the embedding Dimension is</text><text start="1092.76" dur="5.399">12,288 multiplying those tells us this</text><text start="1095.2" dur="4.479">consists of about 617 million weights</text><text start="1098.159" dur="3.561">let&amp;#39;s go ahead and add this to a running</text><text start="1099.679" dur="4.761">tally remembering that by the end we</text><text start="1101.72" dur="5.12">should count up to 175</text><text start="1104.44" dur="3.76">billion in the case of Transformers you</text><text start="1106.84" dur="3.48">really want to think of the vectors in</text><text start="1108.2" dur="4.64">this embedding space as not merely</text><text start="1110.32" dur="4.32">representing individual words for one</text><text start="1112.84" dur="4.079">thing they also encode information about</text><text start="1114.64" dur="4.36">the position of that word which we&amp;#39;ll</text><text start="1116.919" dur="3.521">talk about later but more importantly</text><text start="1119" dur="4.799">you should think of them as having the</text><text start="1120.44" dur="5">capacity to soak in context a vector</text><text start="1123.799" dur="3.921">that started its life as the embedding</text><text start="1125.44" dur="4.28">of the word King for example might</text><text start="1127.72" dur="4.079">progressively get tugged and pulled by</text><text start="1129.72" dur="3.88">various blocks in this network so that</text><text start="1131.799" dur="4.161">by the end it points in a much more</text><text start="1133.6" dur="4.24">specific and nuanced direction that</text><text start="1135.96" dur="3.28">somehow encodes that it was a king who</text><text start="1137.84" dur="3">lived in Scotland</text><text start="1139.24" dur="3.559">and who had achieved his post after</text><text start="1140.84" dur="3.44">murdering the previous King and who&amp;#39;s</text><text start="1142.799" dur="3.721">being described in Shakespearean</text><text start="1144.28" dur="4">language think about your own</text><text start="1146.52" dur="3.88">understanding of a given word the</text><text start="1148.28" dur="4.519">meaning of that word is clearly informed</text><text start="1150.4" dur="4.24">by the surroundings and sometimes this</text><text start="1152.799" dur="4.24">includes context from a long distance</text><text start="1154.64" dur="4.2">away so in putting together a model that</text><text start="1157.039" dur="3.921">has the ability to predict what word</text><text start="1158.84" dur="4.079">comes next the goal is to somehow</text><text start="1160.96" dur="4.16">Empower it to incorporate context</text><text start="1162.919" dur="3.88">efficiently to be clear in that very</text><text start="1165.12" dur="3.6">first step when you create the array of</text><text start="1166.799" dur="3.481">vectors based on the input text each</text><text start="1168.72" dur="3.72">each one of those is simply plucked out</text><text start="1170.28" dur="3.879">of the embedding Matrix so initially</text><text start="1172.44" dur="4.2">each one can only encode the meaning of</text><text start="1174.159" dur="4.361">a single word without any input from its</text><text start="1176.64" dur="3.8">surroundings but you should think of the</text><text start="1178.52" dur="4.36">primary goal of this network that it</text><text start="1180.44" dur="3.92">flows through as being to enable each</text><text start="1182.88" dur="3.039">one of those vectors to soak up a</text><text start="1184.36" dur="3.84">meaning that&amp;#39;s much more rich and</text><text start="1185.919" dur="4.441">specific than what mere individual words</text><text start="1188.2" dur="4.04">could represent the network can only</text><text start="1190.36" dur="4.96">process a fixed number of vectors at a</text><text start="1192.24" dur="5.12">time known as its context size for gpt3</text><text start="1195.32" dur="4.359">it was trained with a context size of</text><text start="1197.36" dur="4.4">248 so the data flowing through the</text><text start="1199.679" dur="4.88">network always looks like this array of</text><text start="1201.76" dur="5.72">20 48 columns Each of which has 12,000</text><text start="1204.559" dur="4.321">Dimensions this context size limits how</text><text start="1207.48" dur="3.079">much text the Transformer can</text><text start="1208.88" dur="3.96">incorporate when it&amp;#39;s making a</text><text start="1210.559" dur="4">prediction of the next word this is why</text><text start="1212.84" dur="4.28">long conversations with certain chat</text><text start="1214.559" dur="4.36">Bots like the early versions of chat GPT</text><text start="1217.12" dur="3.76">often gave the feeling of the bot kind</text><text start="1218.919" dur="4.561">of losing the threat of conversation as</text><text start="1220.88" dur="4.48">you continued too long we&amp;#39;ll go into the</text><text start="1223.48" dur="3.319">details of attention in due time but</text><text start="1225.36" dur="2.92">skipping ahead I want to talk for a</text><text start="1226.799" dur="2.561">minute about what happens at the very</text><text start="1228.28" dur="2.92">end</text><text start="1229.36" dur="4.36">remember the desired output is a</text><text start="1231.2" dur="4.8">probability distribution over all tokens</text><text start="1233.72" dur="4.959">that might come next for example if the</text><text start="1236" dur="5.64">very last word is professor and the</text><text start="1238.679" dur="5.041">context includes words like Harry Potter</text><text start="1241.64" dur="4.08">and immediately proceeding we see least</text><text start="1243.72" dur="3.48">favorite teacher and also if you give me</text><text start="1245.72" dur="3.839">some leeway by letting me pretend that</text><text start="1247.2" dur="4.08">tokens simply look like full words then</text><text start="1249.559" dur="3.36">a well-trained network that had built up</text><text start="1251.28" dur="3.759">knowledge of Harry Potter would</text><text start="1252.919" dur="4.681">presumably assign a high number to the</text><text start="1255.039" dur="4.841">word Snape this involves two different</text><text start="1257.6" dur="4.84">steps the first one is to use another</text><text start="1259.88" dur="5.88">Matrix that Maps the very last Vector in</text><text start="1262.44" dur="5.96">that context to a list of 50,000 values</text><text start="1265.76" dur="4.6">one for each token in the vocabulary</text><text start="1268.4" dur="4.08">then there&amp;#39;s a function that normalizes</text><text start="1270.36" dur="3.72">this into a probability distribution</text><text start="1272.48" dur="3.64">it&amp;#39;s called softmax and we&amp;#39;ll talk more</text><text start="1274.08" dur="3.839">about it in just a second but before</text><text start="1276.12" dur="4.039">that it might seem a little bit weird to</text><text start="1277.919" dur="4.36">only use this last embedding to make a</text><text start="1280.159" dur="3.921">prediction when after all in that last</text><text start="1282.279" dur="4.121">step there are thousands of other</text><text start="1284.08" dur="4.68">vectors in the layer just sitting there</text><text start="1286.4" dur="3.68">with their own context-rich meanings</text><text start="1288.76" dur="3.56">this has to do with the fact that in the</text><text start="1290.08" dur="4.44">training process it turns out to be much</text><text start="1292.32" dur="4.2">more efficient if you use each one of</text><text start="1294.52" dur="3.96">those vectors in the final layer to</text><text start="1296.52" dur="4.279">simultaneously make a prediction for</text><text start="1298.48" dur="3.439">what would come immediately after it</text><text start="1300.799" dur="3">there&amp;#39;s a lot more to be said about</text><text start="1301.919" dur="4.521">training later on but I just want to</text><text start="1303.799" dur="4.441">call that out right now this Matrix is</text><text start="1306.44" dur="4.76">called The unembedded Matrix and we give</text><text start="1308.24" dur="5">it the label W again like all the weight</text><text start="1311.2" dur="3.719">matrices we see it&amp;#39;s entries begin at</text><text start="1313.24" dur="4">random but they are learned during the</text><text start="1314.919" dur="4.041">training process keeping score on our</text><text start="1317.24" dur="4.08">total parameter count this unembedded</text><text start="1318.96" dur="4.68">Matrix has one row for each word in the</text><text start="1321.32" dur="3.959">vocabulary and each row has the same</text><text start="1323.64" dur="3.56">number of elements as the embedding</text><text start="1325.279" dur="3.561">Dimension it&amp;#39;s very similar to the</text><text start="1327.2" dur="4.44">embedding Matrix just with the order</text><text start="1328.84" dur="4.68">swapped so it adds another 617 million</text><text start="1331.64" dur="4.56">parameters to the network meaning our</text><text start="1333.52" dur="4.96">count so far is a little over a billion</text><text start="1336.2" dur="4.44">a small but not wholly insignificant</text><text start="1338.48" dur="4.96">fraction of the 175 billion that we&amp;#39;ll</text><text start="1340.64" dur="4.519">end up with in total as the very last</text><text start="1343.44" dur="3.76">mini lesson for this chapter I want to</text><text start="1345.159" dur="3.801">talk more about the softmax function</text><text start="1347.2" dur="3.52">since it makes another appearance for us</text><text start="1348.96" dur="4.16">once we dive into the atttention</text><text start="1350.72" dur="4.04">blocks the idea is that if you want a</text><text start="1353.12" dur="3.559">sequence of numbers to act as a</text><text start="1354.76" dur="3.72">probability distribution say a</text><text start="1356.679" dur="4.161">distribution over all possible next</text><text start="1358.48" dur="5.16">words then each value has to be between</text><text start="1360.84" dur="5.319">0o and one and you also need all of them</text><text start="1363.64" dur="4.519">to add up to one however if you&amp;#39;re</text><text start="1366.159" dur="3.481">playing the Deep learning game where</text><text start="1368.159" dur="3.88">everything you do looks like Matrix</text><text start="1369.64" dur="4.72">Vector multiplication the outputs that</text><text start="1372.039" dur="4.681">you get by default don&amp;#39;t abide by this</text><text start="1374.36" dur="4.16">at all the values are often negative or</text><text start="1376.72" dur="4.36">much bigger than one and they almost</text><text start="1378.52" dur="4.399">certainly don&amp;#39;t add up to one softmax is</text><text start="1381.08" dur="3.56">the standard way to turn an arbitrary</text><text start="1382.919" dur="3.601">list of numbers into a valid</text><text start="1384.64" dur="4.56">distribution in such a way that the</text><text start="1386.52" dur="4.48">largest values end up closest to one and</text><text start="1389.2" dur="4">the smaller values end up very close to</text><text start="1391" dur="3.72">zero that&amp;#39;s all you really need to know</text><text start="1393.2" dur="4.12">but if you&amp;#39;re curious the way that it</text><text start="1394.72" dur="4.439">works is to First raise e to the power</text><text start="1397.32" dur="4.239">of each of the numbers which means you</text><text start="1399.159" dur="4.12">now have a list of positive values and</text><text start="1401.559" dur="4.161">then you can take the sum of all those</text><text start="1403.279" dur="5.28">positive values and divide each term by</text><text start="1405.72" dur="5.12">that sum which normalizes it into a list</text><text start="1408.559" dur="3.881">that adds up to one you&amp;#39;ll notice that</text><text start="1410.84" dur="3.8">if one of the numbers in the input is</text><text start="1412.44" dur="4.359">meaningfully bigger than the rest then</text><text start="1414.64" dur="4.08">in the output the corresponding term</text><text start="1416.799" dur="3.281">dominates the distribution so if you</text><text start="1418.72" dur="3.4">were sampling from it you&amp;#39;d almost</text><text start="1420.08" dur="4.68">certainly just be picking the maximizing</text><text start="1422.12" dur="4.52">input but it&amp;#39;s softer than just picking</text><text start="1424.76" dur="4.279">the max in the sense that when other</text><text start="1426.64" dur="4.36">values are similarly large they also get</text><text start="1429.039" dur="3.921">meaningful weight in the distribution</text><text start="1431" dur="4.48">and everything changes continuously as</text><text start="1432.96" dur="4.8">you continuously vary the inputs in some</text><text start="1435.48" dur="4.84">situations like when chat GPT is using</text><text start="1437.76" dur="4.039">this distribution to create a next word</text><text start="1440.32" dur="3.88">there&amp;#39;s room for a little bit of extra</text><text start="1441.799" dur="5.161">fun by adding a little extra spice into</text><text start="1444.2" dur="5.24">this function with a constant T thrown</text><text start="1446.96" dur="4.36">into the denominator of those exponents</text><text start="1449.44" dur="3.16">we call it the temperature since it</text><text start="1451.32" dur="2.959">vaguely resembles the role of</text><text start="1452.6" dur="4.16">temperature in certain thermodynamics</text><text start="1454.279" dur="5">equations and the effect is that when T</text><text start="1456.76" dur="4.72">is larger you give more weight to the</text><text start="1459.279" dur="4.801">lower values meaning the distribution is</text><text start="1461.48" dur="4.28">a little bit more uniform and if T is</text><text start="1464.08" dur="3.959">smaller then the bigger values will</text><text start="1465.76" dur="5.039">dominate more aggressively we in the</text><text start="1468.039" dur="5.401">extreme setting t equal to Zer means all</text><text start="1470.799" dur="5.321">of the weight goes to that maximum value</text><text start="1473.44" dur="5.16">for example I&amp;#39;ll have gpt3 generate a</text><text start="1476.12" dur="5.2">story with the seed text once upon a</text><text start="1478.6" dur="4.92">time there was a but I&amp;#39;m going to use</text><text start="1481.32" dur="4.52">different temperatures in each case</text><text start="1483.52" dur="5.12">temperature zero means that it always</text><text start="1485.84" dur="4.48">goes with the most predictable word and</text><text start="1488.64" dur="4.88">what you get ends up being kind of a</text><text start="1490.32" dur="5.16">trit derivative of Goldilocks a higher</text><text start="1493.52" dur="4">temperature gives it a chance to choose</text><text start="1495.48" dur="4.36">less likely words but it comes with a</text><text start="1497.52" dur="4.639">risk in case the story starts out a bit</text><text start="1499.84" dur="4.8">more originally about a young web artist</text><text start="1502.159" dur="5.201">from South Korea but it quickly</text><text start="1504.64" dur="4.56">degenerates into nonsense technically</text><text start="1507.36" dur="3.76">speaking the API doesn&amp;#39;t actually let</text><text start="1509.2" dur="3.599">you pick a temperature bigger than two</text><text start="1511.12" dur="3.48">there is no mathematical reason for this</text><text start="1512.799" dur="3.721">it&amp;#39;s just an arbitrary constraint</text><text start="1514.6" dur="3.52">imposed I suppose to keep their tool</text><text start="1516.52" dur="4.159">from being seen generating things that</text><text start="1518.12" dur="3.799">are too nonsensical so if you&amp;#39;re curious</text><text start="1520.679" dur="3.48">the way this animation is actually</text><text start="1521.919" dur="5.201">working is I&amp;#39;m taking the 20 most</text><text start="1524.159" dur="4.481">probable next tokens that gpt3 generates</text><text start="1527.12" dur="3.039">which seems to be the maximum they&amp;#39;ll</text><text start="1528.64" dur="3.519">give me and then I tweak the</text><text start="1530.159" dur="4.76">probabilities based on an exponent of</text><text start="1532.159" dur="4.201">1/5 as another bit of jargon in the same</text><text start="1534.919" dur="3.081">way that you might call the components</text><text start="1536.36" dur="3.88">of the output of this function</text><text start="1538" dur="5.2">probabilities people often refer to the</text><text start="1540.24" dur="5.039">inputs as logits or some people say</text><text start="1543.2" dur="4.16">logits some people say logits I&amp;#39;m going</text><text start="1545.279" dur="3.52">to say logits so for instance when you</text><text start="1547.36" dur="3.24">feed in some text you have all these</text><text start="1548.799" dur="3.521">word embeddings flow through the network</text><text start="1550.6" dur="3.6">and you do this final multiplication</text><text start="1552.32" dur="3.719">with the unembedded Matrix machine</text><text start="1554.2" dur="4.12">learning people would refer to the</text><text start="1556.039" dur="6.24">components in that Raw unnormalized</text><text start="1558.32" dur="6.04">output as the logits for the next word</text><text start="1562.279" dur="4.241">prediction a lot of the goal with this</text><text start="1564.36" dur="3.96">chapter was to lay the foundations for</text><text start="1566.52" dur="4.92">understanding the attention mechanism</text><text start="1568.32" dur="4.92">Karate Kid wax on wax off style you see</text><text start="1571.44" dur="4.359">if you have a strong intuition for word</text><text start="1573.24" dur="4.76">embeddings for soft Max for how dot</text><text start="1575.799" dur="3.961">products measure similarity and also the</text><text start="1578" dur="3.559">underlying premise that most of the</text><text start="1579.76" dur="3.799">calculations have to look like matrix</text><text start="1581.559" dur="5">multiplication with matrices full of</text><text start="1583.559" dur="5.321">tunable parameters then understanding</text><text start="1586.559" dur="4.441">the attention mechanism this Cornerstone</text><text start="1588.88" dur="4.2">piece in the whole modern boom in AI</text><text start="1591" dur="4.679">should be relatively smooth for that</text><text start="1593.08" dur="4.92">come join me in the next</text><text start="1595.679" dur="4.041">chapter as I&amp;#39;m publishing this a draft</text><text start="1598" dur="4.039">of that next chapter is available for</text><text start="1599.72" dur="4.04">review by patreon supporters a final</text><text start="1602.039" dur="3.721">version should be up in public in a week</text><text start="1603.76" dur="4.08">or two it usually depends on how much I</text><text start="1605.76" dur="3.72">end up changing based on that review in</text><text start="1607.84" dur="3.04">the meantime if you want to dive into</text><text start="1609.48" dur="2.88">attention and if you want to help the</text><text start="1610.88" dur="4.48">channel out a little bit it&amp;#39;s there</text><text start="1612.36" dur="3">waiting</text><text start="1622.02" dur="13.41">[Music]</text></transcript>