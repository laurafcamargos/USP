<?xml version="1.0" encoding="utf-8" ?><transcript><text start="8.16" dur="5.04">since chat GPT launched in 2022 large</text><text start="11.4" dur="3.8">language models have progressed at a</text><text start="13.2" dur="4.8">rapid Pace often developing</text><text start="15.2" dur="5.159">unpredictable abilities when dpd4 came</text><text start="18" dur="5.119">out it&amp;#39;s clearly felt like the chatbot</text><text start="20.359" dur="4.84">has some level of understanding but do</text><text start="23.119" dur="4.361">these abilities reflect actual</text><text start="25.199" dur="4">understanding or are the models simply</text><text start="27.48" dur="4.24">repeating their training data like</text><text start="29.199" dur="4.161">so-called stick castic parrots there&amp;#39;s a</text><text start="31.72" dur="3.48">lot of scientific interest in</text><text start="33.36" dur="3.719">understanding the capabilities and the</text><text start="35.2" dur="4.359">properties of large language models how</text><text start="37.079" dur="4.921">do we evaluate these large language</text><text start="39.559" dur="4.68">models we need evaluations that are</text><text start="42" dur="5">clearly relevant to the general purpose</text><text start="44.239" dur="4.881">intelligence and language understanding</text><text start="47" dur="4.32">recently researchers from Princeton and</text><text start="49.12" dur="4.04">Google deepmind created a mathematically</text><text start="51.32" dur="3.96">provable argument for how language</text><text start="53.16" dur="5">models develop so many skills and</text><text start="55.28" dur="4.88">designed a method for testing them the</text><text start="58.16" dur="4.199">results suggest that the largest model</text><text start="60.16" dur="4.039">develop new skills in a way that hints</text><text start="62.359" dur="3.961">at</text><text start="64.199" dur="4.56">understanding language models are</text><text start="66.32" dur="5.2">basically trained to solve next word</text><text start="68.759" dur="5.121">prediction tasks so they are given a lot</text><text start="71.52" dur="4.44">of text and at every step it has some</text><text start="73.88" dur="3.8">idea of what the next word is and that</text><text start="75.96" dur="4.36">idea is expressed in terms of a</text><text start="77.68" dur="4.399">probability and if the next word didn&amp;#39;t</text><text start="80.32" dur="4.159">get high enough probability there&amp;#39;s a</text><text start="82.079" dur="4.281">slight adjustment that&amp;#39;s done and after</text><text start="84.479" dur="3.96">many many many trillions of such small</text><text start="86.36" dur="4.36">adjustments it learns to predict the</text><text start="88.439" dur="4.521">next word over time time researchers</text><text start="90.72" dur="3.96">have observed neural scaling laws an</text><text start="92.96" dur="3.56">empirical relationship between the</text><text start="94.68" dur="4.439">performance of language models and the</text><text start="96.52" dur="4.959">data used to train them as models</text><text start="99.119" dur="5.121">improve they minimize training loss or</text><text start="101.479" dur="5.561">make fewer errors this sudden increase</text><text start="104.24" dur="5.8">in performance produces new behaviors a</text><text start="107.04" dur="5.28">phenomenon called emergence there&amp;#39;s no</text><text start="110.04" dur="5.119">scientific explanation as to why that&amp;#39;s</text><text start="112.32" dur="5.079">happening so this phenomena is not well</text><text start="115.159" dur="4.841">understood the researchers wondered if</text><text start="117.399" dur="5.08">GPT 4&amp;#39;s sudden improvements could be</text><text start="120" dur="4.96">explained by emergence perhaps the model</text><text start="122.479" dur="5.601">had learned compositional generalization</text><text start="124.96" dur="5.359">the ability to combine language skills</text><text start="128.08" dur="4.72">this was some kind of a meta capability</text><text start="130.319" dur="4.56">there was no mathematical framework to</text><text start="132.8" dur="4.12">think about that and so we had to come</text><text start="134.879" dur="3.921">up with a mathematical framework the</text><text start="136.92" dur="4.24">researchers found their first Hint by</text><text start="138.8" dur="4.68">considering neural scaling laws so those</text><text start="141.16" dur="4.079">scaling laws already suggest that</text><text start="143.48" dur="4.44">there&amp;#39;s some statistical phenomenon</text><text start="145.239" dur="5.121">going on so Random graphs have a long</text><text start="147.92" dur="5.24">history in terms of thinking about</text><text start="150.36" dur="4.8">statistical phenomena random graphs are</text><text start="153.16" dur="4.88">made of noes which are connected by</text><text start="155.16" dur="4.68">randomly generated edges the researchers</text><text start="158.04" dur="4.44">built their mathematical model with</text><text start="159.84" dur="6.08">bipartite graphs which contain two types</text><text start="162.48" dur="6.6">of nodes one representing chunks of text</text><text start="165.92" dur="5.08">and the other language skills the edges</text><text start="169.08" dur="4.36">of the graph the connections correspond</text><text start="171" dur="4.92">to which skill is needed to understand</text><text start="173.44" dur="4.48">that piece of text now the researchers</text><text start="175.92" dur="4.2">needed to connect these bipartite graphs</text><text start="177.92" dur="4.319">to actual language models but there was</text><text start="180.12" dur="4.52">a problem we don&amp;#39;t have access to the</text><text start="182.239" dur="4.841">training data so if I evaluating that</text><text start="184.64" dur="4.56">language model on my evaluation set how</text><text start="187.08" dur="5.04">do I know that the language model hasn&amp;#39;t</text><text start="189.2" dur="4.52">seen that data into the training Corpus</text><text start="192.12" dur="3.52">there was one crucial piece of</text><text start="193.72" dur="4.68">information that the researchers could</text><text start="195.64" dur="5.239">access using that scaling law we made a</text><text start="198.4" dur="4.88">prediction as models get better at</text><text start="200.879" dur="4.481">predicting the next word that they will</text><text start="203.28" dur="4.64">be able to combine more of the</text><text start="205.36" dur="4.92">underlying skills according to random</text><text start="207.92" dur="4.48">graph Theory every combination arises</text><text start="210.28" dur="4.8">from a random sampling of possible</text><text start="212.4" dur="4.68">skills if there are 100 skill nodes in</text><text start="215.08" dur="4.32">the graph and you want to combine four</text><text start="217.08" dur="4.96">skills then there are about 100 to the</text><text start="219.4" dur="3.6">fourth power or 100 million ways to</text><text start="222.04" dur="3.559">combine</text><text start="223" dur="4.879">them the researchers developed a test</text><text start="225.599" dur="4.081">called skill mix to evaluate if large</text><text start="227.879" dur="3.521">language models can generalize to</text><text start="229.68" dur="3.08">combinations of skills they likely</text><text start="231.4" dur="4.119">hadn&amp;#39;t seen</text><text start="232.76" dur="5.679">before so the model is given a list of</text><text start="235.519" dur="5.681">skills and a topic and then it&amp;#39;s</text><text start="238.439" dur="5.52">supposed to create a piece of text on</text><text start="241.2" dur="5.2">that topic using that list of skills for</text><text start="243.959" dur="4.681">example the researchers asked gp4 to</text><text start="246.4" dur="4.559">generate a short text about sewing that</text><text start="248.64" dur="3.959">exhibit spatial reasoning self-serving</text><text start="250.959" dur="4.961">bias and</text><text start="252.599" dur="5.6">metaphor here&amp;#39;s what it answered in The</text><text start="255.92" dur="4.839">Labyrinth of sewing I am the needle</text><text start="258.199" dur="4.681">navigating between the intricate weaves</text><text start="260.759" dur="5.921">any errors are due to the faulty Compass</text><text start="262.88" dur="6.28">of lowquality thread not my skill we</text><text start="266.68" dur="4.92">showed in our mathematical framework</text><text start="269.16" dur="5.44">that as we we scale up the model is able</text><text start="271.6" dur="5.56">to learn these skills you would see this</text><text start="274.6" dur="5.039">increase in compositional capability as</text><text start="277.16" dur="4.72">you scale up the models when given the</text><text start="279.639" dur="4.641">skill mix test small language models</text><text start="281.88" dur="5.12">struggle to combine just a couple of</text><text start="284.28" dur="5.359">skills mediumsized models could combine</text><text start="287" dur="5.759">two skills more comfortably but the</text><text start="289.639" dur="4.681">largest models like GPT 4 could combine</text><text start="292.759" dur="3.681">five or six</text><text start="294.32" dur="4.2">skills because these models couldn&amp;#39;t</text><text start="296.44" dur="4.199">have seen all possible combinations of</text><text start="298.52" dur="3.64">skills the research ERS argue that it</text><text start="300.639" dur="3.4">must have developed compositional</text><text start="302.16" dur="4.039">generalization through</text><text start="304.039" dur="4.201">emergence once the model has learned</text><text start="306.199" dur="6.681">these language skills um model can</text><text start="308.24" dur="7.28">generalize to random unseen compositions</text><text start="312.88" dur="5.159">of these skills what they showed was</text><text start="315.52" dur="4.799">that their mathematical model had this</text><text start="318.039" dur="5">property of compositionality and that by</text><text start="320.319" dur="4.72">itself gives this ability to extrapolate</text><text start="323.039" dur="4">and compose new combinations from</text><text start="325.039" dur="4.241">existing pieces and that is really the</text><text start="327.039" dur="3.321">Hallmark of novelty and the Hallmark of</text><text start="329.28" dur="2.96">creativ</text><text start="330.36" dur="3.6">and so the argument is that large</text><text start="332.24" dur="3.56">language models can move Beyond being</text><text start="333.96" dur="4.079">stochastic</text><text start="335.8" dur="3.839">parents the researchers are already</text><text start="338.039" dur="3.921">working to extend the skill mix</text><text start="339.639" dur="4">evaluation to other domains as part of a</text><text start="341.96" dur="4.16">larger effort to understand the</text><text start="343.639" dur="4.041">capabilities of large language models</text><text start="346.12" dur="5.919">can we create an</text><text start="347.68" dur="7.44">ecosystem um of skill mix which is not</text><text start="352.039" dur="5.72">just valid for language skills but</text><text start="355.12" dur="5.12">mathematical skills as well as coding</text><text start="357.759" dur="4.841">skills so skill mix was one example</text><text start="360.24" dur="4.72">where we made a prediction by just</text><text start="362.6" dur="4.319">mathematical thinking and that was</text><text start="364.96" dur="4.04">correct but there are all kinds of other</text><text start="366.919" dur="6.491">phenomena that we probably are not aware</text><text start="369" dur="7.039">of and we need someone understanding of</text><text start="373.41" dur="5.11">[Music]</text><text start="376.039" dur="5.241">that Quantum systems are some of the</text><text start="378.52" dur="4.6">most complex structures in nature to</text><text start="381.28" dur="4.12">model them you need to compute a</text><text start="383.12" dur="4.519">hamiltonian a super equation that</text><text start="385.4" dur="3.799">describes how particles interact locally</text><text start="387.639" dur="2.521">to produce the system&amp;#39;s possible</text><text start="389.199" dur="3.241">physical</text><text start="390.16" dur="4.08">properties but entanglement spreads</text><text start="392.44" dur="4.4">information across the system</text><text start="394.24" dur="4.56">correlating particles that are far apart</text><text start="396.84" dur="4.56">this makes Computing hamiltonians</text><text start="398.8" dur="5.36">exceptionally difficult you have a giant</text><text start="401.4" dur="4.96">system of atoms it&amp;#39;s a very big problem</text><text start="404.16" dur="5.039">to learn all those parameters you could</text><text start="406.36" dur="4.72">never hope to write down the hamiltonian</text><text start="409.199" dur="3.601">if you ever even tried to write it down</text><text start="411.08" dur="4.2">the game would be over and you wouldn&amp;#39;t</text><text start="412.8" dur="4.2">have an efficient algorithm people were</text><text start="415.28" dur="3.199">actually trying to prove that efficient</text><text start="417" dur="3.8">algorithms were impossible in this</text><text start="418.479" dur="4.961">regime but a team of computer scientists</text><text start="420.8" dur="5.04">from MIT and UC Berkeley cracked the</text><text start="423.44" dur="4.479">problem they created an algorithm that</text><text start="425.84" dur="4.84">can produce the hamiltonian of a Quantum</text><text start="427.919" dur="4.84">system at any constant temperature the</text><text start="430.68" dur="4.4">results could have big implications for</text><text start="432.759" dur="5.321">the future of quantum Computing and</text><text start="435.08" dur="4.959">understanding exotic Quantum Behavior so</text><text start="438.08" dur="4.44">when we have systems that behave and do</text><text start="440.039" dur="4.72">interesting things like super fluidity</text><text start="442.52" dur="3.92">and super conductivity you want to</text><text start="444.759" dur="3.521">understand the building blocks and how</text><text start="446.44" dur="3.52">they fit together to create those</text><text start="448.28" dur="4.039">properties that you want to harness for</text><text start="449.96" dur="3.799">technological reasons so we&amp;#39;re trying to</text><text start="452.319" dur="4.16">learn this object which is the</text><text start="453.759" dur="4.201">hamiltonian it&amp;#39;s defined by a small set</text><text start="456.479" dur="4.241">of parameters and what we&amp;#39;re trying to</text><text start="457.96" dur="5.28">do is learn these parameters what we</text><text start="460.72" dur="4.96">have access to is these experimental</text><text start="463.24" dur="4.72">measurements of the quantum system so</text><text start="465.68" dur="5.079">the question then becomes can you learn</text><text start="467.96" dur="4.679">a description of the system through</text><text start="470.759" dur="4">experiments previous efforts in</text><text start="472.639" dur="3.68">hamiltonian learning produced algorithms</text><text start="474.759" dur="3.44">that could measure particles at high</text><text start="476.319" dur="3.641">temperatures but these systems are</text><text start="478.199" dur="4.201">largely classical so so there&amp;#39;s no</text><text start="479.96" dur="4.799">entanglement between the particles the</text><text start="482.4" dur="4.759">MIT and Berkeley team set their SES on</text><text start="484.759" dur="4.361">the low temperature Quantum regimes I</text><text start="487.159" dur="3.88">wanted to understand what kinds of</text><text start="489.12" dur="4.24">strategies worked algorithmically on the</text><text start="491.039" dur="4.201">classical side and what could be</text><text start="493.36" dur="4.04">manifestations of those strategies on</text><text start="495.24" dur="3.56">the quantum side once you like look at</text><text start="497.4" dur="3.639">the problem in the right way and you</text><text start="498.8" dur="4.079">bring to bear these tools it turns out</text><text start="501.039" dur="4.44">that you can really make progress on</text><text start="502.879" dur="4.28">these problems first the team ported</text><text start="505.479" dur="4.72">over a tool from classical machine</text><text start="507.159" dur="4.521">learning called polom optimization</text><text start="510.199" dur="3.361">this allowed them to approximate the</text><text start="511.68" dur="4.4">measurements of their system as a family</text><text start="513.56" dur="4.839">of polinomial equations we were like</text><text start="516.08" dur="4.6">maybe we can write Hamilton learning as</text><text start="518.399" dur="5.12">a polinomial optimization problem and if</text><text start="520.68" dur="4.36">we manage to do this maybe we can try to</text><text start="523.519" dur="3.88">optimize this polinomial system</text><text start="525.04" dur="4.919">efficiently so all of a sudden it&amp;#39;s in a</text><text start="527.399" dur="4.241">domain that&amp;#39;s more familiar and you have</text><text start="529.959" dur="3.721">a bunch of algorithmic tools at your</text><text start="531.64" dur="3.879">disposal you can&amp;#39;t solve polinomial</text><text start="533.68" dur="4.24">systems but what you can do is you can</text><text start="535.519" dur="4.721">sort of solve a relaxation of them we</text><text start="537.92" dur="4.56">use uh something called the sum of</text><text start="540.24" dur="4.599">squares relaxation to actually solve</text><text start="542.48" dur="4.919">this polinomial system starting with a</text><text start="544.839" dur="4.401">challenging polom optimization problem</text><text start="547.399" dur="3.241">the team used the sum of squares method</text><text start="549.24" dur="3.64">to relax its</text><text start="550.64" dur="4.759">constraints this expanded the equations</text><text start="552.88" dur="4.28">to a larger allowable set of solutions</text><text start="555.399" dur="4.801">effectively converting it from a hard</text><text start="557.16" dur="5.04">problem to an easier one the real trick</text><text start="560.2" dur="4.84">is to argue that when you&amp;#39;ve expanded</text><text start="562.2" dur="5.92">the set of solutions you can still find</text><text start="565.04" dur="5.44">a good solution inside it you need a</text><text start="568.12" dur="5.36">procedure to take that approximate</text><text start="570.48" dur="4.88">relaxed solution and round it back into</text><text start="573.48" dur="3.64">an actual solution to the problem you</text><text start="575.36" dur="4.12">really cared about so that&amp;#39;s really</text><text start="577.12" dur="4.64">where the coolest parts of the proof</text><text start="579.48" dur="4.56">happen the researchers proved that the</text><text start="581.76" dur="4.24">sum of squares relaxation could solve</text><text start="584.04" dur="4.16">their learning problem resulting in the</text><text start="586" dur="5.12">first efficient hamiltonian algorithm in</text><text start="588.2" dur="5.319">a low temperature regime so we first</text><text start="591.12" dur="4.56">make some set of measurements of the</text><text start="593.519" dur="4.44">macroscopic properties of the system and</text><text start="595.68" dur="4.88">then we use these measurements to set up</text><text start="597.959" dur="4.481">a system of polinomial equations and</text><text start="600.56" dur="4">then we solve the system of polinomial</text><text start="602.44" dur="4.36">equations so the output is a description</text><text start="604.56" dur="4.76">of the local interactions in the</text><text start="606.8" dur="4.279">system there are actually some very</text><text start="609.32" dur="3.4">interesting learning problems that are</text><text start="611.079" dur="3.401">at the heart of understanding Quantum</text><text start="612.72" dur="3.92">systems and to me that was the most</text><text start="614.48" dur="4.12">exciting part was really a connection</text><text start="616.64" dur="3.199">between two different worlds this like</text><text start="618.6" dur="2.359">combination of tools is really</text><text start="619.839" dur="2.841">interesting it&amp;#39;s something I haven&amp;#39;t</text><text start="620.959" dur="4.201">seen before I&amp;#39;m hoping it&amp;#39;s like a</text><text start="622.68" dur="4.56">useful perspective with with which to um</text><text start="625.16" dur="4.799">tackle other questions as well I think</text><text start="627.24" dur="5.279">we find ourselves at the start of this</text><text start="629.959" dur="6.681">new bridge between theoretical computer</text><text start="632.519" dur="4.121">science and quantum mechanics</text><text start="638.61" dur="10.01">[Music]</text></transcript>