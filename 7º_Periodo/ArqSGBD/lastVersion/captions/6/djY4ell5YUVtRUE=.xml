<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0" dur="3.679">the game Wordle has gone pretty viral in</text><text start="1.719" dur="3.481">the last month or two and never want to</text><text start="3.679" dur="3.08">overlook an opportunity for a math</text><text start="5.2" dur="3.64">lesson it occurs to me that this game</text><text start="6.759" dur="4.281">makes for a very good Central example in</text><text start="8.84" dur="3.959">a lesson about information Theory and in</text><text start="11.04" dur="3.92">particular a topic known as</text><text start="12.799" dur="3.921">entropy you see like a lot of people I</text><text start="14.96" dur="3.28">got kind of sucked into the puzzle and</text><text start="16.72" dur="3.68">like a lot of programmers I also got</text><text start="18.24" dur="4.039">sucked into trying to write an algorithm</text><text start="20.4" dur="3.48">that would play the game as optimally as</text><text start="22.279" dur="3.201">it could and what I thought I&amp;#39;d do here</text><text start="23.88" dur="3.239">is just talk through with you some of my</text><text start="25.48" dur="3.28">process in that and explain some of the</text><text start="27.119" dur="5.48">math that went into it since the whole</text><text start="28.76" dur="3.839">algorithm centers on this idea of</text><text start="37.8" dur="4.48">entropy first things first in case you</text><text start="40" dur="3.84">haven&amp;#39;t heard of it what is wle and to</text><text start="42.28" dur="3.04">kill two birds with one stone here while</text><text start="43.84" dur="3.239">we go through the rules of the game let</text><text start="45.32" dur="3.239">me also preview where we&amp;#39;re going with</text><text start="47.079" dur="3.12">this which is to develop a little</text><text start="48.559" dur="3.84">algorithm that will basically play the</text><text start="50.199" dur="4.081">game for us so I haven&amp;#39;t done today&amp;#39;s</text><text start="52.399" dur="4.041">wle this is February 4th and we&amp;#39;ll see</text><text start="54.28" dur="4.52">how the bot does the goal of Wordle is</text><text start="56.44" dur="4.439">to guess a mystery felet word and you&amp;#39;re</text><text start="58.8" dur="4.079">given six different chances to guess for</text><text start="60.879" dur="4.801">example my world bot suggests that I</text><text start="62.879" dur="4.401">start with the guess crane each time</text><text start="65.68" dur="3.439">that you make a guess you get some</text><text start="67.28" dur="4.36">information about how close your guess</text><text start="69.119" dur="4.601">is to the true answer here the gray box</text><text start="71.64" dur="4.04">is telling me there&amp;#39;s no C in the actual</text><text start="73.72" dur="3.719">answer the yellow box is telling me</text><text start="75.68" dur="3.6">there is an R but it&amp;#39;s not in that</text><text start="77.439" dur="3.72">position the green box is telling me</text><text start="79.28" dur="3.64">that the secret word does have an A and</text><text start="81.159" dur="4.28">it&amp;#39;s in the third position and then</text><text start="82.92" dur="3.96">there&amp;#39;s no n and there&amp;#39;s no e so let me</text><text start="85.439" dur="3.281">just go in and tell the wordbot that</text><text start="86.88" dur="4.8">information we started with crane we got</text><text start="88.72" dur="4.28">gray yellow green Gray gray don&amp;#39;t worry</text><text start="91.68" dur="3.64">about all the data that it&amp;#39;s showing</text><text start="93" dur="4.32">right now I&amp;#39;ll explain that in due time</text><text start="95.32" dur="5.04">but its top suggestion for our second</text><text start="97.32" dur="5.439">pick is shtick and your guess does have</text><text start="100.36" dur="3.92">to be an actual felet word but as you&amp;#39;ll</text><text start="102.759" dur="3.881">see it&amp;#39;s pretty liberal with what it</text><text start="104.28" dur="3.4">will actually let you guess in this case</text><text start="106.64" dur="3.119">we try</text><text start="107.68" dur="4.24">shtick and all right things are looking</text><text start="109.759" dur="3.441">pretty good we hit the S and the H so we</text><text start="111.92" dur="3.12">know the first three letters we know</text><text start="113.2" dur="5.239">that there&amp;#39;s an r and so it&amp;#39;s going to</text><text start="115.04" dur="5.52">be like sh a something r or sh a r</text><text start="118.439" dur="3.881">something and it looks like the worldle</text><text start="120.56" dur="4.68">bot knows that it&amp;#39;s down to just two</text><text start="122.32" dur="4.479">possibilities either Shard or sharp it&amp;#39;s</text><text start="125.24" dur="3.4">kind of a tossup between them at this</text><text start="126.799" dur="3.361">point so I guess probably just cuz it&amp;#39;s</text><text start="128.64" dur="4.2">alphabetical it goes with</text><text start="130.16" dur="4.6">Shard which hooray is the actual answer</text><text start="132.84" dur="3.32">so we got it in three if you&amp;#39;re</text><text start="134.76" dur="3">wondering if that&amp;#39;s any good the way I</text><text start="136.16" dur="4.6">heard one person phrase it is that with</text><text start="137.76" dur="5.04">wle four is par and three is birdie</text><text start="140.76" dur="3.839">which I think is a pretty apt analogy</text><text start="142.8" dur="3.6">you have to be consistently on your game</text><text start="144.599" dur="4.041">to be getting four but it&amp;#39;s certainly</text><text start="146.4" dur="4.24">not crazy but when you get it in three</text><text start="148.64" dur="3.239">it just feels great</text><text start="150.64" dur="2.599">so if you&amp;#39;re down for it what I&amp;#39;d like</text><text start="151.879" dur="2.881">to do here is just talk through my</text><text start="153.239" dur="3.681">thought process from the beginning for</text><text start="154.76" dur="3.52">how I approach the wordbot and like I</text><text start="156.92" dur="3.239">said really it&amp;#39;s an excuse for an</text><text start="158.28" dur="3.8">information Theory lesson the main goal</text><text start="160.159" dur="4.201">is to explain what is information and</text><text start="162.08" dur="2.28">what is</text><text start="167.48" dur="3.92">entropy my first thought in approaching</text><text start="169.76" dur="3.119">this was to take a look at the relative</text><text start="171.4" dur="3.559">frequencies of different letters in the</text><text start="172.879" dur="3.72">English language so I thought okay is</text><text start="174.959" dur="3.401">there an opening guess or an opening</text><text start="176.599" dur="3.801">pair of guesses that hits a lot of these</text><text start="178.36" dur="3.519">most frequent letters and and one that I</text><text start="180.4" dur="4.199">was pretty fond of was doing other</text><text start="181.879" dur="3.961">followed by Nails the thought is that if</text><text start="184.599" dur="2.881">you hit a letter you know you get a</text><text start="185.84" dur="3.479">green or a yellow that always feels good</text><text start="187.48" dur="3.64">it feels like you&amp;#39;re getting information</text><text start="189.319" dur="3.56">but in these cases even if you don&amp;#39;t hit</text><text start="191.12" dur="3.32">and you always get Grays that&amp;#39;s still</text><text start="192.879" dur="3.121">giving you a lot of information since</text><text start="194.44" dur="3.76">it&amp;#39;s pretty rare to find a word that</text><text start="196" dur="3.76">doesn&amp;#39;t have any of these letters but</text><text start="198.2" dur="3.319">even still that doesn&amp;#39;t feel super</text><text start="199.76" dur="3.08">systematic cuz for example it does</text><text start="201.519" dur="3.44">nothing to consider the order of the</text><text start="202.84" dur="4.399">letters why type nails when I could type</text><text start="204.959" dur="4.601">snail is it better to have that S at the</text><text start="207.239" dur="3.881">end I&amp;#39;m not really sure now A friend of</text><text start="209.56" dur="4.08">mine said that he liked to open with the</text><text start="211.12" dur="3.92">word weary which kind of surprised me</text><text start="213.64" dur="4.2">cuz it has some uncommon letters in</text><text start="215.04" dur="4.52">there like the W and the Y but who knows</text><text start="217.84" dur="3.759">maybe that is a better opener is there</text><text start="219.56" dur="4">some kind of quantitative score that we</text><text start="221.599" dur="4.321">can give to judge the quality of a</text><text start="223.56" dur="3.72">potential guess now to set up for the</text><text start="225.92" dur="3.2">way that we&amp;#39;re going to rank possible</text><text start="227.28" dur="3.76">guesses let&amp;#39;s go back and add a little</text><text start="229.12" dur="3.8">Clarity to how exactly the game is set</text><text start="231.04" dur="3.32">up so there&amp;#39;s a list of words that it</text><text start="232.92" dur="3.399">will allow you to enter that are</text><text start="234.36" dur="4.12">considered valid guesses that&amp;#39;s just</text><text start="236.319" dur="3.48">about 13,000 words long but when you</text><text start="238.48" dur="4.039">look at it there&amp;#39;s a lot of really</text><text start="239.799" dur="4.681">uncommon things things like ahead or Ali</text><text start="242.519" dur="3.44">and ARG the kind of words that bring</text><text start="244.48" dur="3.52">about family arguments in a game of</text><text start="245.959" dur="3.401">Scrabble but the vibe of the game is</text><text start="248" dur="3.519">that the answer is always going to be a</text><text start="249.36" dur="4.719">decently common word and in fact there&amp;#39;s</text><text start="251.519" dur="5">another list of around 2300 words that</text><text start="254.079" dur="4.601">are the possible answers and this is a</text><text start="256.519" dur="3.881">human curated list I think specifically</text><text start="258.68" dur="3.92">by the game Creator&amp;#39;s girlfriend which</text><text start="260.4" dur="4.519">is kind of fun but what I would like to</text><text start="262.6" dur="3.76">do our challenge for this project is to</text><text start="264.919" dur="3.761">see if we can write a program solving</text><text start="266.36" dur="4.76">Wordle that doesn&amp;#39;t incorporate previous</text><text start="268.68" dur="3.56">knowledge about this list for one thing</text><text start="271.12" dur="3">there&amp;#39;s plenty of prettyy common</text><text start="272.24" dur="3.6">five-letter words that you won&amp;#39;t find in</text><text start="274.12" dur="3.48">that list so it would be better to write</text><text start="275.84" dur="3.72">a program that&amp;#39;s a little more resilient</text><text start="277.6" dur="3.48">and would play Wordle against anyone not</text><text start="279.56" dur="4.079">just what happens to be the official</text><text start="281.08" dur="4.36">website and also the reason that we know</text><text start="283.639" dur="3.681">what this list of possible answers is is</text><text start="285.44" dur="3.28">because it&amp;#39;s visible in the source code</text><text start="287.32" dur="3.719">but the way that it&amp;#39;s visible in the</text><text start="288.72" dur="4.44">source code is in the specific order in</text><text start="291.039" dur="3.561">which answers come up from day to day so</text><text start="293.16" dur="3.56">you could always just look up what</text><text start="294.6" dur="3.56">tomorrow&amp;#39;s answer will be so clearly</text><text start="296.72" dur="3.08">there&amp;#39;s some sense in which using the</text><text start="298.16" dur="3.12">list is cheating and what makes for a</text><text start="299.8" dur="3.52">more interesting puzzle and a richer</text><text start="301.28" dur="3.96">information Theory lesson is to instead</text><text start="303.32" dur="4.04">use some more Universal data like</text><text start="305.24" dur="3.64">relative word frequencies in general to</text><text start="307.36" dur="3.2">capture this intuition of having a</text><text start="308.88" dur="5.52">preference for more common</text><text start="310.56" dur="5.88">words so of these 13,000 possibilities</text><text start="314.4" dur="3.919">how should we choose the opening guess</text><text start="316.44" dur="4.08">for example if my friend proposes weary</text><text start="318.319" dur="3.521">how should we analyze its quality well</text><text start="320.52" dur="4">the reason he said he likes that</text><text start="321.84" dur="4.56">unlikely W is that he likes The Longshot</text><text start="324.52" dur="4.16">nature of just how good it feels if you</text><text start="326.4" dur="4.4">do hit that W for example if the first</text><text start="328.68" dur="4.16">pattern revealed was something like this</text><text start="330.8" dur="4.28">then it turns out there are only 58</text><text start="332.84" dur="4.44">words in this giant lexicon that match</text><text start="335.08" dur="4.88">that pattern so that&amp;#39;s a huge reduction</text><text start="337.28" dur="4.68">from 13,000 but the flip side of that of</text><text start="339.96" dur="4.4">course is that it&amp;#39;s very uncommon to get</text><text start="341.96" dur="4.72">a pattern like this specifically if each</text><text start="344.36" dur="3.959">word was equally likely to be the answer</text><text start="346.68" dur="5">the probability of hitting this pattern</text><text start="348.319" dur="4.921">would be 58 divided around 13,000 of</text><text start="351.68" dur="3.28">course they&amp;#39;re not equally likely to be</text><text start="353.24" dur="3.64">answers most of these are very obscure</text><text start="354.96" dur="3.64">and even questionable words but at least</text><text start="356.88" dur="2.8">for our first pass at all of this let&amp;#39;s</text><text start="358.6" dur="3.319">assume that they&amp;#39;re all equally like</text><text start="359.68" dur="3.68">likely and then refine that a bit later</text><text start="361.919" dur="3.801">the point is the pattern with a lot of</text><text start="363.36" dur="4.92">information is by its very nature</text><text start="365.72" dur="5.16">unlikely to occur in fact what it means</text><text start="368.28" dur="5">to be informative is that it&amp;#39;s</text><text start="370.88" dur="4.48">unlikely a much more probable pattern to</text><text start="373.28" dur="4.16">see with this opening would be something</text><text start="375.36" dur="4.2">like this where of course there&amp;#39;s not a</text><text start="377.44" dur="4.64">w in it maybe there&amp;#39;s an e and maybe</text><text start="379.56" dur="4.56">there&amp;#39;s no a there&amp;#39;s no R there&amp;#39;s no Y</text><text start="382.08" dur="4.36">in this case there are 1,400 possible</text><text start="384.12" dur="4.04">matches so if all were equally likely it</text><text start="386.44" dur="3.84">works out to be a probability of about</text><text start="388.16" dur="4.24">11% that this is the pattern you would</text><text start="390.28" dur="4.359">see so the most likely outcomes are also</text><text start="392.4" dur="3.799">the least informative to get a more</text><text start="394.639" dur="3.801">global view here let me show you the</text><text start="396.199" dur="3.761">full distribution of probabilities</text><text start="398.44" dur="4.479">across all of the different patterns</text><text start="399.96" dur="4.48">that you might see so each bar that</text><text start="402.919" dur="3.241">you&amp;#39;re looking at corresponds to a</text><text start="404.44" dur="3.72">possible pattern of colors that could be</text><text start="406.16" dur="3.72">revealed of which there are 3 to the</text><text start="408.16" dur="3.4">5ifth possibilities and they&amp;#39;re</text><text start="409.88" dur="4">organized from left to right most common</text><text start="411.56" dur="4.039">to least common so the most common</text><text start="413.88" dur="4.68">possibility here is that you get all</text><text start="415.599" dur="4.361">Grays that happens about 14% of the time</text><text start="418.56" dur="2.88">and what you&amp;#39;re hoping for for when you</text><text start="419.96" dur="3.519">make a guess is that you end up</text><text start="421.44" dur="3.64">somewhere out in this long tail like</text><text start="423.479" dur="3">over here where there&amp;#39;s only 18</text><text start="425.08" dur="4.799">possibilities for what matches this</text><text start="426.479" dur="4.84">pattern that evidently look like this or</text><text start="429.879" dur="3.281">if we venture a little further to the</text><text start="431.319" dur="4.6">left you know maybe we go all the way</text><text start="433.16" dur="4.36">over here okay here&amp;#39;s a good puzzle for</text><text start="435.919" dur="3.881">you what are the three words in the</text><text start="437.52" dur="4.799">English language that start with a W end</text><text start="439.8" dur="5.079">with a y and have an R somewhere in them</text><text start="442.319" dur="6.041">turns out the answers are uh let&amp;#39;s see</text><text start="444.879" dur="5.681">wordy wormy and Riley so to judge how</text><text start="448.36" dur="4.44">good this word is overall all we want</text><text start="450.56" dur="3.6">some kind of measure of the expected</text><text start="452.8" dur="3.959">amount of information that you&amp;#39;re going</text><text start="454.16" dur="4.28">to get from this distribution if we go</text><text start="456.759" dur="4.321">through each pattern and we multiply its</text><text start="458.44" dur="4.56">probability of occurring times something</text><text start="461.08" dur="3.76">that measures how informative it is that</text><text start="463" dur="4.12">can maybe give us an objective</text><text start="464.84" dur="4.039">score now your first instinct for what</text><text start="467.12" dur="3.4">that something should be might be the</text><text start="468.879" dur="4.121">number of matches you know you want a</text><text start="470.52" dur="4.44">lower average number of matches but</text><text start="473" dur="3.36">instead I&amp;#39;d like to use a more Universal</text><text start="474.96" dur="3.32">measurement that we often ascribe to</text><text start="476.36" dur="3.519">information and one that will be more</text><text start="478.28" dur="3.08">flexible once we have a different</text><text start="479.879" dur="3.521">probability assigned to each of these</text><text start="481.36" dur="4.559">13,000 words for whether or not they&amp;#39;re</text><text start="483.4" dur="2.519">actually the</text><text start="489.479" dur="4.44">answer the standard unit of information</text><text start="492.12" dur="3.6">is the bit which has a little bit of a</text><text start="493.919" dur="4.24">funny formula but it&amp;#39;s really intuitive</text><text start="495.72" dur="4.159">if we just look at examples if you have</text><text start="498.159" dur="4.081">an observation that cuts your space of</text><text start="499.879" dur="4.76">possibilities in half we say that it has</text><text start="502.24" dur="3.76">one bit of information in our example</text><text start="504.639" dur="3.24">the space of possibilities is all</text><text start="506" dur="3.879">possible words and it turns out about</text><text start="507.879" dur="4.121">half of the felet words have an F a</text><text start="509.879" dur="3.761">little less than that but about half so</text><text start="512" dur="4.64">that observation would give you one bit</text><text start="513.64" dur="4.56">of information if instead a new fact</text><text start="516.64" dur="3.6">chops down that space of possibilities</text><text start="518.2" dur="4.16">by a factor of four we say that it has</text><text start="520.24" dur="3.719">two bits of information for example it</text><text start="522.36" dur="3.96">turns out about a quarter of these words</text><text start="523.959" dur="4.041">have a t if the observation cuts that</text><text start="526.32" dur="3.959">space by factor of eight we say it&amp;#39;s</text><text start="528" dur="4.32">three bits of information and so on and</text><text start="530.279" dur="4.721">so forth four bits cuts it into a 16th</text><text start="532.32" dur="3.68">five bits cuts it into a 30 second so</text><text start="535" dur="2.88">now it&amp;#39;s when you might want to take a</text><text start="536" dur="4.079">moment and pause and ask for yourself</text><text start="537.88" dur="4">what is the formula for information for</text><text start="540.079" dur="3.121">the number of bits in terms of the</text><text start="541.88" dur="3.12">probability of An</text><text start="543.2" dur="3.52">Occurrence well what we&amp;#39;re saying here</text><text start="545" dur="3.32">is basically that when you take 1/2 to</text><text start="546.72" dur="3.48">the number of bits that&amp;#39;s the same thing</text><text start="548.32" dur="3.4">as the probability which is the same</text><text start="550.2" dur="2.96">thing as saying two to the power of the</text><text start="551.72" dur="3.48">number of bits is one over the</text><text start="553.16" dur="4.239">probability which rearranges further to</text><text start="555.2" dur="4.92">saying the information is the log base 2</text><text start="557.399" dur="4.161">of 1 / the probability and sometimes you</text><text start="560.12" dur="2.76">see this with one more rearrangement</text><text start="561.56" dur="4">still where the information is the</text><text start="562.88" dur="4.24">negative log base 2 of the probability</text><text start="565.56" dur="3.36">expressed like this it can look a little</text><text start="567.12" dur="3.92">bit weird to the uninitiated but it</text><text start="568.92" dur="3.64">really is the very intuitive idea of</text><text start="571.04" dur="4.44">asking how many times you&amp;#39;ve cut down</text><text start="572.56" dur="3.92">your possibilities in half now if you&amp;#39;re</text><text start="575.48" dur="2.479">wondering you know I thought we were</text><text start="576.48" dur="3.44">just playing a fun word game why your</text><text start="577.959" dur="3.521">logarithms entering the picture one</text><text start="579.92" dur="3.359">reason this is a nicer unit is it&amp;#39;s just</text><text start="581.48" dur="3.44">a lot easier to talk about very unlikely</text><text start="583.279" dur="3.961">events much easier to say that an</text><text start="584.92" dur="3.84">observation has 20 bits of information</text><text start="587.24" dur="4.719">then it is to say that the probability</text><text start="588.76" dur="3.199">of such and such occurring is</text><text start="592.279" dur="3.961">0.0095 but a more substantive reason</text><text start="594.56" dur="3.48">that this logarithmic expression turned</text><text start="596.24" dur="3.92">out to be a very useful addition to the</text><text start="598.04" dur="4.56">theory of probability is the way that</text><text start="600.16" dur="4.08">information adds together for example if</text><text start="602.6" dur="3.359">one observation gives you two bits of</text><text start="604.24" dur="3.839">information cutting your space down by</text><text start="605.959" dur="4.241">four and then a second observation like</text><text start="608.079" dur="3.641">your second guess in Wordle gives you</text><text start="610.2" dur="3.12">another three bits of information</text><text start="611.72" dur="3.72">chopping you down further by another</text><text start="613.32" dur="4.32">factor of eight the two together give</text><text start="615.44" dur="4.16">you five bits of information in the same</text><text start="617.64" dur="4.72">way that probabilities like to multiply</text><text start="619.6" dur="4.04">information likes to add so as soon as</text><text start="622.36" dur="2.64">we&amp;#39;re in the realm of something like an</text><text start="623.64" dur="3.199">expected value where we&amp;#39;re adding a</text><text start="625" dur="4.12">bunch of numbers up the logs make it a</text><text start="626.839" dur="4">lot nicer to deal with let&amp;#39;s go back to</text><text start="629.12" dur="3.52">our distribution for weary and add</text><text start="630.839" dur="3.44">another little tracker on here showing</text><text start="632.64" dur="3.52">us how much information there is for</text><text start="634.279" dur="3.441">each pattern the main thing I want you</text><text start="636.16" dur="2.919">to notice is that the higher the</text><text start="637.72" dur="3.04">probability as we get to those more</text><text start="639.079" dur="4.44">likely patterns the lower the</text><text start="640.76" dur="4.519">information the fewer bits you gain the</text><text start="643.519" dur="3.76">way we measure the quality of this guess</text><text start="645.279" dur="3.761">will be to take the expected value of</text><text start="647.279" dur="3.601">this information where we go through</text><text start="649.04" dur="3.56">each pattern we say how probable is it</text><text start="650.88" dur="4.24">and then we multiply that by how many</text><text start="652.6" dur="4.359">bits of information do we get and in the</text><text start="655.12" dur="5.04">example of weary that turns out to be</text><text start="656.959" dur="5.161">4.9 bits so on average the information</text><text start="660.16" dur="3.2">you get from this opening guess is as</text><text start="662.12" dur="3.8">good as chopping your space of</text><text start="663.36" dur="4.44">possibilities in half about five times</text><text start="665.92" dur="4.28">by contrast an example of a guess with a</text><text start="667.8" dur="4.279">higher expected information value would</text><text start="670.2" dur="3.879">be something like</text><text start="672.079" dur="4.161">slate in this case you&amp;#39;ll notice the</text><text start="674.079" dur="3.921">distribution looks a lot flatter in</text><text start="676.24" dur="4.399">particular the most probable occurrence</text><text start="678" dur="4.6">of all Grays only has about a 6% chance</text><text start="680.639" dur="4.121">of occurring so at minimum you&amp;#39;re</text><text start="682.6" dur="4.2">getting evidently 3.9 bits of</text><text start="684.76" dur="3.319">information but that&amp;#39;s a minimum more</text><text start="686.8" dur="3.08">typically you&amp;#39;d get something better</text><text start="688.079" dur="3.281">than that and it turns out when you</text><text start="689.88" dur="3.6">crunch the numbers on this one and you</text><text start="691.36" dur="5">add up all of the relevant terms the</text><text start="693.48" dur="5.84">average information is about</text><text start="696.36" dur="4.8">5.8 so in contrast with Wei your space</text><text start="699.32" dur="5.04">of possibilities will be about half as</text><text start="701.16" dur="4.6">big after this first guess on average</text><text start="704.36" dur="3.08">there&amp;#39;s actually a fun story about the</text><text start="705.76" dur="3.759">name for this expected value of</text><text start="707.44" dur="3.8">information quantity you see information</text><text start="709.519" dur="3.081">theory was developed by Claude Shannon</text><text start="711.24" dur="3.48">who was working at Bell labs in the</text><text start="712.6" dur="4">1940s but he was talking about some of</text><text start="714.72" dur="3.96">his yet to be published ideas with John</text><text start="716.6" dur="3.96">Von noyman who was this intellectual</text><text start="718.68" dur="3.52">giant of the time very prominent in math</text><text start="720.56" dur="3.68">and physics and the beginnings of what</text><text start="722.2" dur="3.16">was becoming computer science and when</text><text start="724.24" dur="3.12">he mentioned that he didn&amp;#39;t really have</text><text start="725.36" dur="4.279">a good name for this expected value of</text><text start="727.36" dur="4.68">information quantity but noyman</text><text start="729.639" dur="4.161">supposedly said so the story goes well</text><text start="732.04" dur="3.52">you should call it entropy and for two</text><text start="733.8" dur="3.399">reasons in the first place your</text><text start="735.56" dur="3.8">uncertainty function has been used in</text><text start="737.199" dur="4.281">statistical mechanics under that name so</text><text start="739.36" dur="4.039">it already has a name and in the second</text><text start="741.48" dur="3.96">place and more important nobody knows</text><text start="743.399" dur="4.401">what entropy really is so in a debate</text><text start="745.44" dur="4.04">you&amp;#39;ll always have the advantage so if</text><text start="747.8" dur="3.44">the name seems a little bit mysterious</text><text start="749.48" dur="4.2">and if this story is to be believed</text><text start="751.24" dur="4.039">that&amp;#39;s kind of by Design also if you&amp;#39;re</text><text start="753.68" dur="3.32">wondering about its relation to all of</text><text start="755.279" dur="3.481">that second law of thermodynamic stuff</text><text start="757" dur="3.72">from physics there definitely is a</text><text start="758.76" dur="3.639">connection but in its Origins Shannon</text><text start="760.72" dur="3.72">was just dealing with pure probability</text><text start="762.399" dur="3.721">Theory and for our purposes here when I</text><text start="764.44" dur="4.12">use the word entropy I just want you to</text><text start="766.12" dur="5.079">think the expected information value of</text><text start="768.56" dur="4.76">a particular guess you can think of</text><text start="771.199" dur="4.681">entropy as measuring two things</text><text start="773.32" dur="4.519">simultaneously the first one is how flat</text><text start="775.88" dur="3.759">is the distribution the closer a</text><text start="777.839" dur="4.041">distribution is to uniform</text><text start="779.639" dur="4.081">the higher that entropy will be in our</text><text start="781.88" dur="4.36">case where there are 3 to the 5th total</text><text start="783.72" dur="4.44">patterns for a uniform distribution</text><text start="786.24" dur="4.839">observing any one of them would have</text><text start="788.16" dur="4">information log base 2 of 3 to 5th which</text><text start="791.079" dur="3.801">happens to be</text><text start="792.16" dur="4.64">7.92 so that is the absolute maximum</text><text start="794.88" dur="4.28">that you could possibly have for this</text><text start="796.8" dur="4.24">entropy but entropy is also kind of a</text><text start="799.16" dur="3.679">measure of how many possibilities there</text><text start="801.04" dur="3.359">are in the first place for example if</text><text start="802.839" dur="4.161">you happen to have some word where</text><text start="804.399" dur="5.12">there&amp;#39;s only 16 possible patterns and</text><text start="807" dur="4.76">each one is equally likely this entropy</text><text start="809.519" dur="4.281">this expected information would be four</text><text start="811.76" dur="4.12">bits but if you have another word where</text><text start="813.8" dur="4.36">there&amp;#39;s 64 possible patterns that could</text><text start="815.88" dur="3.84">come up and they&amp;#39;re all equally likely</text><text start="818.16" dur="4.52">then the entropy would work out to be</text><text start="819.72" dur="4.96">six bits so if you see some distribution</text><text start="822.68" dur="3.959">out in the wild that has an entropy of</text><text start="824.68" dur="4.24">six bits it&amp;#39;s sort of like it&amp;#39;s saying</text><text start="826.639" dur="4.44">there&amp;#39;s as much variation and</text><text start="828.92" dur="5.279">uncertainty in what&amp;#39;s about to happen as</text><text start="831.079" dur="5.2">if there were 64 equally likely outcomes</text><text start="834.199" dur="4.401">for my first pass at the world bot I</text><text start="836.279" dur="4.12">basically had it just do this it goes</text><text start="838.6" dur="3.76">through all of the different possible</text><text start="840.399" dur="4.24">guesses that you could have all 13,000</text><text start="842.36" dur="4.76">words it computes the entropy for each</text><text start="844.639" dur="4.44">one or more specifically the entropy of</text><text start="847.12" dur="4.24">the distribution across all patterns</text><text start="849.079" dur="3.721">that you might see for each one and then</text><text start="851.36" dur="2.76">it picks the highest since that&amp;#39;s the</text><text start="852.8" dur="3.24">one that&amp;#39;s likely to chop down your</text><text start="854.12" dur="3.839">space of possibilities as much as</text><text start="856.04" dur="3.4">possible and even though I&amp;#39;ve only been</text><text start="857.959" dur="2.841">talking about the first guess here it</text><text start="859.44" dur="3.519">does the same thing for the next few</text><text start="860.8" dur="3.88">guesses for example after you see some</text><text start="862.959" dur="3.401">pattern on that first guess which would</text><text start="864.68" dur="3.159">restrict you to a smaller number of</text><text start="866.36" dur="3.56">possible words based on what matches</text><text start="867.839" dur="3.601">with that you just play the same game</text><text start="869.92" dur="3.68">with respect to that smaller set of</text><text start="871.44" dur="4.24">words for a proposed second guess you</text><text start="873.6" dur="3.32">look at the distribution of all patterns</text><text start="875.68" dur="3.76">that could occur from that more</text><text start="876.92" dur="4.64">restricted set of words you search</text><text start="879.44" dur="4.88">through all 13,000 possibilities and you</text><text start="881.56" dur="5">find the one that maximizes that</text><text start="884.32" dur="3.84">entropy to show you how this works in</text><text start="886.56" dur="3.12">action let me just pull up a little</text><text start="888.16" dur="3.359">variant of Wordle that I wrote that</text><text start="889.68" dur="3.24">shows the highlights of this analysis in</text><text start="891.519" dur="3.801">the</text><text start="892.92" dur="3.68">margins so after doing all its entropy</text><text start="895.32" dur="3.28">calculations on the right here it&amp;#39;s</text><text start="896.6" dur="4.28">showing us which ones have the highest</text><text start="898.6" dur="3.56">expected information turns out the top</text><text start="900.88" dur="5.56">answer at least at the moment we&amp;#39;ll</text><text start="902.16" dur="8.84">refine this later is tears which means</text><text start="906.44" dur="5.959">um of course a vetch the most common vet</text><text start="911" dur="2.399">each time we make a guess here where</text><text start="912.399" dur="2.761">maybe I kind of ignore its</text><text start="913.399" dur="4.481">recommendations and go with slate cuz I</text><text start="915.16" dur="4.64">like slate we can see how much expected</text><text start="917.88" dur="3.199">information it had but then on the right</text><text start="919.8" dur="3.599">of the word here it&amp;#39;s showing us how</text><text start="921.079" dur="4.361">much actual information we got given</text><text start="923.399" dur="3.56">this particular pattern so here it looks</text><text start="925.44" dur="3.28">like we were a little unlucky we were</text><text start="926.959" dur="3.641">expected to get 5.8 but we happened to</text><text start="928.72" dur="3.28">get something with less than that and</text><text start="930.6" dur="3.12">then on the left side here it&amp;#39;s showing</text><text start="932" dur="4.44">us all of the different possible words</text><text start="933.72" dur="4.359">given where we are now the blue bars are</text><text start="936.44" dur="3.72">telling us How likely it thinks each</text><text start="938.079" dur="4">word is so at the moment it&amp;#39;s assuming</text><text start="940.16" dur="4.119">each word is equally likely to occur but</text><text start="942.079" dur="3.68">we&amp;#39;ll refine that in a moment and then</text><text start="944.279" dur="3.961">this uncertainty measurement is telling</text><text start="945.759" dur="4.841">us the entropy of this distribution</text><text start="948.24" dur="4.079">across the possible Words which right</text><text start="950.6" dur="3.96">now because it&amp;#39;s a uniform distribution</text><text start="952.319" dur="4.241">is just a needlessly complicated way to</text><text start="954.56" dur="3.88">count the number of possibilities for</text><text start="956.56" dur="5.32">example if we were to take 2 to the^ of</text><text start="958.44" dur="5.519">13 66 that should be around the 13,000</text><text start="961.88" dur="3.48">possibilities um a little bit off here</text><text start="963.959" dur="3.521">but only because I&amp;#39;m not showing all the</text><text start="965.36" dur="3.719">decimal places at the moment that might</text><text start="967.48" dur="3.159">feel redundant and like it&amp;#39;s overly</text><text start="969.079" dur="2.961">complicating things but you&amp;#39;ll see why</text><text start="970.639" dur="2.841">it&amp;#39;s useful to have both numbers in a</text><text start="972.04" dur="2.96">minute so here it looks like it&amp;#39;s</text><text start="973.48" dur="4.52">suggesting the highest entropy for a</text><text start="975" dur="6.279">second guess is ramen which again just</text><text start="978" dur="4.519">really doesn&amp;#39;t feel like a word so to</text><text start="981.279" dur="3.201">take the moral High Ground here I&amp;#39;m</text><text start="982.519" dur="4.201">going to go ahead and type in</text><text start="984.48" dur="4.159">Reigns and again it looks like we were a</text><text start="986.72" dur="4.2">little unlucky we were expecting 4.3</text><text start="988.639" dur="5.241">bits and we only got 3.39 bits of</text><text start="990.92" dur="5.2">information so that takes us down to 55</text><text start="993.88" dur="3.759">possibilities and here maybe I&amp;#39;ll just</text><text start="996.12" dur="4.519">actually go with what it&amp;#39;s suggesting</text><text start="997.639" dur="4.721">which is comu whatever that means and</text><text start="1000.639" dur="3.241">okay this is actually um a good chance</text><text start="1002.36" dur="4.52">for a puzzle it&amp;#39;s telling us this</text><text start="1003.88" dur="5">pattern gives us 4.7 bits of information</text><text start="1006.88" dur="4.399">but over on the left before we see that</text><text start="1008.88" dur="4.879">pattern there were 5.78 bits of</text><text start="1011.279" dur="4.081">uncertainty so as a quiz for you what</text><text start="1013.759" dur="3.281">does that mean about the number of</text><text start="1015.36" dur="3.599">remaining</text><text start="1017.04" dur="4.68">possibilities well it means that we&amp;#39;re</text><text start="1018.959" dur="4.081">reduced down to one bit of uncertainty</text><text start="1021.72" dur="3.04">which is the same thing as saying that</text><text start="1023.04" dur="4.399">there&amp;#39;s two possible answers it&amp;#39;s a</text><text start="1024.76" dur="4.36">50/50 choice and from here because you</text><text start="1027.439" dur="3.64">and I know which words are more common</text><text start="1029.12" dur="3.079">we know that the answer should be Abyss</text><text start="1031.079" dur="2.921">but as it&amp;#39;s written right now the</text><text start="1032.199" dur="3.24">program doesn&amp;#39;t know that so it just</text><text start="1034" dur="3.6">keeps going trying to gain as much</text><text start="1035.439" dur="4.12">information as it can until it&amp;#39;s only</text><text start="1037.6" dur="4.319">one possibility left and then it guesses</text><text start="1039.559" dur="4.041">it so obviously we need a better endgame</text><text start="1041.919" dur="3.801">strategy but let&amp;#39;s say we call this</text><text start="1043.6" dur="3.839">version one of our Wordle solver and</text><text start="1045.72" dur="4.88">then we go and run some simulations to</text><text start="1047.439" dur="4.681">see how it does</text><text start="1050.6" dur="3.68">so the way this is working is it&amp;#39;s</text><text start="1052.12" dur="4.919">playing every possible Wordle game it&amp;#39;s</text><text start="1054.28" dur="4.6">going through all of those 2315 words</text><text start="1057.039" dur="4.201">that are the actual word answers it&amp;#39;s</text><text start="1058.88" dur="3.96">basically using that as a testing set</text><text start="1061.24" dur="3.64">and with this naive method of not</text><text start="1062.84" dur="3.64">considering how common a word is and</text><text start="1064.88" dur="3.52">just trying to maximize the information</text><text start="1066.48" dur="4.12">at each step along the way until it gets</text><text start="1068.4" dur="3.68">down to one and only one choice by the</text><text start="1070.6" dur="3.48">end of the simulation the average score</text><text start="1072.08" dur="5.56">works out to be about</text><text start="1074.08" dur="5.599">4124 which you know it&amp;#39;s not bad to be</text><text start="1077.64" dur="3.519">honest I kind of expected to do worse</text><text start="1079.679" dur="3.24">but the people who play Wordle will tell</text><text start="1081.159" dur="3.321">you that they can usually get it in four</text><text start="1082.919" dur="3.361">the real challenge is to get as many in</text><text start="1084.48" dur="3.16">three as you can it&amp;#39;s a pretty big jump</text><text start="1086.28" dur="3.72">between the score of four and the score</text><text start="1087.64" dur="4">of three the obvious low hanging fruit</text><text start="1090" dur="3.919">here is to somehow incorporate whether</text><text start="1091.64" dur="4.8">or not a word is common and how exactly</text><text start="1093.919" dur="2.521">do we do</text><text start="1099.6" dur="4.959">[Music]</text><text start="1102.08" dur="4.28">that the way I approached it is to get a</text><text start="1104.559" dur="3.961">list of the relative frequencies for all</text><text start="1106.36" dur="4.16">of the words in the English language and</text><text start="1108.52" dur="3.92">I just used mathematica&amp;#39;s word frequency</text><text start="1110.52" dur="3.88">data function which itself pulls from</text><text start="1112.44" dur="3.96">the Google Books English engram public</text><text start="1114.4" dur="3.56">data set and it&amp;#39;s kind of fun to look at</text><text start="1116.4" dur="3.6">for example if we sort it from the most</text><text start="1117.96" dur="3.48">common words to the least common words</text><text start="1120" dur="3.799">evidently these are the most common</text><text start="1121.44" dur="4.719">felet words in the English language or</text><text start="1123.799" dur="4.441">rather these is the eighth most common</text><text start="1126.159" dur="4.52">first is which after which there&amp;#39;s there</text><text start="1128.24" dur="4.04">and there first itself is not first but</text><text start="1130.679" dur="3.641">ninth and it makes sense that these</text><text start="1132.28" dur="4.12">other words could come about more often</text><text start="1134.32" dur="3.92">where those after first are after where</text><text start="1136.4" dur="2.84">and those being just a little bit less</text><text start="1138.24" dur="3.28">common</text><text start="1139.24" dur="4.16">now in using this data to model How</text><text start="1141.52" dur="3.519">likely each of these words is to be the</text><text start="1143.4" dur="3.639">final answer it shouldn&amp;#39;t just be</text><text start="1145.039" dur="5.321">proportional to the frequency cuz for</text><text start="1147.039" dur="5.401">example which is given a score of 0.002</text><text start="1150.36" dur="4">in this data set whereas the word braid</text><text start="1152.44" dur="4.16">is in some sense about a thousand times</text><text start="1154.36" dur="3.319">less likely but both of these are common</text><text start="1156.6" dur="3.04">enough words that they&amp;#39;re almost</text><text start="1157.679" dur="4.641">certainly worth considering so we want</text><text start="1159.64" dur="4.519">more of a binary cut off the way I went</text><text start="1162.32" dur="4.599">about it is to imagine taking this whole</text><text start="1164.159" dur="5.081">sorted list of words and then arranging</text><text start="1166.919" dur="4.12">it on an x-axis and then applying the</text><text start="1169.24" dur="3.52">sigmoid function which is the standard</text><text start="1171.039" dur="3.441">way to have a function whose output is</text><text start="1172.76" dur="3.76">basically binary it&amp;#39;s either zero or</text><text start="1174.48" dur="4.52">it&amp;#39;s one but there&amp;#39;s a smoothing in</text><text start="1176.52" dur="4.24">between for that region of uncertainty</text><text start="1179" dur="3.76">so essentially the probability that I&amp;#39;m</text><text start="1180.76" dur="4.24">assigning to each word for being in the</text><text start="1182.76" dur="4.72">final list will be the value of the</text><text start="1185" dur="3.48">sigmoid function above wherever it sits</text><text start="1187.48" dur="3.36">on the</text><text start="1188.48" dur="4.439">x-axis now obviously this depends on a</text><text start="1190.84" dur="4.48">few parameters for example how wide a</text><text start="1192.919" dur="4.321">space on the x-axis those words fill</text><text start="1195.32" dur="4.08">determines how gradually or steeply we</text><text start="1197.24" dur="3.919">drop off from 1 to Z</text><text start="1199.4" dur="4.159">and where we situate them left to right</text><text start="1201.159" dur="3.76">determines the cut off and to be honest</text><text start="1203.559" dur="2.921">the way I did this was kind of just</text><text start="1204.919" dur="3.401">licking my finger and sticking it into</text><text start="1206.48" dur="3.64">the wind I looked through the sorted</text><text start="1208.32" dur="3.839">list and tried to find a window where</text><text start="1210.12" dur="3.559">when I looked at it I figured about half</text><text start="1212.159" dur="3.481">of these words are more likely than not</text><text start="1213.679" dur="4.601">to be the final answer and use that as</text><text start="1215.64" dur="4.68">the cut off now once we have a</text><text start="1218.28" dur="3.56">distribution like this across the words</text><text start="1220.32" dur="3.08">it gives us another situation where</text><text start="1221.84" dur="3.56">entropy becomes this really useful</text><text start="1223.4" dur="3.72">measurement for example let&amp;#39;s say we</text><text start="1225.4" dur="4.24">were playing a game and we start with my</text><text start="1227.12" dur="4.32">old openers which were other nails and</text><text start="1229.64" dur="4.08">we end up with a situation where there&amp;#39;s</text><text start="1231.44" dur="3.8">four possible words that match it and</text><text start="1233.72" dur="3.64">let&amp;#39;s say we consider them all equally</text><text start="1235.24" dur="4.84">likely let me ask you what is the</text><text start="1237.36" dur="5.199">entropy of this</text><text start="1240.08" dur="4.36">distribution well the information</text><text start="1242.559" dur="3.721">associated with each one of these</text><text start="1244.44" dur="4.359">possibilities is going to be the log</text><text start="1246.28" dur="4.879">base 2 of four since each one is one and</text><text start="1248.799" dur="4.161">four and that&amp;#39;s two it&amp;#39;s two bits of</text><text start="1251.159" dur="4.681">information four possibilities all very</text><text start="1252.96" dur="4.4">well and good but what if I told you</text><text start="1255.84" dur="3.52">that actually there&amp;#39;s more than four</text><text start="1257.36" dur="4.319">matches in reality when we look through</text><text start="1259.36" dur="4.36">the full word list There are 16 words</text><text start="1261.679" dur="4.041">that match it but suppose our model puts</text><text start="1263.72" dur="4">a really low probability on those other</text><text start="1265.72" dur="4.04">12 words of actually being The Final</text><text start="1267.72" dur="4.16">Answer something like 1 in a thousand</text><text start="1269.76" dur="4.56">cuz they&amp;#39;re really obscure now let me</text><text start="1271.88" dur="4.6">ask you what is the entropy of this</text><text start="1274.32" dur="4.44">distribution if entropy was purely</text><text start="1276.48" dur="3.679">measuring the number of matches here</text><text start="1278.76" dur="4.2">then you might expect it to be something</text><text start="1280.159" dur="4.801">like the log base 2 of 16 which would be</text><text start="1282.96" dur="4.079">four two more bits of uncertainty than</text><text start="1284.96" dur="3.92">we had before but of course the actual</text><text start="1287.039" dur="3.481">uncertainty is not really that different</text><text start="1288.88" dur="3.6">from what we had before cuz just because</text><text start="1290.52" dur="3.32">there&amp;#39;s these 12 really obscure words</text><text start="1292.48" dur="3.04">doesn&amp;#39;t mean that it would be all that</text><text start="1293.84" dur="4.6">more surprising to learn that The Final</text><text start="1295.52" dur="4.56">Answer is charm for example so when you</text><text start="1298.44" dur="3">actually do the calculation here and you</text><text start="1300.08" dur="3">add up the probability of each</text><text start="1301.44" dur="5.08">occurrence times the corresponding</text><text start="1303.08" dur="5.24">information what you get is 2.11 bits</text><text start="1306.52" dur="3.56">just saying it&amp;#39;s basically two bits it&amp;#39;s</text><text start="1308.32" dur="3.12">basically those four possibilities but</text><text start="1310.08" dur="3.199">there&amp;#39;s a little more uncertainty</text><text start="1311.44" dur="3.28">because of all of those highly unlikely</text><text start="1313.279" dur="3.681">events though if you did learn them</text><text start="1314.72" dur="3.88">you&amp;#39;d get a ton of information from it</text><text start="1316.96" dur="3.36">so zooming out this is part of what</text><text start="1318.6" dur="3.52">makes Wordle such a nice example for an</text><text start="1320.32" dur="3.88">information Theory lesson we have these</text><text start="1322.12" dur="4.559">two distinct feeling applications for</text><text start="1324.2" dur="4.04">entropy the first one telling us what&amp;#39;s</text><text start="1326.679" dur="4.321">the expected information we&amp;#39;ll get from</text><text start="1328.24" dur="5.319">a given guess and the second one saying</text><text start="1331" dur="4.32">can we measure the remaining uncertainty</text><text start="1333.559" dur="3.961">among all of the words that we have</text><text start="1335.32" dur="3.56">possible and I should emphasize in that</text><text start="1337.52" dur="3.399">first case where we&amp;#39;re looking at the</text><text start="1338.88" dur="4.039">expected information of a guess once we</text><text start="1340.919" dur="4.12">have an unequal waiting to the words</text><text start="1342.919" dur="3.721">that affects the entropy calculation for</text><text start="1345.039" dur="2.961">example let me pull up that same case we</text><text start="1346.64" dur="3.639">were looking at earlier of the</text><text start="1348" dur="3.88">distribution associated with Wei but</text><text start="1350.279" dur="4.121">this time using a non-uniform</text><text start="1351.88" dur="4.52">distribution across all possible words</text><text start="1354.4" dur="5.279">so let me see if I can find a part here</text><text start="1356.4" dur="6.24">that illustrates it pretty</text><text start="1359.679" dur="4.441">well uh okay here this is pretty good</text><text start="1362.64" dur="3.56">here we have two adjacent patterns that</text><text start="1364.12" dur="4.36">are about equally likely but one of them</text><text start="1366.2" dur="4.359">we&amp;#39;re told has 32 possible words that</text><text start="1368.48" dur="4.4">match it and if we check what they are</text><text start="1370.559" dur="4.36">these are those 32 which are all just</text><text start="1372.88" dur="3.279">very unlikely words as you scan your</text><text start="1374.919" dur="2.88">eyes over them you know it&amp;#39;s hard to</text><text start="1376.159" dur="4.161">find any that feel like plausible</text><text start="1377.799" dur="3.641">answers maybe yells but if we look at</text><text start="1380.32" dur="2.8">the neighboring pattern in the</text><text start="1381.44" dur="3.839">distribution which is considered just</text><text start="1383.12" dur="4.439">about as likely we&amp;#39;re told that it only</text><text start="1385.279" dur="4.64">has eight possible matches so a quarter</text><text start="1387.559" dur="4">as many matches but it&amp;#39;s about as likely</text><text start="1389.919" dur="3.281">and when we pull up those matches we can</text><text start="1391.559" dur="5.12">see why some of these are actual</text><text start="1393.2" dur="6.04">plausible answers like ring or wrath or</text><text start="1396.679" dur="4.441">wraps to illustrate how we incorporate</text><text start="1399.24" dur="4.039">all that let me pull up version two of</text><text start="1401.12" dur="3.48">the wordbot here and there are two or</text><text start="1403.279" dur="3.321">three main differences from the first</text><text start="1404.6" dur="3.48">one that we saw first off like I just</text><text start="1406.6" dur="3.52">said the way that we&amp;#39;re Computing these</text><text start="1408.08" dur="4.16">entrop piece these expected values of</text><text start="1410.12" dur="4.08">information is now using the more</text><text start="1412.24" dur="3.52">refined distributions across the</text><text start="1414.2" dur="3.04">patterns that incorporates the</text><text start="1415.76" dur="4.2">probability that a given word would</text><text start="1417.24" dur="5.12">actually be the answer uh as it happens</text><text start="1419.96" dur="4.199">tears is still number one though the</text><text start="1422.36" dur="3.679">ones following are a bit different</text><text start="1424.159" dur="3.241">second when it ranks its top picks it&amp;#39;s</text><text start="1426.039" dur="3.361">now going to keep a model of the</text><text start="1427.4" dur="4">probability that each word is the actual</text><text start="1429.4" dur="4.159">answer and it&amp;#39;ll incorporate that into</text><text start="1431.4" dur="4.84">its decision which is easier to see once</text><text start="1433.559" dur="4.6">we have a few guesses on the table again</text><text start="1436.24" dur="5">ignoring its recommendation because we</text><text start="1438.159" dur="4.281">can&amp;#39;t let machines rule our lives and I</text><text start="1441.24" dur="3.36">suppose I should mention another thing</text><text start="1442.44" dur="4.359">different here is over on the left that</text><text start="1444.6" dur="4.04">uncertainty value that number of bits is</text><text start="1446.799" dur="4.081">no longer just redundant with the number</text><text start="1448.64" dur="3.919">of possible matches now if we pull it up</text><text start="1450.88" dur="5.039">and you know we calculate say 2 to the</text><text start="1452.559" dur="6.321">8.02 which should be a little above 256</text><text start="1455.919" dur="5.721">I guess uh 259 what it&amp;#39;s saying is even</text><text start="1458.88" dur="4.6">though there are 526 total words that</text><text start="1461.64" dur="4">actually match this pattern the amount</text><text start="1463.48" dur="4.24">of uncertainty it has is more akin to</text><text start="1465.64" dur="4.44">what it would be if there were 259</text><text start="1467.72" dur="4.679">equally likely outcomes you can think of</text><text start="1470.08" dur="4.12">it like this it knows Bor is not the</text><text start="1472.399" dur="4">answer same with yorts and Zoro and</text><text start="1474.2" dur="3.68">zorus so it&amp;#39;s a little less uncertain</text><text start="1476.399" dur="4">than it was in the previous case this</text><text start="1477.88" dur="4.08">number of bits will be smaller and if I</text><text start="1480.399" dur="3.441">keep playing the game I&amp;#39;m refining this</text><text start="1481.96" dur="5.56">down with a couple guesses that are appr</text><text start="1483.84" dur="5.92">propo of what I would like to explain</text><text start="1487.52" dur="4.159">here by the fourth guess if you look</text><text start="1489.76" dur="4.72">over at its top picks you can see it&amp;#39;s</text><text start="1491.679" dur="4.561">no longer just maximizing the entropy so</text><text start="1494.48" dur="3.52">at this point there&amp;#39;s technically seven</text><text start="1496.24" dur="4.52">possibilities but the only ones with a</text><text start="1498" dur="4.32">meaning chance are dorms and words and</text><text start="1500.76" dur="3.72">you can see it ranks choosing both of</text><text start="1502.32" dur="4.16">those above all of these other values</text><text start="1504.48" dur="4.079">that strictly speaking would give more</text><text start="1506.48" dur="3.88">information the very first time I did</text><text start="1508.559" dur="3.641">this I just added up these two numbers</text><text start="1510.36" dur="2.919">to measure the quality of each guess</text><text start="1512.2" dur="3.199">which actually worked better than you</text><text start="1513.279" dur="3.52">might suspect but it really didn&amp;#39;t feel</text><text start="1515.399" dur="2.921">systematic and I&amp;#39;m sure there&amp;#39;s other</text><text start="1516.799" dur="3.6">approaches people could take but here&amp;#39;s</text><text start="1518.32" dur="3.88">the one I landed on if we&amp;#39;re considering</text><text start="1520.399" dur="4.081">the prospect of a next guess like in</text><text start="1522.2" dur="4.839">this case words what we really care</text><text start="1524.48" dur="4.679">about is the expected score of our game</text><text start="1527.039" dur="4">if we do that and and to calculate that</text><text start="1529.159" dur="3.52">expected score we say what&amp;#39;s the</text><text start="1531.039" dur="3.561">probability that words is the actual</text><text start="1532.679" dur="5.441">answer which at the moment it describes</text><text start="1534.6" dur="5.72">58% to so we say with a 58% chance our</text><text start="1538.12" dur="4.36">score in this game would be four and</text><text start="1540.32" dur="4.8">then with a probability of one minus</text><text start="1542.48" dur="5">that 58% um our score will be more than</text><text start="1545.12" dur="4.32">that four how much more we don&amp;#39;t know</text><text start="1547.48" dur="4.199">but we can estimate it based on how much</text><text start="1549.44" dur="4.28">uncertainty there&amp;#39;s likely to be once we</text><text start="1551.679" dur="4.801">get to that point specifically at the</text><text start="1553.72" dur="4.959">moment there&amp;#39;s 1.44 bits of uncertainty</text><text start="1556.48" dur="5.319">if we guess words it&amp;#39;s telling us the Ed</text><text start="1558.679" dur="4.921">information we&amp;#39;ll get is 1.27 bits so if</text><text start="1561.799" dur="3.6">we guess words this difference</text><text start="1563.6" dur="3.6">represents how much uncertainty were</text><text start="1565.399" dur="3.88">likely to be left with after that</text><text start="1567.2" dur="4.04">happens what we need is some kind of</text><text start="1569.279" dur="3.681">function which I&amp;#39;m calling F here that</text><text start="1571.24" dur="3.679">Associates this uncertainty with an</text><text start="1572.96" dur="3.4">expected score and the way I went about</text><text start="1574.919" dur="3.601">this was to just plot a bunch of the</text><text start="1576.36" dur="4.679">data from previous games based on</text><text start="1578.52" dur="5.12">version one of the bot to say hey what</text><text start="1581.039" dur="4.281">was the actual score after various</text><text start="1583.64" dur="4.12">points with certain very measurable</text><text start="1585.32" dur="3.959">amounts of uncertainty for example these</text><text start="1587.76" dur="4.24">data points here that are sitting above</text><text start="1589.279" dur="4.961">a value that&amp;#39;s around like 8.7 or so are</text><text start="1592" dur="4.76">saying for some games after a point at</text><text start="1594.24" dur="4.4">which there were 8.7 bits of uncertainty</text><text start="1596.76" dur="3.6">it took two guesses to get the final</text><text start="1598.64" dur="3.44">answer for other games it took three</text><text start="1600.36" dur="3.96">guesses for other games it took four</text><text start="1602.08" dur="4.36">guesses if we shift over to the left</text><text start="1604.32" dur="3.4">here all the points over zero are saying</text><text start="1606.44" dur="3.16">whenever there&amp;#39;s zero bits of</text><text start="1607.72" dur="4.079">uncertainty which is to say there&amp;#39;s only</text><text start="1609.6" dur="3.959">one possibility then the number of</text><text start="1611.799" dur="3.641">guesses required is always just one</text><text start="1613.559" dur="3.72">which is reassuring Whenever there was</text><text start="1615.44" dur="3.28">one bit of uncertainty meaning it was</text><text start="1617.279" dur="3.52">essentially just down to two</text><text start="1618.72" dur="3.72">possibilities then sometimes it required</text><text start="1620.799" dur="4.961">one more guess sometimes it required two</text><text start="1622.44" dur="5.119">more guesses and so on and so forth here</text><text start="1625.76" dur="3.799">maybe a slightly easier way to visualize</text><text start="1627.559" dur="4.321">this data is to bucket it together and</text><text start="1629.559" dur="4.281">take averages for example this bar here</text><text start="1631.88" dur="4.6">is saying among all the points where we</text><text start="1633.84" dur="4.48">had one bit of uncertainty on average</text><text start="1636.48" dur="4.199">the number of new guesses required was</text><text start="1638.32" dur="2.359">about</text><text start="1641.279" dur="3.961">1.5 and the bar over here is saying</text><text start="1643.88" dur="3.12">among all of the different games where</text><text start="1645.24" dur="3.64">at some point the uncertainty was a</text><text start="1647" dur="3.52">little above four bits which is like</text><text start="1648.88" dur="3.679">narrowing it down to 16 different</text><text start="1650.52" dur="3.8">possibilities then on average it</text><text start="1652.559" dur="4.041">requires a little more than two guesses</text><text start="1654.32" dur="3.839">from that point forward and from here I</text><text start="1656.6" dur="3.52">just did a regression to fit a function</text><text start="1658.159" dur="3.601">that seemed reasonable to this and</text><text start="1660.12" dur="3.52">remember the whole point of doing any of</text><text start="1661.76" dur="3.88">that is so that we can quantify this</text><text start="1663.64" dur="4.039">intuition that the more information we</text><text start="1665.64" dur="5.24">gain from a word the lower the expected</text><text start="1667.679" dur="5.521">score will be so with this as version</text><text start="1670.88" dur="3.88">2.0 if we go back and we run the same</text><text start="1673.2" dur="2.8">set of simulations having it play</text><text start="1674.76" dur="4.48">against all</text><text start="1676" dur="5.6">2315 possible Word answers how does it</text><text start="1679.24" dur="3.679">do well in contrast to our first version</text><text start="1681.6" dur="3.36">it&amp;#39;s definitely better which is</text><text start="1682.919" dur="4.921">reassuring all said and done the average</text><text start="1684.96" dur="4.559">is around 3.6 although unlike the first</text><text start="1687.84" dur="3.88">version there are a couple times that it</text><text start="1689.519" dur="4.16">loses and requires more than six in this</text><text start="1691.72" dur="3.559">circumstance presumably because there&amp;#39;s</text><text start="1693.679" dur="3.041">times when it&amp;#39;s making that trade-off to</text><text start="1695.279" dur="2.64">actually go for the goal rather than</text><text start="1696.72" dur="4.28">maximizing</text><text start="1697.919" dur="5.921">information so can we do better than</text><text start="1701" dur="4.76">3.6 we definitely can now I said at the</text><text start="1703.84" dur="3.64">start that it&amp;#39;s most fun to try not</text><text start="1705.76" dur="3.08">incorporating the true list of Wordle</text><text start="1707.48" dur="4.199">answers into the the way that it builds</text><text start="1708.84" dur="4.199">its model but if we do incorporate it</text><text start="1711.679" dur="2.321">the best performance I could get was</text><text start="1713.039" dur="3.081">around</text><text start="1714" dur="3.76">3.43 so if we try to get more</text><text start="1716.12" dur="3.799">sophisticated than just using word</text><text start="1717.76" dur="4.72">frequency data to choose this prior</text><text start="1719.919" dur="4.201">distribution this 3.43 probably gives a</text><text start="1722.48" dur="3">Max at how good we could get with that</text><text start="1724.12" dur="3.6">or at least how good I could get with</text><text start="1725.48" dur="3.559">that that best performance essentially</text><text start="1727.72" dur="3">just uses the ideas that I&amp;#39;ve been</text><text start="1729.039" dur="3.201">talking about here but it goes a little</text><text start="1730.72" dur="3.48">further like it does a search for the</text><text start="1732.24" dur="3.919">expected information two steps forward</text><text start="1734.2" dur="3.52">rather than just one originally I was</text><text start="1736.159" dur="3.4">planning on talking more about that but</text><text start="1737.72" dur="3.839">I realize we&amp;#39;ve actually gone quite long</text><text start="1739.559" dur="3.321">as it is the one thing I&amp;#39;ll say is after</text><text start="1741.559" dur="3.081">doing this two-step search and then</text><text start="1742.88" dur="3.919">running a couple sample simulations in</text><text start="1744.64" dur="3.879">the top candidates so far for me at</text><text start="1746.799" dur="4.6">least it&amp;#39;s looking like crane is the</text><text start="1748.519" dur="4.4">best opener who would have guessed also</text><text start="1751.399" dur="3.441">if you use the true word list to</text><text start="1752.919" dur="3.64">determine your space of possibilities</text><text start="1754.84" dur="3.92">then the uncertainty you start with is a</text><text start="1756.559" dur="3.881">little over 11 bits and it turns out</text><text start="1758.76" dur="4">just from a Brute Force search the</text><text start="1760.44" dur="5.04">maximum possible expected information</text><text start="1762.76" dur="5.36">after the first two guesses is around 10</text><text start="1765.48" dur="5.039">bits which suggests that best case</text><text start="1768.12" dur="4.399">scenario after your first two guesses</text><text start="1770.519" dur="4.561">with perfectly optimal play you&amp;#39;ll be</text><text start="1772.519" dur="4.04">left with around one bit of uncertainty</text><text start="1775.08" dur="3.36">which is the same as being down to two</text><text start="1776.559" dur="3.521">possible guesses so I think it&amp;#39;s fair</text><text start="1778.44" dur="3.359">and probably pretty conservative to say</text><text start="1780.08" dur="3.439">that you could never possibly write an</text><text start="1781.799" dur="3.48">algorithm that gets this average as low</text><text start="1783.519" dur="3.64">as three because with the words</text><text start="1785.279" dur="4.24">available to you there&amp;#39;s simply not room</text><text start="1787.159" dur="4.201">to get enough information after only two</text><text start="1789.519" dur="3.601">steps to be able to guarantee the answer</text><text start="1791.36" dur="5.08">in the third slot every single time</text><text start="1793.12" dur="3.32">without fail</text><text start="1796.56" dur="11.489">[Music]</text><text start="1816.21" dur="3.519">[Music]</text><text start="1830.56" dur="8.69">[Music]</text></transcript>