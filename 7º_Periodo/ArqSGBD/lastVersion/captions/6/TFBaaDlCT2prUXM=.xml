<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0.68" dur="2.479">[Music]</text><text start="1.319" dur="3.881">imagine you happen across a short movie</text><text start="3.159" dur="4.72">script that describes a scene between a</text><text start="5.2" dur="5.68">person and their AI assistant the script</text><text start="7.879" dur="6.041">has what the person asks the AI but the</text><text start="10.88" dur="4.919">ai&amp;#39;s response has been torn off suppose</text><text start="13.92" dur="4">you also have this powerful magical</text><text start="15.799" dur="4.041">machine that can take any text and</text><text start="17.92" dur="4.24">provide a sensible prediction of what</text><text start="19.84" dur="3.96">word comes next you could then finish</text><text start="22.16" dur="3.48">the script by feeding in what you have</text><text start="23.8" dur="4.68">to the machine seeing what it would</text><text start="25.64" dur="4.84">predict to start the ai&amp;#39;s answer and</text><text start="28.48" dur="3.8">then repeating this over and over</text><text start="30.48" dur="3.72">with a growing script completing the</text><text start="32.28" dur="4.64">dialogue when you interact with a</text><text start="34.2" dur="4.24">chatbot this is exactly what&amp;#39;s happening</text><text start="36.92" dur="3.68">a large language model is a</text><text start="38.44" dur="4.68">sophisticated mathematical function that</text><text start="40.6" dur="4.76">predicts what word comes next for any</text><text start="43.12" dur="4.56">piece of text instead of predicting one</text><text start="45.36" dur="4.76">word with certainty though what it does</text><text start="47.68" dur="5.519">is assign a probability to all possible</text><text start="50.12" dur="4.88">next words to build a chatbot what you</text><text start="53.199" dur="3.921">do is lay out some text that describes</text><text start="55" dur="5.039">an interaction between a user and a</text><text start="57.12" dur="4.88">hypothetical AI assistant you add on</text><text start="60.039" dur="4.08">whatever the user types in as the first</text><text start="62" dur="4.04">part of that interaction and then you</text><text start="64.119" dur="4.241">have the model repeatedly predict the</text><text start="66.04" dur="4.96">next word that such a hypothetical AI</text><text start="68.36" dur="4.759">assistant would say in response and</text><text start="71" dur="3.72">that&amp;#39;s what&amp;#39;s presented to the user in</text><text start="73.119" dur="3.281">doing this the output tends to look a</text><text start="74.72" dur="4.12">lot more natural if you allow it to</text><text start="76.4" dur="4.84">select less likely words along the way</text><text start="78.84" dur="4.76">at random so what this means is even</text><text start="81.24" dur="4.08">though the model itself is deterministic</text><text start="83.6" dur="4.28">a given prompt typically gives a</text><text start="85.32" dur="3.88">different answer each time it&amp;#39;s run</text><text start="87.88" dur="3.76">models learn how to make these</text><text start="89.2" dur="4.48">predictions by processing an enormous</text><text start="91.64" dur="4.24">amount of text typically pulled from the</text><text start="93.68" dur="3.6">internet for a standard human to read</text><text start="95.88" dur="4.279">the amount of text that was used to</text><text start="97.28" dur="6.64">train gpt3 for example if they read</text><text start="100.159" dur="6.24">non-stop 24/7 it would take over 2600</text><text start="103.92" dur="5.04">years larger models since then train on</text><text start="106.399" dur="4.68">much much more you can think of training</text><text start="108.96" dur="4">a little bit like tuning the dials on a</text><text start="111.079" dur="4.36">big machine the way that a language</text><text start="112.96" dur="5.04">model behaves is entirely determined by</text><text start="115.439" dur="5.68">these many different continuous values</text><text start="118" dur="5.36">usually called parameters or weights</text><text start="121.119" dur="4.121">changing those parameters will change</text><text start="123.36" dur="4.6">the probabilities that the model gives</text><text start="125.24" dur="5.359">for the next word on a given input what</text><text start="127.96" dur="4.84">puts the large inlarge language model is</text><text start="130.599" dur="3.601">how they can have hundreds of billions</text><text start="132.8" dur="4.04">of these</text><text start="134.2" dur="5.119">parameters no human ever deliberately</text><text start="136.84" dur="4.679">sets those parameters instead they begin</text><text start="139.319" dur="5.081">at random meaning the model just outputs</text><text start="141.519" dur="5.8">gibberish but they&amp;#39;re repeatedly refined</text><text start="144.4" dur="4.64">based on many example pieces of text one</text><text start="147.319" dur="3.041">of these training examples could be just</text><text start="149.04" dur="3.279">a handful of words</text><text start="150.36" dur="4">or it could be thousands but in either</text><text start="152.319" dur="4.601">case the way this works is to pass in</text><text start="154.36" dur="4.4">all but the last word from that example</text><text start="156.92" dur="4">into the model and compare the</text><text start="158.76" dur="5.119">prediction that it makes with the true</text><text start="160.92" dur="5.76">last word from the example an algorithm</text><text start="163.879" dur="4.881">called back propagation is used to tweak</text><text start="166.68" dur="4.08">all of the parameters in such a way that</text><text start="168.76" dur="4.28">it makes the model a little more likely</text><text start="170.76" dur="4.36">to choose the true last word and a</text><text start="173.04" dur="5.119">little less likely to choose all the</text><text start="175.12" dur="5.399">others when you do this for many many</text><text start="178.159" dur="3.72">trillions of examples not only does the</text><text start="180.519" dur="3.64">model start to give more accurate</text><text start="181.879" dur="3.841">predictions on the training data but it</text><text start="184.159" dur="4.16">also starts to make more reasonable</text><text start="185.72" dur="5.079">predictions on text that it&amp;#39;s never seen</text><text start="188.319" dur="4.401">before given the huge number of</text><text start="190.799" dur="4.52">parameters and the enormous amount of</text><text start="192.72" dur="4.2">training data the scale of computation</text><text start="195.319" dur="3.161">involved in training a large language</text><text start="196.92" dur="3.879">model is</text><text start="198.48" dur="4.6">mindboggling to illustrate imagine that</text><text start="200.799" dur="5.16">you could perform 1 billion additions</text><text start="203.08" dur="4.56">and multiplications every single second</text><text start="205.959" dur="4.081">how long do you think that it would take</text><text start="207.64" dur="4">for you to do all of the operations</text><text start="210.04" dur="3.96">involved in training the largest</text><text start="211.64" dur="6.28">language models do you think it would</text><text start="214" dur="6.28">take a year maybe something like 10,000</text><text start="217.92" dur="6.64">years the answer is actually much more</text><text start="220.28" dur="6.76">than that it&amp;#39;s well over 100 million</text><text start="224.56" dur="4.039">years this is only part of the story</text><text start="227.04" dur="3.839">though this whole process is called</text><text start="228.599" dur="3.881">pre-training the goal of autocompleting</text><text start="230.879" dur="3.681">a random passage of text from the</text><text start="232.48" dur="4.8">Internet is very different from the goal</text><text start="234.56" dur="4.84">of being a good AI assistant to address</text><text start="237.28" dur="4.2">this chatbots undergo another type of</text><text start="239.4" dur="3.919">training just as important called</text><text start="241.48" dur="4.759">reinforcement learning with human</text><text start="243.319" dur="4.721">feedback workers flag unhelpful or</text><text start="246.239" dur="3.84">problematic predictions and their</text><text start="248.04" dur="4.64">Corrections further change the model&amp;#39;s</text><text start="250.079" dur="5.24">parameters making them more likely to</text><text start="252.68" dur="4.6">give predictions that users prefer</text><text start="255.319" dur="4.44">looking back at the pre-training though</text><text start="257.28" dur="4.56">this staggering amount of computation is</text><text start="259.759" dur="3.841">only made possible by using special</text><text start="261.84" dur="4.48">computer chips that are optimized for</text><text start="263.6" dur="5.84">running many many operations in parallel</text><text start="266.32" dur="5.719">known as gpus however not all language</text><text start="269.44" dur="5.52">models models can be easily parallelized</text><text start="272.039" dur="5.761">prior to 2017 most language models would</text><text start="274.96" dur="4.6">process text one word at a time but then</text><text start="277.8" dur="4.44">a team of researchers at Google</text><text start="279.56" dur="5.479">introduced a new model known as the</text><text start="282.24" dur="4.679">Transformer Transformers don&amp;#39;t read text</text><text start="285.039" dur="5.121">from the start to the Finish they soak</text><text start="286.919" dur="5.681">it all in at once in parallel the very</text><text start="290.16" dur="4.36">first step inside a Transformer and most</text><text start="292.6" dur="4.159">other language models for that matter is</text><text start="294.52" dur="4.44">to associate each word with a long list</text><text start="296.759" dur="4.28">of numbers the reason for this is that</text><text start="298.96" dur="4.32">the training process only works with</text><text start="301.039" dur="5.041">continuous values so you have to somehow</text><text start="303.28" dur="4.44">encode language using numbers and each</text><text start="306.08" dur="3.839">of these list of numbers May somehow</text><text start="307.72" dur="4.919">encode the meaning of the corresponding</text><text start="309.919" dur="4.961">word what makes Transformers unique is</text><text start="312.639" dur="5.241">their Reliance on a special operation</text><text start="314.88" dur="4.92">known as attention this operation gives</text><text start="317.88" dur="4.08">all of these lists of numbers a chance</text><text start="319.8" dur="4.36">to talk to one another and refine the</text><text start="321.96" dur="5.48">meanings that they encode based on the</text><text start="324.16" dur="5.12">context around all done in parallel for</text><text start="327.44" dur="4.12">example the numbers encoding the word</text><text start="329.28" dur="4.56">Bank might be changed based on the</text><text start="331.56" dur="5.759">context surrounding it to somehow encode</text><text start="333.84" dur="5.52">the more specific notion of a river bank</text><text start="337.319" dur="4.201">Transformers typically also include a</text><text start="339.36" dur="4.44">second type of operation known as a</text><text start="341.52" dur="4.399">feedforward neural network and this</text><text start="343.8" dur="3.92">gives the model extra capacity to store</text><text start="345.919" dur="4.201">more patterns about language learned</text><text start="347.72" dur="4.199">during training all of this data</text><text start="350.12" dur="3.919">repeatedly flows through many different</text><text start="351.919" dur="4.761">iterations of these two fundamental</text><text start="354.039" dur="5.681">operations and as it does so the hope is</text><text start="356.68" dur="4.88">that each list of numbers is enriched to</text><text start="359.72" dur="4.36">code whatever information might be</text><text start="361.56" dur="5.6">needed to make an accurate prediction of</text><text start="364.08" dur="5.559">what word follows in the passage at the</text><text start="367.16" dur="5">end one final function is performed on</text><text start="369.639" dur="4.761">the last Vector in this sequence which</text><text start="372.16" dur="4.039">now has had a chance to be influenced by</text><text start="374.4" dur="3.44">all the other context from the input</text><text start="376.199" dur="4.081">text as well as everything the model</text><text start="377.84" dur="5.12">learned during training to produce a</text><text start="380.28" dur="4.4">prediction of the next word again the</text><text start="382.96" dur="4.6">model&amp;#39;s prediction looks like a</text><text start="384.68" dur="5.48">probability for every possible next</text><text start="387.56" dur="4.359">word although researchers design the</text><text start="390.16" dur="3.84">framework for how each of these steps</text><text start="391.919" dur="4.481">work it&amp;#39;s important to understand that</text><text start="394" dur="4.639">the specific behavior is an emergent</text><text start="396.4" dur="4.639">phenomenon based on how those hundreds</text><text start="398.639" dur="5.12">of billions of parameters are tuned</text><text start="401.039" dur="4.841">during training this makes it incredibly</text><text start="403.759" dur="4.641">challenging to determine why the model</text><text start="405.88" dur="4.4">makes the exact predictions that it does</text><text start="408.4" dur="3.84">what you can see is that when you use</text><text start="410.28" dur="4.12">large language model predictions to</text><text start="412.24" dur="4.799">autocomplete a prompt the words that it</text><text start="414.4" dur="5.53">generates are uncannily fluent</text><text start="417.039" dur="6.77">fascinating and even useful</text><text start="419.93" dur="3.879">[Music]</text><text start="425.639" dur="2.761">if you&amp;#39;re a new viewer and you&amp;#39;re</text><text start="426.879" dur="3.841">curious about more details on how</text><text start="428.4" dur="4.76">Transformers and attention work boy do I</text><text start="430.72" dur="4.599">have some material for you one option is</text><text start="433.16" dur="4.719">to jump into a series I made about deep</text><text start="435.319" dur="4.921">learning where we visualize and motivate</text><text start="437.879" dur="4.961">the details of attention and all the</text><text start="440.24" dur="4.359">other steps in a Transformer but also on</text><text start="442.84" dur="3.28">my second Channel I just posted a talk</text><text start="444.599" dur="4.561">that I gave a couple months ago about</text><text start="446.12" dur="4.639">this topic for the company TNG in Munich</text><text start="449.16" dur="3.439">sometimes I actually prefer the content</text><text start="450.759" dur="3.961">that I make as a casual talk rather than</text><text start="452.599" dur="3.44">a produced video but I leave it up to</text><text start="454.72" dur="3.92">you which one of these feels like the</text><text start="456.039" dur="7.58">better follow on</text><text start="458.64" dur="4.979">[Music]</text></transcript>