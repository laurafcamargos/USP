<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0.42" dur="6.06">[Music]</text><text start="4.44" dur="4.119">last video I laid out the structure of a</text><text start="6.48" dur="3.44">neural network I&amp;#39;ll give a quick recap</text><text start="8.559" dur="3.521">here just so that it&amp;#39;s fresh in our</text><text start="9.92" dur="4.48">minds and then I have two main goals for</text><text start="12.08" dur="4.8">this video the first is to introduce the</text><text start="14.4" dur="4.24">idea of gradient descent which underlies</text><text start="16.88" dur="3.04">not only how neural networks learn but</text><text start="18.64" dur="3.04">how a lot of other machine learning</text><text start="19.92" dur="3.16">works as well then after that we&amp;#39;re</text><text start="21.68" dur="3.439">going to dig in a little more to how</text><text start="23.08" dur="3.599">this particular Network performs and</text><text start="25.119" dur="3.24">what those hidden layers of neurons end</text><text start="26.679" dur="4.6">up actually looking</text><text start="28.359" dur="5.161">for as a reminder our goal here is the</text><text start="31.279" dur="4.6">classic example of handwritten digit</text><text start="33.52" dur="4.719">recognition the hello world of neural</text><text start="35.879" dur="5.241">networks these digits are rendered on a</text><text start="38.239" dur="6">28x 28 pixel grid each pixel with some</text><text start="41.12" dur="5.279">grayscale value between 0 and 1 those</text><text start="44.239" dur="5.8">are what determine the activations of</text><text start="46.399" dur="5.96">784 neurons in the input layer of the</text><text start="50.039" dur="4.68">network and then the activation for each</text><text start="52.359" dur="5.161">neuron in the following layers is based</text><text start="54.719" dur="4.961">on a weighted sum of all the activations</text><text start="57.52" dur="3.519">in the previous layer plus some special</text><text start="59.68" dur="4.039">number called a</text><text start="61.039" dur="4.561">bias then you compose that sum with some</text><text start="63.719" dur="4.361">other function like the sigmoid squish</text><text start="65.6" dur="4.96">ification or aayu the way that I walked</text><text start="68.08" dur="4.32">through last video in total given the</text><text start="70.56" dur="4.4">somewhat arbitrary choice of two hidden</text><text start="72.4" dur="5.079">layers here with 16 neurons each the</text><text start="74.96" dur="5.36">network has about 13,000 weights and</text><text start="77.479" dur="4.921">biases that we can adjust and it&amp;#39;s these</text><text start="80.32" dur="5.119">values that determine what exactly the</text><text start="82.4" dur="4.64">network you know actually does then what</text><text start="85.439" dur="3.761">we mean when we say that this network</text><text start="87.04" dur="4.16">classifies a given digit is that the</text><text start="89.2" dur="4.76">brightest of those 10 neurons into the</text><text start="91.2" dur="4.4">final layer corresponds to that digit</text><text start="93.96" dur="3.68">and remember the motivation that we had</text><text start="95.6" dur="4.72">in mind here for the layered structure</text><text start="97.64" dur="5">was that maybe the second layer could</text><text start="100.32" dur="4.119">pick up on the edges and the third layer</text><text start="102.64" dur="4.04">might pick up on patterns like loops and</text><text start="104.439" dur="4.241">lines and the last one could just piece</text><text start="106.68" dur="5.079">together those patterns to recognize</text><text start="108.68" dur="5.759">digits so here we learn how the network</text><text start="111.759" dur="4.521">learns what we want is an algorithm</text><text start="114.439" dur="3.64">where you can show this network a whole</text><text start="116.28" dur="3.72">bunch of training data which comes in</text><text start="118.079" dur="4.161">the form of a bunch of different images</text><text start="120" dur="4.2">of handwritten digits along with labels</text><text start="122.24" dur="4.32">for what they&amp;#39;re supposed to be and</text><text start="124.2" dur="4.839">it&amp;#39;ll adjust those 13,000 weights and</text><text start="126.56" dur="5.2">biases so as to improve its performance</text><text start="129.039" dur="4.28">on the training data hopefully this</text><text start="131.76" dur="4.36">layered structure will mean that what it</text><text start="133.319" dur="5.441">learns generalizes to images beyond that</text><text start="136.12" dur="4.6">training data and the way we test that</text><text start="138.76" dur="3.759">is that after you train the network you</text><text start="140.72" dur="3.84">show it more labeled data that it&amp;#39;s</text><text start="142.519" dur="3.681">never seen before and you see how</text><text start="144.56" dur="4.64">accurately it classifies those new</text><text start="146.2" dur="3">images</text><text start="151" dur="3.56">fortunately for us and what makes this</text><text start="152.599" dur="3.64">such a common example to start with is</text><text start="154.56" dur="3.64">that the good people behind the Mist</text><text start="156.239" dur="4.08">database have put together a collection</text><text start="158.2" dur="4.399">of tens of thousands of handwritten</text><text start="160.319" dur="4.761">digit images each one labeled with the</text><text start="162.599" dur="4.201">numbers that they&amp;#39;re supposed to be and</text><text start="165.08" dur="3.72">as provocative as it is to describe a</text><text start="166.8" dur="3.96">machine as learning once you actually</text><text start="168.8" dur="4.24">see how it works it feels a lot less</text><text start="170.76" dur="5.399">like some crazy sci-fi premise and a lot</text><text start="173.04" dur="4.919">more like well a calculus exercise I</text><text start="176.159" dur="5.681">mean basically it comes down to finding</text><text start="177.959" dur="5.721">the minimum of a certain function</text><text start="181.84" dur="4.52">remember conceptually we&amp;#39;re thinking of</text><text start="183.68" dur="4.72">each neuron as being connected to all of</text><text start="186.36" dur="3.879">the neurons in the previous layer and</text><text start="188.4" dur="4.04">the weights in the weighted sum defining</text><text start="190.239" dur="4.481">its activation are kind of like the</text><text start="192.44" dur="4.2">strengths of those connections and the</text><text start="194.72" dur="4.92">bias is some indication of whether that</text><text start="196.64" dur="4.36">neuron tends to be active or inactive</text><text start="199.64" dur="3.04">and to start things off we&amp;#39;re just going</text><text start="201" dur="4.76">to initialize all of those weights and</text><text start="202.68" dur="4.52">biases totally randomly needless to say</text><text start="205.76" dur="3.28">this network is going to perform pretty</text><text start="207.2" dur="3.84">horribly on a given training example</text><text start="209.04" dur="3.8">since it&amp;#39;s just doing something random</text><text start="211.04" dur="4.32">for example you feed in this image of a</text><text start="212.84" dur="5.319">three and the output layer it just looks</text><text start="215.36" dur="5.04">like a mess so what you do is you define</text><text start="218.159" dur="5.761">a cost function a way of telling the</text><text start="220.4" dur="5.64">computer no bad computer that output</text><text start="223.92" dur="4.92">should have activations which are zero</text><text start="226.04" dur="5.919">for most neurons but one for this neuron</text><text start="228.84" dur="4.959">what you gave me is utter trash to say</text><text start="231.959" dur="4">that a little more mathematically what</text><text start="233.799" dur="4.28">you do is add up the squares of the</text><text start="235.959" dur="4.241">differences between each of those trash</text><text start="238.079" dur="4.16">output activations and the value that</text><text start="240.2" dur="4.72">you want them to have and this is what</text><text start="242.239" dur="5.72">we&amp;#39;ll call the cost of a single training</text><text start="244.92" dur="5">example notice this sum is small when</text><text start="247.959" dur="3.56">the network confidently classifies the</text><text start="249.92" dur="3.8">image</text><text start="251.519" dur="3.56">correctly but it&amp;#39;s large when the</text><text start="253.72" dur="3.799">network seems like it doesn&amp;#39;t really</text><text start="255.079" dur="2.44">know what it&amp;#39;s</text><text start="257.639" dur="5.641">doing so then what you do is consider</text><text start="260.079" dur="6.361">the average cost over all of the tens of</text><text start="263.28" dur="5.52">thousands of training examples at your</text><text start="266.44" dur="4.479">disposal this average cost is our</text><text start="268.8" dur="4.64">measure for how how lousy the network is</text><text start="270.919" dur="4.601">and how bad the computer should feel and</text><text start="273.44" dur="3.88">that&amp;#39;s a complicated thing remember how</text><text start="275.52" dur="4.92">the network itself was basically a</text><text start="277.32" dur="6.04">function one that takes in 784 numbers</text><text start="280.44" dur="5.52">as inputs the pixel values and spits out</text><text start="283.36" dur="4.76">10 numbers as its output and in a since</text><text start="285.96" dur="4.799">it&amp;#39;s parameterized by all these weights</text><text start="288.12" dur="5.2">and biases well the cost function is a</text><text start="290.759" dur="5.361">layer of complexity on top of that it</text><text start="293.32" dur="5.159">takes as its input those 13,000 or so</text><text start="296.12" dur="4.6">weights and biases and it spits out a</text><text start="298.479" dur="4.841">single number describing how bad those</text><text start="300.72" dur="4.44">weights and biases are and the way it&amp;#39;s</text><text start="303.32" dur="4.24">defined depends on the Network&amp;#39;s</text><text start="305.16" dur="4.759">Behavior over all the tens of thousands</text><text start="307.56" dur="3.76">of pieces of training data that&amp;#39;s a lot</text><text start="309.919" dur="3.601">to think</text><text start="311.32" dur="4.08">about but just telling the computer what</text><text start="313.52" dur="3.72">a crappy job it&amp;#39;s doing isn&amp;#39;t very</text><text start="315.4" dur="3.919">helpful you want to tell it how to</text><text start="317.24" dur="5">change those weights and biases so that</text><text start="319.319" dur="4.6">it gets better to make it easier rather</text><text start="322.24" dur="4.48">than struggling to imagine a function</text><text start="323.919" dur="5.161">with 13,000 inputs just imagine a simple</text><text start="326.72" dur="5.039">function that has one number as an input</text><text start="329.08" dur="5.44">and one number as an output how do you</text><text start="331.759" dur="5.521">find an input that minimizes the value</text><text start="334.52" dur="4.239">of this function calculus students will</text><text start="337.28" dur="4.32">know that you can sometimes figure out</text><text start="338.759" dur="4.761">that minimum explicitly but that&amp;#39;s not</text><text start="341.6" dur="4.68">always feasible for really complicated</text><text start="343.52" dur="5.239">functions certainly not in the 13,000</text><text start="346.28" dur="4.44">input version of this situation for our</text><text start="348.759" dur="4.56">crazy complicated neural network cost</text><text start="350.72" dur="5.36">function a more flexible tactic is to</text><text start="353.319" dur="4.921">start at any all input and figure out</text><text start="356.08" dur="5">which direction you should step to make</text><text start="358.24" dur="4.959">that output lower specifically if you</text><text start="361.08" dur="4.72">can figure out the slope of the function</text><text start="363.199" dur="4.601">where you are then shift to the left if</text><text start="365.8" dur="5.72">that slope is positive and shift the</text><text start="367.8" dur="3.72">input to the right if that slope is</text><text start="371.599" dur="4.081">negative if you do this repeatedly at</text><text start="374.199" dur="3.161">each point checking the new slope and</text><text start="375.68" dur="3.84">taking the appropriate step you&amp;#39;re going</text><text start="377.36" dur="4.16">to approach some local minimum of the</text><text start="379.52" dur="4.959">function and the image you might have in</text><text start="381.52" dur="4.64">mind here is a ball rolling down a hill</text><text start="384.479" dur="3.801">and notice even for this really</text><text start="386.16" dur="4.28">simplified single input function there</text><text start="388.28" dur="4.56">are many possible valleys that you might</text><text start="390.44" dur="4.44">land in depending on which random input</text><text start="392.84" dur="3.799">you start at and there&amp;#39;s no guarantee</text><text start="394.88" dur="3.56">that the local minimum you land in is</text><text start="396.639" dur="3.721">going to be the smallest possible value</text><text start="398.44" dur="3.8">of the cost function that&amp;#39;s going to</text><text start="400.36" dur="4.2">carry over to our neural network case as</text><text start="402.24" dur="4.519">well and I also want you to notice how</text><text start="404.56" dur="4.28">if you make your step sizes proportional</text><text start="406.759" dur="4.201">to the slope then when the slope is</text><text start="408.84" dur="4.16">flattening out towards the minimum your</text><text start="410.96" dur="3.88">steps get smaller and smaller and that</text><text start="413" dur="4.199">kind of helps you from</text><text start="414.84" dur="4.799">overshooting bumping up the complexity a</text><text start="417.199" dur="4.801">bit imagine instead a function with two</text><text start="419.639" dur="4.881">inputs and one output you might think of</text><text start="422" dur="4.759">the input space as the XY plane and the</text><text start="424.52" dur="5">cost function as being graphed as a</text><text start="426.759" dur="4.84">surface above it now instead of asking</text><text start="429.52" dur="4.56">about the slope of the function you have</text><text start="431.599" dur="5.121">to ask which direction should you step</text><text start="434.08" dur="5.559">in this input space so is to decrease</text><text start="436.72" dur="4.56">the output of the function most quickly</text><text start="439.639" dur="3.641">in other words what&amp;#39;s the downhill</text><text start="441.28" dur="5.319">Direction and again it&amp;#39;s helpful to</text><text start="443.28" dur="5.319">think of a ball rolling down that Hill</text><text start="446.599" dur="3.681">those of you familiar with multivariable</text><text start="448.599" dur="4.16">Calculus will know know that the</text><text start="450.28" dur="5.08">gradient of a function gives you the</text><text start="452.759" dur="4.241">direction of steepest Ascent basically</text><text start="455.36" dur="4.04">which direction should you step to</text><text start="457" dur="4.639">increase the function most quickly</text><text start="459.4" dur="4">naturally enough taking the negative of</text><text start="461.639" dur="4.481">that gradient gives you the direction to</text><text start="463.4" dur="4.96">step that decreases the function most</text><text start="466.12" dur="3.919">quickly and even more than that the</text><text start="468.36" dur="3.519">length of this gradient Vector is</text><text start="470.039" dur="4.641">actually an indication for just how</text><text start="471.879" dur="4.28">steep that steepest slope is now if</text><text start="474.68" dur="3.079">you&amp;#39;re unfamiliar with multivariable</text><text start="476.159" dur="2.76">Calculus and you want to learn more</text><text start="477.759" dur="4.481">check out some of the work that I did</text><text start="478.919" dur="5.601">for k on the topic Honestly though all</text><text start="482.24" dur="4.639">that matters for you and me right now is</text><text start="484.52" dur="4.32">that in principle there exists a way to</text><text start="486.879" dur="3.88">compute this Vector this Vector that</text><text start="488.84" dur="4.52">tells you what the downhill direction is</text><text start="490.759" dur="4.241">and how steep it is you&amp;#39;ll be okay if</text><text start="493.36" dur="2.76">that&amp;#39;s all you know and you&amp;#39;re not rock</text><text start="495" dur="3.56">solid on the</text><text start="496.12" dur="4.759">details cuz if you can get that the</text><text start="498.56" dur="4.56">algorithm for minimizing the function is</text><text start="500.879" dur="4.481">to compute this gradient Direction then</text><text start="503.12" dur="4">take a small step downhill and just</text><text start="505.36" dur="4.279">repeat that over and</text><text start="507.12" dur="4.84">over it&amp;#39;s the same basic idea for</text><text start="509.639" dur="5.361">function that has 13,000 inputs instead</text><text start="511.96" dur="5.759">of two inputs imagine organizing all</text><text start="515" dur="5.919">13,000 weights and biases of our Network</text><text start="517.719" dur="5.88">into a giant column Vector the negative</text><text start="520.919" dur="5.6">gradient of the cost function is just a</text><text start="523.599" dur="5.601">vector it&amp;#39;s some direction inside this</text><text start="526.519" dur="5.161">insanely huge input space that tells you</text><text start="529.2" dur="4.68">which nudges to all of those numbers is</text><text start="531.68" dur="4.92">going to cause the most rapid decrease</text><text start="533.88" dur="4.72">to the cost function and of course with</text><text start="536.6" dur="3.799">our specially designed cost function</text><text start="538.6" dur="4.32">changing the weights of and biases to</text><text start="540.399" dur="4.361">decrease it means making the output of</text><text start="542.92" dur="4.56">the network on each piece of training</text><text start="544.76" dur="5">data look less like a random array of 10</text><text start="547.48" dur="4.4">values and more like an actual decision</text><text start="549.76" dur="4">that we want it to make it&amp;#39;s important</text><text start="551.88" dur="4.84">to remember this cost function involves</text><text start="553.76" dur="4.84">an average over all of the training data</text><text start="556.72" dur="5.72">so if you minimize it it means it&amp;#39;s a</text><text start="558.6" dur="3.84">better performance on all of those</text><text start="563.2" dur="4.4">samples the algorithm for computing this</text><text start="565.839" dur="3.281">gradient efficiently which is</text><text start="567.6" dur="3.32">effectively the heart of how a neural</text><text start="569.12" dur="3.44">network learns is called back</text><text start="570.92" dur="4">propagation and it&amp;#39;s what I&amp;#39;m going to</text><text start="572.56" dur="4">be talking about next video there I</text><text start="574.92" dur="3.84">really want to take the time to walk</text><text start="576.56" dur="4.2">through what exactly happens to each</text><text start="578.76" dur="4.04">weight and each bias for a given piece</text><text start="580.76" dur="3.68">of training data trying to give an</text><text start="582.8" dur="4.08">intuitive feel for what&amp;#39;s happening</text><text start="584.44" dur="4.88">beyond the pile of relevant calculus and</text><text start="586.88" dur="4.24">formulas Right Here Right Now the main</text><text start="589.32" dur="4.199">thing I want you to know independent of</text><text start="591.12" dur="4.04">implementation details is that what we</text><text start="593.519" dur="4.121">mean when we talk about a network</text><text start="595.16" dur="5.2">learning is that it&amp;#39;s just minimizing a</text><text start="597.64" dur="4.4">cost function and notice one consequence</text><text start="600.36" dur="3.68">of that is that it&amp;#39;s important for this</text><text start="602.04" dur="3.96">cost function to have a nice smooth</text><text start="604.04" dur="4.16">output so that we can find a local</text><text start="606" dur="4.399">minimum by taking little steps</text><text start="608.2" dur="4.24">downhill this is why by the way</text><text start="610.399" dur="4.201">artificial neurons have continuously</text><text start="612.44" dur="4.519">ranging activations rather than simply</text><text start="614.6" dur="4.72">being active or inactive in a binary way</text><text start="616.959" dur="5.081">the way that biological neurons</text><text start="619.32" dur="4.68">are this process of repeatedly nudging</text><text start="622.04" dur="3.84">an input of a function by some multiple</text><text start="624" dur="4.36">of the negative gradient is called</text><text start="625.88" dur="4.28">gradient descent it&amp;#39;s a way to converge</text><text start="628.36" dur="4">towards some local minimum of a cost</text><text start="630.16" dur="4.52">function basically a valley in this</text><text start="632.36" dur="3.919">graph I&amp;#39;m still showing the picture of a</text><text start="634.68" dur="3.839">function with two inputs of course</text><text start="636.279" dur="3.961">because nudges in a 13,000 dimensional</text><text start="638.519" dur="3.601">inut space are a little hard to wrap</text><text start="640.24" dur="4.76">your mind around but there is actually a</text><text start="642.12" dur="5.12">nice non-spatial way to think about this</text><text start="645" dur="5.079">each component of the negative gradient</text><text start="647.24" dur="4.56">tells us two things the sign of course</text><text start="650.079" dur="3.721">tells us whether the corresponding</text><text start="651.8" dur="5.12">component of the input Vector should be</text><text start="653.8" dur="5.08">nudged up or down but importantly the</text><text start="656.92" dur="3.2">relative magnitudes of all these</text><text start="658.88" dur="5.519">components</text><text start="660.12" dur="4.279">kind of tells you which changes matter</text><text start="664.48" dur="4.4">more you see in our Network an</text><text start="666.959" dur="3.921">adjustment to one of the weights might</text><text start="668.88" dur="3.68">have a much greater impact on the cost</text><text start="670.88" dur="2.84">function than the adjustment to some</text><text start="672.56" dur="3.68">other</text><text start="673.72" dur="5.84">weight some of these connections just</text><text start="676.24" dur="4.44">matter more for our training data so a</text><text start="679.56" dur="3.279">way that you can think about this</text><text start="680.68" dur="4.959">gradient Vector of our mind-warping</text><text start="682.839" dur="4.721">massive cost function is that it encodes</text><text start="685.639" dur="4.721">the relative importance of each weight</text><text start="687.56" dur="5.399">and bias that is which of these changes</text><text start="690.36" dur="5.12">is going to carry the most bang for your</text><text start="692.959" dur="4.641">buck this really is just another way of</text><text start="695.48" dur="3.68">thinking about direction to take a</text><text start="697.6" dur="3.919">simpler example if you have some</text><text start="699.16" dur="4.56">function with two variables as an input</text><text start="701.519" dur="5.281">and you compute that its gradient at</text><text start="703.72" dur="5.28">some particular Point comes out as</text><text start="706.8" dur="3.839">31 then on the one hand you can</text><text start="709" dur="3.56">interpret that as saying that when</text><text start="710.639" dur="3.801">you&amp;#39;re standing at that input moving</text><text start="712.56" dur="3.959">along this direction increases the</text><text start="714.44" dur="3.88">function most quickly that when you</text><text start="716.519" dur="3.921">graph the function above the plane of</text><text start="718.32" dur="4.6">input points that Vector is what&amp;#39;s</text><text start="720.44" dur="4.399">giving you the straight uphill Direction</text><text start="722.92" dur="4.479">but another way to read that is to say</text><text start="724.839" dur="4.641">that changes to this first variable have</text><text start="727.399" dur="4.44">three times the importance as changes to</text><text start="729.48" dur="4.24">the second variable that at least in the</text><text start="731.839" dur="4.081">neighborhood of the relevant input</text><text start="733.72" dur="4.799">nudging the x value carries a lot more</text><text start="735.92" dur="2.599">bang for your</text><text start="738.72" dur="5.28">buck all right let&amp;#39;s zoom out and sum up</text><text start="741.399" dur="5.481">where we are so far the network itself</text><text start="744" dur="5.199">is this function with 784 inputs and 10</text><text start="746.88" dur="5.079">outputs defined in terms of all of these</text><text start="749.199" dur="5">weighted sums the cost function is a</text><text start="751.959" dur="4.88">layer of complexity on top of that it</text><text start="754.199" dur="5.08">takes the 13,000 weights and biases as</text><text start="756.839" dur="5.56">inputs and spits out a single measure of</text><text start="759.279" dur="5.761">lousiness based on the training examples</text><text start="762.399" dur="5.12">and the gradient of the cost function is</text><text start="765.04" dur="4.72">one more layer of complexity still it</text><text start="767.519" dur="4.56">tells us what nudges to all of these</text><text start="769.76" dur="4.519">weights and biases cause the fastest</text><text start="772.079" dur="3.641">change to the value of the cost function</text><text start="774.279" dur="3.041">which you might interpret as saying</text><text start="775.72" dur="4.799">which changes to which weights matter</text><text start="777.32" dur="3.199">the most</text><text start="782.959" dur="3.801">so when you initialize the network with</text><text start="785" dur="3.519">random weights and biases and adjust</text><text start="786.76" dur="3.8">them many times based on this gradient</text><text start="788.519" dur="3.801">descent process how well does it</text><text start="790.56" dur="4.079">actually perform on images that it&amp;#39;s</text><text start="792.32" dur="4">never seen before well the one that I&amp;#39;ve</text><text start="794.639" dur="4.281">described here with the two hidden</text><text start="796.32" dur="5.759">layers of 16 neurons each chosen mostly</text><text start="798.92" dur="5.479">for aesthetic reasons well it&amp;#39;s not bad</text><text start="802.079" dur="4.801">it classifies about 96% of the new</text><text start="804.399" dur="3.961">images that it sees correctly and</text><text start="806.88" dur="3.6">honestly if you look at some of the</text><text start="808.36" dur="5.599">examples that it messes up on you kind</text><text start="810.48" dur="3.479">of feel compelled to cut it a little</text><text start="815.079" dur="3.721">slack now if you play around with the</text><text start="817.24" dur="4.76">hidden layer structure and make a couple</text><text start="818.8" dur="5.399">tweaks you can get this up to 98% and</text><text start="822" dur="3.76">that&amp;#39;s pretty good it&amp;#39;s not the best you</text><text start="824.199" dur="3.161">can certainly get better performance by</text><text start="825.76" dur="4.12">getting more sophisticated than this</text><text start="827.36" dur="4.8">plain vanilla network but given how</text><text start="829.88" dur="4">daunting the initial task is I just</text><text start="832.16" dur="3.799">think there&amp;#39;s something incredible about</text><text start="833.88" dur="4.079">any network doing this well on images</text><text start="835.959" dur="4">that it&amp;#39;s never seen before given that</text><text start="837.959" dur="3.641">we never specifically told it what</text><text start="839.959" dur="4.32">patterns to look</text><text start="841.6" dur="4.84">for originally the way that I motivated</text><text start="844.279" dur="4.161">this structure was by describing a hope</text><text start="846.44" dur="4">that we might have that the second layer</text><text start="848.44" dur="3.639">might pick up on little edges that the</text><text start="850.44" dur="3.44">third layer would piece together those</text><text start="852.079" dur="3.801">edges to recognize loops and longer</text><text start="853.88" dur="5.12">lines and that those might be pieced</text><text start="855.88" dur="6.079">together to recognize digits so is this</text><text start="859" dur="5.72">what our network is actually doing well</text><text start="861.959" dur="4.44">for this one at least not at all</text><text start="864.72" dur="3.76">remember how last video we looked at how</text><text start="866.399" dur="4.161">the weights of the connections from all</text><text start="868.48" dur="4.159">of the neurons in the first layer to a</text><text start="870.56" dur="4.48">given neuron in the second layer can be</text><text start="872.639" dur="4.161">visualized as a given pixel pattern that</text><text start="875.04" dur="4.68">that second layer neuron is picking up</text><text start="876.8" dur="4.44">on well when we actually do that for the</text><text start="879.72" dur="3.32">weights associated with these</text><text start="881.24" dur="4.32">transitions from the first layer to the</text><text start="883.04" dur="5.479">next instead of picking up on isolated</text><text start="885.56" dur="6.24">little edges here and there they look</text><text start="888.519" dur="5.68">well almost random just with some very</text><text start="891.8" dur="4.399">loose patterns in the middle there it</text><text start="894.199" dur="4.161">would seem that in the unfathomably</text><text start="896.199" dur="4.08">large 13,000 dimensional space of</text><text start="898.36" dur="3.52">possible weights and and biases our</text><text start="900.279" dur="3.8">Network found itself a happy little</text><text start="901.88" dur="4.759">local minimum that despite successfully</text><text start="904.079" dur="3.961">classifying most images doesn&amp;#39;t exactly</text><text start="906.639" dur="4">pick up on the patterns that we might</text><text start="908.04" dur="4.4">have hoped for and to really drive this</text><text start="910.639" dur="4.681">point home watch what happens when you</text><text start="912.44" dur="5.16">input a random image if the system was</text><text start="915.32" dur="4.48">smart you might expect it to either feel</text><text start="917.6" dur="4">uncertain maybe not really activating</text><text start="919.8" dur="4.76">any of those 10 output neurons or</text><text start="921.6" dur="5">activating them all evenly but instead</text><text start="924.56" dur="4.639">it confidently gives you some nonsense</text><text start="926.6" dur="4.919">answer as if it feels as sure that this</text><text start="929.199" dur="5.161">random noise as a five as it does that</text><text start="931.519" dur="5.161">an actual image of a five is a five</text><text start="934.36" dur="4.839">phrased differently even if this network</text><text start="936.68" dur="5.639">can recognize digits pretty well it has</text><text start="939.199" dur="4.601">no idea how to draw them a lot of this</text><text start="942.319" dur="3.88">is because it&amp;#39;s such a tightly</text><text start="943.8" dur="4.24">constrained training setup I mean put</text><text start="946.199" dur="3.841">yourself in the Network&amp;#39;s shoes here</text><text start="948.04" dur="4.239">from its point of view the entire</text><text start="950.04" dur="4.279">universe consists of nothing but clearly</text><text start="952.279" dur="4.441">defined unmoving digits centered in a</text><text start="954.319" dur="4.241">tiny grid and its cost function just</text><text start="956.72" dur="4.28">never gave it any incentive to be</text><text start="958.56" dur="4.68">anything but utterly confident in its</text><text start="961" dur="3.56">decisions so with this as the image of</text><text start="963.24" dur="3.12">what those second layer neurons are</text><text start="964.56" dur="3.32">really doing you might wonder why I</text><text start="966.36" dur="3.159">would introduce this network with the</text><text start="967.88" dur="3.48">motivation of picking up on edges and</text><text start="969.519" dur="4.841">patterns I mean that&amp;#39;s just not at all</text><text start="971.36" dur="5.12">what it ends up doing well this is not</text><text start="974.36" dur="4.36">meant to be our end goal but instead a</text><text start="976.48" dur="4.039">starting point frankly this is old</text><text start="978.72" dur="4.4">technology the kind researched in the &amp;#39;</text><text start="980.519" dur="3.961">80s and &amp;#39;90s and you do need to</text><text start="983.12" dur="3.44">understand it before you can understand</text><text start="984.48" dur="3.76">more detailed modern variants and it</text><text start="986.56" dur="3.719">clearly is capable of solving some</text><text start="988.24" dur="3.68">interesting problems s but the more you</text><text start="990.279" dur="5.641">dig into what those hidden layers are</text><text start="991.92" dur="4">really doing the less intelligent it</text><text start="997.6" dur="5.2">seems Shifting the focus for a moment</text><text start="999.959" dur="4.36">from how networks learn to how you learn</text><text start="1002.8" dur="4.159">that&amp;#39;ll only happen if you engage</text><text start="1004.319" dur="4.161">actively with the material here somehow</text><text start="1006.959" dur="4.481">one pretty simple thing that I want you</text><text start="1008.48" dur="5.479">to do is just pause right now and think</text><text start="1011.44" dur="4.72">deeply for a moment about what changes</text><text start="1013.959" dur="4.36">you might make to this system and how it</text><text start="1016.16" dur="4.56">perceives images if you wanted it to</text><text start="1018.319" dur="4.64">better pick up on things like edges and</text><text start="1020.72" dur="4.479">patterns but better than that to</text><text start="1022.959" dur="3.96">actually engage with the material I</text><text start="1025.199" dur="3.441">highly recommend the book by Michael</text><text start="1026.919" dur="4.321">neelen on deep learning and neural</text><text start="1028.64" dur="5">networks in it you can find the code and</text><text start="1031.24" dur="4.4">the data to download and play with for</text><text start="1033.64" dur="3.88">this exact example and the book will</text><text start="1035.64" dur="4.559">walk you through step by step what that</text><text start="1037.52" dur="5.159">code is doing what&amp;#39;s awesome is that</text><text start="1040.199" dur="4.201">this book is free and publicly available</text><text start="1042.679" dur="3.721">so if you do get something out of it</text><text start="1044.4" dur="4.56">consider joining me in making a donation</text><text start="1046.4" dur="4.12">towards neel&amp;#39;s efforts I&amp;#39;ve also linked</text><text start="1048.96" dur="3.48">a couple other resources that I like a</text><text start="1050.52" dur="3.76">lot in the description including the</text><text start="1052.44" dur="5.119">phenomenal and beautiful blog post by</text><text start="1054.28" dur="5.48">Chris Ola and the articles in</text><text start="1057.559" dur="3.681">distill to close things off here for the</text><text start="1059.76" dur="3.08">last few minutes I want to jump back</text><text start="1061.24" dur="3.84">into a snippet of the interview that I</text><text start="1062.84" dur="4.16">had with Lea Lee you might remember her</text><text start="1065.08" dur="3.76">from the last video she did her PhD work</text><text start="1067" dur="3.48">in deep learning and in this little</text><text start="1068.84" dur="3.36">snippet she talks about two recent</text><text start="1070.48" dur="3.559">papers that really dig into to how some</text><text start="1072.2" dur="4.24">of the more modern image recognition</text><text start="1074.039" dur="4.12">networks are actually learning just to</text><text start="1076.44" dur="3.28">set up where we were in the conversation</text><text start="1078.159" dur="3.64">the first paper took one of these</text><text start="1079.72" dur="4.04">particularly deep neural networks that&amp;#39;s</text><text start="1081.799" dur="3.521">really good at image recognition and</text><text start="1083.76" dur="3.6">instead of training it on a properly</text><text start="1085.32" dur="4.52">labeled data set it shuffled all of the</text><text start="1087.36" dur="4.24">labels around before training obviously</text><text start="1089.84" dur="3.36">the testing accuracy here was going to</text><text start="1091.6" dur="3.68">be no better than random since</text><text start="1093.2" dur="4.16">everything&amp;#39;s just randomly labeled but</text><text start="1095.28" dur="4.399">it was still able to achieve the same</text><text start="1097.36" dur="5.08">training accuracy as you would on a</text><text start="1099.679" dur="4.721">properly labeled data set basically the</text><text start="1102.44" dur="3.8">millions of weights for this particular</text><text start="1104.4" dur="4.12">Network were enough for it to just</text><text start="1106.24" dur="3.72">memorize the random data which kind of</text><text start="1108.52" dur="3.6">Reon is the question for whether</text><text start="1109.96" dur="4">minimizing this cost function actually</text><text start="1112.12" dur="4.16">corresponds to any sort of structure in</text><text start="1113.96" dur="4.52">the image or is it just you know</text><text start="1116.28" dur="4.44">memorization the entire data set of what</text><text start="1118.48" dur="4.24">the correct classification is and so a</text><text start="1120.72" dur="5.04">couple of you know half a year later at</text><text start="1122.72" dur="5.28">icml this year um there was not exactly</text><text start="1125.76" dur="4.279">rebuttal paper paper that addressed some</text><text start="1128" dur="3.2">aspects of like hey actually these</text><text start="1130.039" dur="3.64">networks are doing something a little</text><text start="1131.2" dur="5.56">bit smarter than that if you look at</text><text start="1133.679" dur="6.12">that accuracy curve if you were just</text><text start="1136.76" dur="6.399">training on a random data set that curve</text><text start="1139.799" dur="5.24">sort of went down very you know very</text><text start="1143.159" dur="3.961">slowly in almost kind of a linear</text><text start="1145.039" dur="4.601">fashion so you&amp;#39;re really struggling to</text><text start="1147.12" dur="4.2">find that local Minima of possible you</text><text start="1149.64" dur="3.12">know the right weights that would get</text><text start="1151.32" dur="3.04">you that accuracy whereas if you&amp;#39;re</text><text start="1152.76" dur="3.88">actually training on a structured data</text><text start="1154.36" dur="3.64">set one that has the right labels you</text><text start="1156.64" dur="2.48">know you fiddle around a little bit in</text><text start="1158" dur="4.159">the beginning but then you kind of</text><text start="1159.12" dur="5.039">dropped very fast to get to that uh</text><text start="1162.159" dur="6.361">accuracy level and so in some sense it</text><text start="1164.159" dur="6">was easier to find that local Maxima and</text><text start="1168.52" dur="4.159">so so was also interesting about that is</text><text start="1170.159" dur="4.281">it it brings into light another paper</text><text start="1172.679" dur="5.321">from actually a couple of years ago</text><text start="1174.44" dur="5.239">which has a lot more uh simplifications</text><text start="1178" dur="3.679">about the network layers but one of the</text><text start="1179.679" dur="4.641">results was saying how if you look at</text><text start="1181.679" dur="5.48">the optimization landscape the local</text><text start="1184.32" dur="5.4">Minima that these networks tend to learn</text><text start="1187.159" dur="4.921">are actually of equal quality so in some</text><text start="1189.72" dur="6.16">sense if your data set is structured you</text><text start="1192.08" dur="3.8">should be able to find that much more</text><text start="1197.4" dur="4.399">easily my thanks as always to those of</text><text start="1200.039" dur="3.681">you supporting on patreon I&amp;#39;ve said</text><text start="1201.799" dur="4.081">before just what a game changer patreon</text><text start="1203.72" dur="4.4">is but these videos really would not be</text><text start="1205.88" dur="4.44">possible without you I also want to give</text><text start="1208.12" dur="3.679">a special thanks to the VC firm amplify</text><text start="1210.32" dur="5.44">Partners in their support of these</text><text start="1211.799" dur="3.961">initial videos in the series</text><text start="1216.57" dur="12.17">[Music]</text></transcript>